<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker 初探 (7) - Docker Machine]]></title>
    <url>%2Fdeployment%2Fnote-docker-machine%2F</url>
    <content type="text"><![CDATA[参考资料: Swarms Docker Machine 前言Docker GetStarted Swarm 官方文档中，为了在单机上演示如何搭建 Docker Swarm 而引入了 Docker Machine，并在其中穿插了虚拟 shell 环境的内容，个人认为这部分内容干扰了集群搭建的关键信息。 Docker Machine通过 Docker Machine 可实现在虚拟主机或远程主机上安装 Docker 引擎，并使用 docker-machine 命令行对其进行管理的工具。Docker Machine 在 Docker for Mac 和 Docker for Windows 上都已经预装，Linux 系统需要手动安装。 Docker Machine 有以下两种常用场景: 有些老的桌面系统主机如 Windows 和 Mac，它们不满足预装 Docker for Windows 或 Docker for Mac 的条件，但我想要在这些系统上运行 Docker。 我想要在远程主机系统上运行统一的 Docker image。 docker 守护进程由服务端和客户端两部分组成，服务端发布 REST API 接口开放给客户端，docker CLI 客户端通过 REST API 接口与服务端通信。 无论你的管理主机是 Windows，Mac 还是 Linux，都可以安装 Docker Machine 并使用 docker-machine 命令统一管理大量的 Docker 从机。通常，你会将 Docker Machine 安装到本地主机，Docker Machine 包含 docker-machine 命令行工具和 Docker 引擎的客户端 docker 命令行工具。可通过 docker-machine 命令行工具在本地主机的虚拟环境或云主机上安装 Docker 引擎，它将自动创建虚拟环境，安装 Docker 引擎，配置好 docker 客户端，每一个被管理的从机由 Docker host(运行了 Docker 引擎的主机) 和一个配置好的客户端组成，它们通常又被称为 “machines”。 意即，如果仅仅安装 Docker Machine，将不会在本机安装 Docker 引擎，仅仅安装其客户端管理工具 在 Linux 上安装 Docker Machine首先在 docker/machine release page 页面上找到最新的发布版本号，并修改以下命令对应的位置123$ base=https://github.com/docker/machine/releases/download/v0.14.0 &amp;&amp; curl -L $base/docker-machine-$(uname -s)-$(uname -m) &gt;/tmp/docker-machine &amp;&amp; sudo install /tmp/docker-machine /usr/local/bin/docker-machine 打印 docker-machine 的版本以确认安装完成:12$ docker-machine --versiondocker-machine version 0.14.0, build 89b8332 以非 driver 方式添加主机Docker Machine 为众多的虚拟机和云服务商提供了 driver，以通过这些驱动在不同的服务商主机上安装 Docker 引擎，但也提供了一种 url 的方式来添加已有 docker 主机:1234$ docker-machine create --driver none --url=tcp://50.134.234.20:2376 custombox$ docker-machine lsNAME ACTIVE DRIVER STATE URLcustombox * none Running tcp://50.134.234.20:2376 此后，便可通过 docker-machine 远程管理列表中的所有主机，可通过 docker-machine ssh custombox 向该主机发送指令，例如:1$ docker-machine ssh custombox "docker container ls" 如果出于某些原因出现 ssh 连接的问题，那么可以改用系统本地的 ssh 进行通信，加入 --native-ssh 参数即可: docker-machine --native-ssh ssh 配置虚拟 Docker 主机的 shell 环境除了通过 docker-machine ssh 向 Docker 主机发送命令之外，还可以使用 docker-machine env &lt;machine&gt; 配置一个虚拟 shell 环境直接与目标主机的 Docker 守护进程通信。这样，便可访问执行 docker-machine 的本地主机的资源，如 docker-compose.yml 文件:12345678$ docker-machine env customboxexport DOCKER_TLS_VERIFY="1"export DOCKER_HOST="tcp://50.134.234.20:2376"export DOCKER_CERT_PATH="/Users/pango/.docker/machine/machines/custombox"export DOCKER_MACHINE_NAME="custombox"# Run this command to configure your shell:# eval $(docker-machine env custombox) 执行输出结果返回的最后一行命令:1eval $(docker-machine env myvm1) 执行 docker-machine ls 验证 custombox 变成了活动主机，Active 一栏的 * 指示了该状态:1234$ docker-machine lsNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORScustombox * none Running tcp://50.134.234.20:2376 v17.06.2-ce 清除 docker-machine 的 shell 变量设置1eval $(docker-machine env -u) 该命令退出与目标主机建立的虚拟 shell 环境并回到之前的环境中。 有关 Docker Machine 更多详情参考 Docker Machine。]]></content>
      <categories>
        <category>deployment</category>
      </categories>
      <tags>
        <tag>ops</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 初探 (5) - Stacks]]></title>
    <url>%2Fdeployment%2Fnote-docker-stacks%2F</url>
    <content type="text"><![CDATA[参考资料: Stacks 前言一个 Stack 是一组相互作用并共享依赖的 service，并且一起协同和伸缩的单元，一个 Stack 可以能够定义包含一个系统的所有功能。之前关于 Service 的介绍中已经用到了 stack，但那只是包含单个服务的 stack，在生产环境中 stack 往往许多服务，并将它们运行在不同的主机上。 添加一个新的 service 并重新部署首先添加一个免费的可视化服务以监控 swarm 是如何调度 container 的。 打开 docker-compose.yml，填充以下内容:123456789101112131415161718192021222324252627282930version: "3"services: web: # replace username/repo:tag with your name and image details image: username/repo:tag deploy: replicas: 5 restart_policy: condition: on-failure resources: limits: cpus: "0.1" memory: 50M ports: - "80:80" networks: - webnet visualizer: image: dockersamples/visualizer:stable ports: - "8080:8080" volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: placement: constraints: [node.role == manager] networks: - webnetnetworks: webnet: 在 web 服务之后，添加了一个名为 visualizer 的服务，注意两项: volumns 表示给予其访问 docker 主机的 socket 文件的权限。 placement 确保该服务仅能在 swarm manager 上运行。 重新部署 stack1234$ docker stack deploy -c docker-compose.yml getstartedlabUpdating service getstartedlab_web (id: fmh8sn491klzx4vnz6uft0wc9)Creating service getstartedlab_visualizer 从图上可以看出，visualizer 的单副本服务运行在了作为 Swarm Manager 的 Aiur 主机上。Visualizer 是一个可以包含在任何 stack 中单独运行的服务，它没有任何依赖。 持久化数据现在，为 Stack 添加 Redis 数据库服务。 在 docker-compose.yml 文件中新增服务声明:123456789101112redis: image: redis ports: - "6379:6379" volumes: - "/home/pango/data:/data" deploy: placement: constraints: [node.role == manager] command: redis-server --appendonly yes networks: - webnet Redis 官方提供了 Docker 的 image 并将其命名为 redis，所以没有 username/repo 的前缀，redis 的 container 预设端口为 6379，在 Compose 文件中同样以 6379 端口加以映射并对外界开放。在 redis 服务的声明中，有如下几点重要信息: redis 始终在 swarm manager 上运行，所以它总是使用相同的文件系统 redis 访问 container 中的 /data 目录来持久化数据，并映射到主机文件系统的 /home/docker/data 目录 如果不加以映射，那么 redis 仅将数据保存在 container 中的 /data 目录下，一旦该 container 被重新部署则数据就会被清除。 在 Swarm Manager 主机上创建 /data 目录: 1mkdir ~/data 重新部署 stack: 12345$ docker stack deploy -c docker-compose.yml getstartedlabUpdating service getstartedlab_web (id: fmh8sn491klzx4vnz6uft0wc9)Updating service getstartedlab_visualizer (id: oj86dzaracmuoxb3ucvi7ro1e)Creating service getstartedlab_redis 执行 docker service ls 验证 3 个服务都已运行: 123456$ docker service lsID NAME MODE REPLICAS IMAGE PORTSkuuocvu54rd1 getstartedlab_redis replicated 1/1 redis:latest *:6379-&gt;6379/tcpoj86dzaracmu getstartedlab_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080-&gt;8080/tcpfmh8sn491klz getstartedlab_web replicated 5/5 frosthe/get-started:part2 *:80-&gt;80/tcp 现在访问任意节点的 ip:8080，可以看到 redis 服务已经运行在 Swarm Manager 节点上。]]></content>
      <categories>
        <category>deployment</category>
      </categories>
      <tags>
        <tag>ops</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 初探 (4) - Swarms]]></title>
    <url>%2Fdeployment%2Fnote-docker-swarms%2F</url>
    <content type="text"><![CDATA[参考资料: Swarms Create a swarm Swarm 简介Swarm 是一组运行了 Docker 的机器并加入到同一个集群，通过使用相同的命令，但这些命令由在该集群上被称作 「集群管理员(Swarm Manager)」 的机器执行。集群上的机器可以是物理主机，也可以是虚拟主机，在加入集群之后，它们统一称为「节点(nodes)」。 Swarm Manager 是唯一能够执行命令的主机，并且负责授权其他主机以 worker 身份加入到集群中，worker 仅提供集群容量，没有任何指挥其他主机的权限。Swarm Manager 可通过 「最小化节点」策略: 提取最为空闲的主机运行 container。 「全局化」策略: 确保集群中的每一个节点都运行一个指定的 container。 采用何种策略在 docker-compose.yml 文件中配置。 之前的案例都是以「单机模式」运行应用程序，Docker 可转换为「集群模式」，启用集群模式的主机将立即成为 Swarm Manager，自此之后，所有的命令都将作用于整个集群而非仅仅本地主机。 搭建 Docker 集群执行 docker swarm init 便可启用集群模式便将当前主机作为 Swarm Manager。然后在另外一台主机执行 docker swarm join 命令以 worker 身份加入集群。 接下来以一个可由公网访问的 Linux 主机(Aiur) 作为 Swarm Manager，一台 Windows PC(moby) 作为 Worker: 可借助 Docker Machine 在一台第三方的主机执行 docker-machine ssh &lt;hostname&gt; &quot;&lt;command&gt;&quot; 远程连接到另外一台 docker 主机，并执行相应的命令，简单起见，这里统一采用本机执行命令的方式。有关 Docker Machine 的更多信息参考 Docker Machine 首先创建 Swarm 并暴露 Manager 的公网 ip 地址:123456789$ docker swarm init --advertise-addr 192.168.99.100Swarm initialized: current node (bsa93xks81o4ab971nv8ppzza) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-40lg4ocglu93sdfyp28nosqvrupjgmb4iv9cnxv46kcwdqvqtw-btzb7tu4ks5stfe0spf0x62ty 192.168.99.100:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. --advertise-addr 配置 Manager 节点以 192.168.99.100 向外发布，其他主机或集群中的其他节点必须能够访问该地址，输出的内容包含了如何加入新的 worker 节点以及如何加入新的 manager 节点。--token 是加入集群的凭证。 2377 是 Docker 集群的默认管理端口，2376 是 Docker 守护进程的端口。 查看当前 docker 的状态:12345678910$ docker info# ...snip...Swarm: active NodeID: bsa93xks81o4ab971nv8ppzza Is Manager: true ClusterID: s767fic8a9abaxq6n42d8vhd2 Managers: 1 Nodes: 1# ...snip... 1234$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONbsa93xks81o4ab971nv8ppzza * Aiur Ready Active Leader 18.03.1-ce 将其他主机加入该节点由创建集群提供的命令行代码将其他主机加入到节点:12docker swarm join --token SWMTKN-1-40lg4ocglu93sdfyp28nosqvrupjgmb4iv9cnxv46kcwdqvqtw-btzb7tu4ks5stfe0spf0x62ty 192.168.99.100:2377This node joined a swarm as a worker. 执行 docker node ls 查看节点:12345$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONbsa93xks81o4ab971nv8ppzza * Aiur Ready Active Leader 18.03.1-cel1tat89t1ihfi04ad11mccsmi moby Ready Active 17.09.1-ce ID 后面的 * 表示当前连接的节点。 在集群上部署应用现在，在 Swarm Manager 主机环境中执行在Docker 初探 - Serivces 中相同的命令 docker stack deploy 来部署应用:1234$ docker stack deploy -c docker-compose.yml getstartedlabCreating network getstartedlab_webnetCreating service getstartedlab_web 执行相同的命令来查看 service 中的 containers，与之前不同的是，5 份 container 的副本被分配到了集群的两个节点上 Aiur 和 moby。12345678$ docker stack ps getstartedlabID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSaumkvyzfh3oo getstartedlab_web.1 frosthe/get-started:part2 moby Running Running about a minute agoy82s9nuonx3p getstartedlab_web.2 frosthe/get-started:part2 Aiur Running Running about a minute agoyp2zx0bncv55 getstartedlab_web.3 frosthe/get-started:part2 moby Running Running about a minute agoul7f48j0grzy getstartedlab_web.4 frosthe/get-started:part2 moby Running Running about a minute agojzn0fndpf206 getstartedlab_web.5 frosthe/get-started:part2 Aiur Running Running about a minute ago 访问集群通过访问 Aiur 和 moby 两者的 ip 地址都可以访问应用程序，返回的 Hostname 轮流展示 5 个 container 的 ID。但这些 container 实例分别运行于两个节点上。 之所以访问任何一个节点的 ip 地址都可以到达应用程序是因为参与集群的节点共用「入口路由网格」，这保证了一个 service 在部署到集群的某个端口后，无论哪个节点正在运行或没有运行任何 container 实例，该端口都会保留给该 service。下图表示了一个名为 my-web 的服务在一个三节点的集群中发布 8080 端口:要使入口路由网格正常运作，请确保集群中各个节点的以下端口是可访问的: 7946 TCP/UDP 端口用于发现 container 网络 4789 UDP 端口用于 ingress network 清理与重启停止 stack:1234$ docker stack rm getstartedlabRemoving service getstartedlab_webRemoving network getstartedlab_webnet 将节点主机从集群分离: Worker: 12$ docker swarm leaveNode left the swarm. Manager: 12$ docker swarm leave --forceNode left the swarm.]]></content>
      <categories>
        <category>deployment</category>
      </categories>
      <tags>
        <tag>ops</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 初探 (3) - Services]]></title>
    <url>%2Fdeployment%2Fnote-docker-services%2F</url>
    <content type="text"><![CDATA[参考资料: Services Docker ComposeDocker Compose 是用以定义与运行多容器 Docker 应用的工具，使用 YAML 文件配置应用的服务，之后，执行一句简单的命令行来创建和启动定义好的应用。Compose 在 Docker for Mac 和 Docker for Windows 上都已经预装了，但 Linux 系统需要手动安装。 安装 Docker Compose 执行命令下载 Docker Compose 的最新版1$ sudo curl -L https://github.com/docker/compose/releases/download/1.21.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose 将版本号改为 Compose repository release page on GitHub 上最新版的版本号。 为 docker-compose 可执行程序添加执行权限。 1$ sudo chmod +x /usr/local/bin/docker-compose 测试安装是否成功: 12$ docker-compose --versiondocker-compose version 1.21.2, build a133471 有关 Docker Compose 的更多详情参考 Docker Compose 关于 Service在分布式系统中，各个部件被称作 Services。例如，一个视频分享网站，它很可能包含一个把应用数据存到数据库中的服务，用户上传一个视频后，在后台转码视频的专有服务以及一个响应前台的服务。一个 service 仅仅运行一个 image，但它指导了改 image 应该如何运行 - 应该使用哪个端口，需要运行多少个 container 实例以支撑服务的容量。container 实例数量的增减用以伸缩该服务。 docker-compose.yml 文件docker-compose.yml 文件是一个定义 container 应该如何运作的 YAML 文件。新建该文件放置于任何想要的位置，并为其填充内容:123456789101112131415161718192021$ touch docker-compose.ymlversion: "3"services: web: # replace username/repo:tag with your name and image details image: frosthe/get-started:part2 deploy: replicas: 5 resources: limits: cpus: "0.1" memory: 50M restart_policy: condition: on-failure ports: - "80:80" networks: - webnetnetworks: webnet: 该文件告知 Docker 执行以下事情: 拉取 frosthe/get-started:part2 的 image 定义了一个名为 web 的服务，该服务运行该 image 的 5 份实例，且限制每个实例最多占用 10% 的 CPU 和 50M 的内存 立即重启 container 如果任何一个启动失败 映射主机的 80 端口到 web 服务的 80 端口 告知名为 web 的服务的所有 container 实例通过一个名为 webnet 的负载均衡网络共享 80 端口(其内部机制为 container 轮流将它们自己发布到 web 服务的 80 端口) 定义 webnet 网络，默认配置为一个叠加的负载均衡网络 启动新的负载均衡应用首先初始化一个 swarm:123$ docker swarm initSwarm initialized: current node (iqaw9hws38ctvpghyxaycrj4t) is now a manager. 有关 swarm 的介绍，参考 Docker 初探 (4) - Swarms，如果不运行该指令，将会报 “this node is not a swarm manager” 的错误信息 现在，运行该程序，给它一个名称:1234$ docker stack deploy -c docker-compose.yml getstartedlabCreating network getstartedlab_webnetCreating service getstartedlab_web 执行 docker service ls 查看是否成功运行:1234$ docker service lsID NAME MODE REPLICAS IMAGE PORTSo3jo325ds62i getstartedlab_web replicated 5/5 frosthe/get-started:part2 *:80-&gt;80/tcp 可以看到名为 web 的服务已经运行，在一个服务中单独运行的 container 称为一个 task，task 会被分配递增的数字 ID，直到定义的副本数量的最大值，列出该服务的所有 task:12345678$ docker service ps lsID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSg7dr8ecu5a6d getstartedlab_web.1 frosthe/get-started:part2 Aiur Running Running 2 minutes ago1gu635hjck46 getstartedlab_web.2 frosthe/get-started:part2 Aiur Running Running 2 minutes agorny0soxz5rh6 getstartedlab_web.3 frosthe/get-started:part2 Aiur Running Running 2 minutes agozfvdxi8e6jpt getstartedlab_web.4 frosthe/get-started:part2 Aiur Running Running 2 minutes agoqudnod8hs9xw getstartedlab_web.5 frosthe/get-started:part2 Aiur Running Running 2 minutes ago 如果使用命令列出当前系统中的 container，这些 task 也会被包含其中，但不会由其从属的 service 作筛选:12345678$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6642f46afaa4 frosthe/get-started:part2 &quot;python app.py&quot; 4 minutes ago Up 4 minutes 80/tcp getstartedlab_web.4.zfvdxi8e6jpte0qcobidxvuq600ffa6c89b09 frosthe/get-started:part2 &quot;python app.py&quot; 4 minutes ago Up 4 minutes 80/tcp getstartedlab_web.5.qudnod8hs9xwyhe07o2xomm25bc1563fa67f8 frosthe/get-started:part2 &quot;python app.py&quot; 4 minutes ago Up 4 minutes 80/tcp getstartedlab_web.3.rny0soxz5rh641082ybs1h6vj526e37ad6994 frosthe/get-started:part2 &quot;python app.py&quot; 4 minutes ago Up 4 minutes 80/tcp getstartedlab_web.1.g7dr8ecu5a6dh5ua5lqo0cbjwee599ab17416 frosthe/get-started:part2 &quot;python app.py&quot; 4 minutes ago Up 4 minutes 80/tcp getstartedlab_web.2.1gu635hjck46xt8leib3md9nb 现在，通过 ip 地址访问该应用，连续刷新浏览器，会看到每次刷新后 Hostname 一项都改变。之前提到过，Hostname 返回的是 container 的 ID，所以这里会看到 Hostname 的值为 5 个副本 ID 值轮流变化。 扩展该应用可以在 docker-compose.yml 文件中修改 replicas 的值来扩展计算量，然后再次执行 docker stack deploy 命令:1$ docker stack deploy -c docker-compose.yml getstartedlab Docker 支持实时更新，不需要关闭 stack 或停止任何 container。 停止应用及 swarm 停止 app: 1234$ docker stack rm getstartedlabRemoving service getstartedlab_webRemoving network getstartedlab_webnet 停止 swarm: 123$ docker swarm leave --forceNode left the swarm.]]></content>
      <categories>
        <category>deployment</category>
      </categories>
      <tags>
        <tag>ops</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 初探 (2) - Containers]]></title>
    <url>%2Fdeployment%2Fnote-docker-containers%2F</url>
    <content type="text"><![CDATA[参考资料: Containers 以 Docker 的方式定义一个应用Container 位于架构层次的最底层，其上是 Service，服务定义了 Container 如何在生成环境互作用。Service 之上是 Stack，其定义了所有服务之间的互作用。 在过去，如果希望便携一个 python 应用，那么第一件事就是要在主机上安装 python 运行时，这便限制了该主机的功能很难用作它途，如果想要部署一个 .net core 应用，那么 python 的运行时毫无意义。 在 Docker 生态中，python 以 image 的形式定义，并且可由任何其他 image 引用，从而确保所有的 image 都是可插拔的，并且不会干扰本地主机的环境。 使用 Dockerfile 定义一个 image用于定义 image 的被称为 Dockerfile，该文件描述了哪些环境需要被加载到 container 中，类似访问网络资源的接口和硬盘驱动都在此环境中被虚拟化，并与系统的其他部分完全隔离。因此，我们需要将端口映射到 container 外部，并且确切定义要将哪些文件「复制到」该环境中，完成这些配置之后，便可期待该 Dockerfile 定义的应用可以在任何地方运行了。 创建一个新目录，并导航到其中作为工作目录 12$ mkdir my-first-docker-image$ cd my-first-docker-image 新建一个名为 Dockerfile 的文件: 12$ touch Dockerfile$ nano Dockerfile 复制以下内容至该文件: 1234567891011121314151617181920# Use an official Python runtime as a parent imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install --trusted-host pypi.python.org -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD ["python", "app.py"] 该 Dockerfile 引用的 app.py 及 requirements.txt 尚未创建，执行命令以创建它们:12$ touch requirements.txt$ touch app.py 注意，两者位于与 Dockerfile 相同的目录 由此，应用所需的文件都已就绪，当上述 Dockerfile 生成一个 image 时，Dockerfile 中的 Add 指令会将当前目录下的所有文件拷贝至子目录 /app，并且 app.py 将可通过 HTTP 协议访问因为 EXPOSE 指令暴露了 80 端口。 填充 requirements.txt1234$ nano requirements.txtFlaskRedis 填充 app.py1234567891011121314151617181920212223242526$ nano app.pyfrom flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host="redis", db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route("/")def hello(): try: visits = redis.incr("counter") except RedisError: visits = "&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;" html = "&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;" \ "&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;" \ "&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;" return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname(), visits=visits)if __name__ == "__main__": app.run(host='0.0.0.0', port=80) 以上两段代码值得注意的是 pip install -r requirements.txt app.py 中使用了环境变量 NAME socket.gethostname() 的调用 至此，本地主机不需要安装任何声明在 requirements.txt 文件中的 python 库，但该程序仍然不完整，因为我们仅仅安装了 Redius 的 python 库，但 Redius 进程本身并没有在本地主机安装运行。 从 container 中查询主机名称将返回 container ID，它的值相当于进程的 ID。 生成应用 在生成应用之前，首先确保工作目录在新建目录的顶层: 123$ lsDockerfile app.py requirements.txt 执行生成指令，这将会产生一个 Docker image，使用 -t 选项给它一个标签。 1$ docker build -t friendlyhello . 注意 . 表示生成基于的目录位置，表示当前目录 生成过程中 Docker 引擎会根据 Dockerfile 声明的引用库去下载需要的文件，这可能需要一些时间。生成完成后，如何查看生成的位置呢？执行 docker image ls 指令即可看到新生成的 image。 1234$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEfriendlyhello latest 2f701134298d About a minute ago 145MB 在本地生成的 image 会放至 Docker 的本地 Registry，Docker 以 Registry 的形式进行本地与远程 image 库的同步。 运行应用使用 -p 选项将本地主机的 8000 端口映射到 container 的 80 端口1$ docker run -p 4000:80 friendlyhello 执行以上命令之后，可以看到一条 python 消息称应用侦听 http://0.0.0.0:80，该消息来自于 container 内部，其并不知道外部如何对其映射。 现在在浏览器中输入 {your-host-ip}:4000 将会得到预期的结果。同样，也可以使用命令行工具 curl 来获取相同的结果:1$ curl http://&#123;your-host-ip&#125;:4000 端口映射 4000:80 很好的对应了在 Dockerfile 中声明的 EXPOSE 和使用 docker run -p 指定的端口。 现在，使用 -d 选项让该应用以 detached 模式在后台运行：123$ docker run -d -p 4000:80 friendlyhellob3076b38a52b82c9c39fa0e99bd51a0f49912869a141ca2f3c677deb3e481bab 该命令返回一个 Container ID 执行 docker container ls 将会看到正在运行的应用，该 Container ID 与之前返回的 ID 一致。12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb3076b38a52b friendlyhello "python app.py" 7 seconds ago Up 6 seconds 0.0.0.0:4000-&gt;80/tcp admiring_vaugh 使用相同的 Container ID 执行 docker container stop 来结束进程: 分享 imageimage 的集合称作 repository，类似于一个 Github 仓库，而一个 Registry 是 repositories 的集合，一个 registry 的帐号可以创建多个 repository，docker 默认使用 Docker 的公共 Registry。有关 Registry 的详情参考 Docker Trusted Registry。 使用 Docker ID 登录cloud.docker.com 提供了托管 image 的云服务，注册一个帐号来使用公开的 Registry。docker CLI 同样集成了 docker cloud 的登录功能，执行以下代码:1$ docker login 为 image 设置标签将本地 image 与远程 registry 的 repository 同步的符号格式为 username/repository:tag，tag 是可选的，但建议为 image 设置标签，因为它是 registry 为 Docker image 添加版本号的机制。为 repository 和 tag 定义有意义的名称，例如 get-started:part2，这会将该 image 推送到 get-started 仓库并将其标签设置为 part2。 使用 docker tag {local-image} {your-docker-id}/{your-repository}:{your-tag} 来为 image 设置标签，例如:1$ docker tag friendlyhello pango/get-started:part2 再次执行 docker image ls 查看:1234REPOSITORY TAG IMAGE ID CREATED SIZEfriendlyhello latest d9e555c53008 3 minutes ago 195MBpango/get-started part2 d9e555c53008 3 minutes ago 195MBpython 2.7-slim 1c7128a655f6 5 days ago 183MB 发布 image将标签化的 image 上传至 repository:1$ docker push &#123;your-docker-id&#125;/&#123;your-repository&#125;:&#123;your-tag&#125; 上传完成后，使用 Docker ID 登录 Docker Hub 将会看到刚刚上传的 image。 拉取并运行 image现在，可以执行以下代码在任何地方运行应用:1$ docker run -p 4000:80 username/repository:tag 如果该 image 无法在本地获取，Docker 会从远程 repository 将其拉取至本地。]]></content>
      <categories>
        <category>deployment</category>
      </categories>
      <tags>
        <tag>ops</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 初探 (1) - 搭建 Docker 环境]]></title>
    <url>%2Fdeployment%2Fnote-docker-setup%2F</url>
    <content type="text"><![CDATA[参考资料: Get Docker CE for Ubuntu Post-installation steps for Linux Docker 继承自 Linux 系统的「容器化(Containerization)」，是一个为开发人员和系统管理员用于开发，部署和运行应用程序的容器系统，容器化的特点有: 灵活性: 任何复杂的应用都可以被容器化 轻量: 容器共享主机的内核 可互换: 可在运行过程中部署更新 便携性: 本地编译，云端部署，在任何地方都可以运行 伸缩性: 增加容器副本相当容易且自动化 可堆叠: 可在运行过程中纵向扩展 Docker 官方目前提供两个版本：Community Edition (CE) 和 Enterprise Edition (EE)。企业版是收费的。 搭建 Docker 环境Image 和 ContainerImage 是一个包含了运行应用程序所需所有东西的包——源代码，运行时，库，环境变量和配置文件Container 是 Image 的运行时实例，是 Image 在内存中的体现。 Containers 和虚拟机Container 运行在 Linux 系统本地并与其他 Container 共享主机内核，它以「离散的进程」形式存在，不会占用比一般进程更多的资源。而虚拟机则运行整个客户机操作系统，并以虚拟化的方式访问主机资源，因此虚拟机会占用更多不必要的资源。 在 Ubuntu Xenial 16.04(LTS) 系统上安装 Docker CE首先移除任何 Docker 旧版本:1$ sudo apt-get remove docker docker-engine docker.io 从 Repository 安装搭建 Repository 从 apt 更新包: 1$ sudo apt-get update 允许 apt 使用 https 来安装包 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 添加 Docker 官方 GPG key: 1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 验证 key 的最后 8 位字符:9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 123456$ sudo apt-key fingerprint 0EBFCD88pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) &lt;docker@docker.com&gt;sub 4096R/F273FCD8 2017-02-22 使用以下命令搭建稳定版的 repository: 1234$ sudo add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" lsb_release 返回 Ubuntu 发行版的名称，如 xenial。stable 为每季度发行一次的稳定版，edge 为每月发行一次的尝鲜版。 安装 Docker CE 更新包的索引: 1$ sudo apt-get update 安装最新版的 Docker CE: 1$ sudo apt-get install docker-ce 安装完成后，Docker 守护进程将自动启动 通过运行一个 hello world 程序来验证 Docker CE 已经正确安装1$ sudo docker run hello-world 该命令会从网络下载一个测试 image，并以新的容器实例执行。执行以下命令查看已下载的 image:1$ sudo docker image ls 检查正在运行的 container 实例: 1$ sudo docker container ls --all 该指令检查包含正在运行和过往运行的 container 实例记录，如果有 container 正在运行，则不需要 --all 选项。 卸载 Docker CE 卸载 Docker CE 包: 1$ sudo apt-get purge docker-ce image, container, volumns 或其他自定义的配置文件将不会自动删除，如果想要完全删除，则执行: 1$ sudo rm -rf /var/lib/docker 以 non-root 用户管理 Dockerdocker 守护进程绑定一个 Unix 套接字而非普通的 TCP 端口，默认情况下，Unix 套接字被 root 用户所有，其他用户只能通过 sudo 进行访问。docker 进程始终以 root 用户运行。 Docker CE 在安装完成后，会创建一个新的 docker 群组，但不会加入任何现有用户到该群组下，如果不想每次执行 docker 命令时加上 sudo，可以将指定用户加入到群组下。当 docker 进程启动时，docker 群组对 docker 使用的 Unix 套接字具有读写权限。 由于 docker 群组与 root 权限一致，有关 docker 的安全问题请参考 Docker Security 将当前用户添加至 docker 群组: 1$ sudo usermod -aG docker $USER 登出用户再登录以使群组重新评估再次执行 docker run hello-world 不再要求 root 权限，如果在将用户添加到 docker 群组之前已经执行过 docker 的任何命令，那么 ~/.docker 文件夹的权限会以 root 创建，为了解决这个问题，要么移除 ~/.docker(它将会自行创建)，要么更改其拥有者和权限: 12$ sudo chown "$USER":"$USER" /home/"$USER"/.docker -R$ sudo chmod g+rwx "/home/$USER/.docker" -R 将 docker 配置为开启启动有许多 Linux 发行版使用 systemd 来管理自启动服务，要使 docker 开机启动，执行:1$ sudo systemctl enable docker 禁用开机启动:1$ sudo systemctl disable docker]]></content>
      <categories>
        <category>deployment</category>
      </categories>
      <tags>
        <tag>ops</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《实现领域驱动设计》读书笔记(6) - 战术建模之领域服务]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A8%A1%E5%BC%8F%2FDDD%2Fnote-ddd-tactical-domain-service%2F</url>
    <content type="text"><![CDATA[系列大纲: 《实现领域驱动设计》读书笔记 本文大纲: 前言 什么是领域服务 独立的接口和命名实践 前言领域中的服务表示一个无状态的操作，它用于实现特定于某个领域的任务。当某个操作不适合放在实体和值对象上时，最好的方式便是使用领域服务了。有时我们倾向于使用聚合根上的静态方法来实现这些操作，但是在 DDD 中，这是一种代码异味。 什么是领域服务虽然领域服务中有 “服务” 这个词，但它并不意味着作为远程的，重量级的事务操作的提供方。当领域中的某个操作过程或转换过程不是实体或值对象的职责时，我们便应该将该操作放在一个单独的接口中，即领域服务。参考以下几点来对领域模型建模: 执行一个显著的业务操作过程 对领域对象进行转换 以多个领域对象作为输入进行计算，结果产生一个值对象。 以上第三点提到的 “计算”，也应该具有 “显著的业务操作过程” 的特点。请确保领域服务是无状态的，并且能够明确的表达限界上下文中的「通用语言」。 过度得使用领域服务将导致贫血领域模型，即所有的业务逻辑都位于领域服务中，而不是实体和值对象中。以下的例子是一个使用领域服务的情况，假设我们有以下需求: 系统必须对 User 进行认证(authenticate)，并且只有当 Tenant 处于激活状态时才能对 User 进行认证。 密码必须经过加密，且不能使用明文密码 此时，认证细节不属于 Tenant 或 User 的职责，应该创建一个专门处理认证逻辑的领域服务，客户端伪代码如下:12var authenticationService = DomainRegistry.AuthenticationService();var userDescriptor = authenticationService.Authenticate(tenantId, username, password);客户端只需获取到一个无状态的 AuthenticationService，然后调用它的 Authenticate 方法即可。与认证有关的所有实现细节放在领域服务中，在需要的情况下，领域服务可以使用任何领域对象来完成操作，包括对密码的加密过程。客户端不需要知道任何认证细节。该方法返回一个 UserDescriptor 值对象，这是一个很小的对象，并且是安全的。 而调用这段代码的客户方，在多数情况下为「应用服务」，它可以进一步将该 UserDescriptor 对象返回给它自己的调用者，由此可见领域服务和应用服务的区别。 独立的接口和命名实践如果该领域服务可能有多种实现，那么应该为其定义单独的接口，该接口应该与身份相关聚合(比如 Tenant，User 和 Group)定义在相同的「模块」中，因为 AuthenticationService 也是一个与身份相关的概念。而该接口的实现类——如果正在使用「依赖倒置原则」或「六边形架构」，可以放置在基础设施层的某个模块中。 在 C# 中通常以 I 字符开头来表示接口，此处的接口名称为 IAuthenticationService，但如果这里将实现类命名为 AuthentionService 或 DefaultAuthenticationService，这通常意味着根本就不需要一个接口，如果领域服务有多个实现类，那么应该根据各种实现类的特点进行命名，这也意味着在领域中存在一些特定的功能。对于非技术性的领域服务来说，去除接口是不会破坏可测试性的，因为该服务依赖的所有接口都可以注入进来。 依据笔者的理解，作者此处是想说明，接口很容易遭到滥用，很多模块将接口和其默认实现定义在同一个包中，这通常可以由一个单一的实现类来代替。]]></content>
      <categories>
        <category>架构与模式</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>ddd</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《实现领域驱动设计》读书笔记(5) - 战术建模之值对象]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A8%A1%E5%BC%8F%2FDDD%2Fnote-ddd-tactical-value-object%2F</url>
    <content type="text"><![CDATA[系列大纲: 《实现领域驱动设计》读书笔记 本文大纲: 不变性 概念整体 可替换性 值对象相等性 无副作用行为 最小化集成 用值对象表示标准类型 实现 持久化值对象 单个值对象 值对象集合序列化到单个列中 使用数据库实体保存多个值对象 ORM 与枚举状态对象 值对象用于度量和描述事物，即便一个领域概念必须建模成实体，在设计时也应该更偏向于将其作为值对象的容器，而不是子实体的容器 笔者曾一度认为值对象就是 C# 语言中使用 struct 结构来表示的多个数据代表一个整体的集合，后来发现书中讲到的值对象无关技术实现，而是从概念上定义它的职责，包括不可变性和非唯一性。 同样的，在有了实体这把武器之后，当一个实体需要嵌套其他对象时，实体经常遭到滥用。当面临将对象定义为实体还是值对象的选择时，由于缺乏对于值对象的充分认识，很多开发人员选择了嵌套实体。 不变性当我们只关心某个对象的属性时，该对象便可作为一个值对象，为其添加有意义的属性，并赋予它们相应的行为。值对象在其生命周期中是「不可变」的，本身代表了某种状态，它没有任何身份标识，也应该尽量避免像实体一样复杂。在设计得当的前提下，我们可以对值对象的实例进行创建和传递，甚至在使用完之后将其直接扔掉。我们不必担心客户代码对值对象进行修改，一个值对象的生命周期可长可短，它就像一个无害的过客在系统中来来往往。 当决定一个领域概念是否是一个值对象时，考虑以下特征： 它度量或者描述了领域中的一件东西 它可以作为不变量 它将不同的相关的属性组合成一个概念性的整体 当度量和描述改变时，可以用另一个值对象予以替换 它可以和其他值对象进行相等性比较 它不会对协作对象造成副作用 为了保持值对象的不变性，创建它所依赖的参数必须一次性全部传给其构造函数，之后任何时间都不可能再改变它。有时根据需要，会在值对象中引用实体对象，这种情况需要谨慎，当实体对象的状态发生改变时，引用它的值对象也将发生改变，这违背了值对象不变性特征。 概念整体编程语言提供的基元类型(如 string, int, double 等)似乎是值对象的最佳类型，但有时，这种思维方式会造成对基元类型的滥用。 假如需要在 「ThingOfWorth」 类中加入名为 「Name」 的属性，我们自然而然的会想到将其定义为 string 类型，但很快我们就发现该类型的名字需要以不同的方式进行展示，此时，处理展示方式的逻辑就会莫名其妙的由客户代码来完成，例如：123// 客户代码String name = thingOfWorth.name();String capitalizedName = name.subString(0,1).toUpperCase() + name.subString(1).toLowerCase(); 在以上示例中，客户代码自己试图解决 name 的大小写问题。通过定义 「ThingName」 类型，我们可以将与 name 有关的所有逻辑操作放到该类型中，然后在构造该值对象时进行格式化，客户代码只需调用相应的方法即可得到结果，而不必自行处理这些逻辑。 有些编程语言允许我们简单地向一个类添加新的行为(例如 C# 的扩展方法)。此时，你可能会想着用 Double 类型来表示货币，如果需要计算不同货币之间的汇率，我们只需要向 Double 类型添加 convertToCurrency(Currency aCurrency) 扩展方法即可。但是在这种场景下使用语言特性就一定是一个好主意吗？首先，和货币相关的行为很有可能丢失在浮点数计算中；其次，Double 类型也丝毫没有表达出领域概念。很快，我们就会丢掉领域关注点。 当你试图将多个属性加在一个实体上，这有可能弱化了各个属性之间的关系，那么此时就应该考虑将这些相互关联的属性组合在一个值对象中了。每个值对象都是一个「内聚的概念整体」，它表达了通用语言中的一个概念。 可替换性值对象的可替换性可通过数字的替换来理解，假设领域中有一个名为 total 的概念，该概念用整数表示。如果 total 的当前值为 3，但是之后需要重设为 4，此时我们并不会将整数修改成 4，而是简单地将 total 的值重新赋值为 4。 从语言层面来说，这里的修改其实是对该属性赋新值，但看上去像是修改，实际上只是语法糖，原先为 3 的内存并不会被修改为 4，而是被新的代表 4 的内存块替代。 考虑下面一种更复杂的值对象替换：123FullName name = new FullName("金","沐");// 稍后name = new FullName("金","灶沐"); 这里，我们并没有使用 FullName 类型的某个方法来修改其自身的状态(这破坏了值对象的不变性)，而是构造一个新的值对象实例来替换原来的实例。 值对象相等性值对象的相等性应该由组成其实例的每一个属性及其类型来决定，在上文的 「FullName」 对象中，当两个 「FullName」 实例的每个属性及其类型都相等，我们才认为两个实例相等，尽管他们在内存中是不同的地址。值对象的相等性可用来支撑「聚合」唯一标识的比较，实体的唯一标识是不能改变的，这可以部分通过值对象的不变性实现。值对象的整体概念也可以用来支撑不只一个属性的实体标识，同时，如果实体的唯一标识需要一些「无副作用行为」，这些行为便可以在值对象上实现。 无副作用行为一个对象的方法可以设计成一个「无副作用函数(Side-Effect Free Function)」，该函数表示对某个对象的操作，只用于产生输出，而不会修改对象的状态。对于不变的值对象而言，所有的方法都必须是无副作用函数。下面的例子通过调用 「FullName」 对象上的无副作用方法将该对象本身替换成另一个实例：123FullName name = new FullName("金","沐");// 稍后name = name.withMiddleInitial("灶"); 这里的代码更具表达性，withMiddleInitial 方法并没有修改值对象的状态，因此它不会产生副作用。该方法通过已有 firstName 和 lastName，外加传入的 middleName 创建一个新的 FullName 值对象实例。withMiddleInitial() 还捕获到了重要的领域业务逻辑，从而避免了将这些逻辑泄漏到客户代码中。 这里所说的捕获重要的领域业务逻辑，是指该方法本身是具有表达性的，比起使用 new 语句创建实例，更像是调用了该实例支持的某个行为满足了客户代码的需求。 有些值对象的方法引用了实体，这存在一些问题。例如下面的代码，我们有一个实体对象 product，该对象被值对象 BusinessPriority 引用。1float priority = businessPriority.priorityOf(product); 我们至少可以看出以下问题： BusinessPriority 不仅依赖 Product 类型，还试图去理解该实体的内部状态，我们应该尽量使值对象只依赖于它自己的属性，并且只理解它自身的状态。 阅读本段代码的人并不知道使用了 Product 的哪些部分，这种表达方法并不明确，从而降低了模型的清晰度。更好的方式是只传入需要用到的 Product 属性。 更重要的是，在将实体作为参数的值对象方法中，我们很难看出该方法是否会对实体进行修改，测试也将变得非常困难。 有了以上分析，我们需要对值对象进行改进，要增加一个值对象的健壮性，我们传给值对象方法的参数依然应该是值对象。这样我们可以获得更高层次的无副作用行为：1float priority = businessPriority.priority(product.businessPriorityTotals()); 这里，我们把 Product 实体的 BusinessPriorityTotals 值对象传递给了 priority() 方法。 如果打算使用编程语言提供的基本值对象类型，而不使用特定的值对象，我们是无法将领域特定的无副作用函数分配给编程语言提供的基元值对象的。有些真正简单的属性是没有必要特殊对待的。例如，一些布尔类型或数值类型，它们已经能够自给了，并不需要额外的功能支持，也并不和实体中的其他属性关联。这些简单的属性称为意义整体。 最小化集成当模型概念从上游上下文流入下游上下文时，尽量使用值对象来表示这些概念。这样做的好处是可以达到最小化集成，即最小化下游模型中用于管理职责的属性数目。 用值对象表示标准类型系统中既有表示事物的实体和描述实体的值对象，同时还存在「标准类型(Standard Type)」来区分不同的类型。假设通用语言中定义了一个 「PhoneNumber」 值对象，同时需要为每个 「PhoneNumber」 对象制定一个类型，用以区分家庭电话，移动电话，工作电话还是其他类型的电话号码。不同类型的电话号码类型需要建模成一种类的层级关系吗？为每一个类型创建一个类对于客户代码的使用来说是非常困难的。此时，你需要标准类型来描述不同的电话号码，比如 Home，Mobile，Work 或者 Other。 枚举类型是实现标准类型的一种简单方法。枚举提供了一组有限数量的值对象，它非常轻量且无副作用。通常来说，没有必要为标准类型提供描述信息，只需要名字就足够了。为什么？文本描述通常只在用户界面层中才会用到，此时可以用一个显示资源和类型名字匹配起来。很多时候用于显示的文本都需要进行本地化，因此将这种功能放在模型中并不合适。通常来说，在模型中使用标准类型的名字是最好的方式。 为了维护方便，最好是为标准类型创建单独的限界上下文。 有些标准类型所表达的概念不像是某种标准而更像是一种状态，此时标准类型实现为状态模式，但为每一种状态创建单独的类会使系统变得复杂。对于实体的状态类来说，有些行为来自于自身，有些继承自抽象基类，这一方面在子类和父类之间形成了紧耦合，另一方面使代码的可读性变差。如果你不打算使用状态模式，那么枚举可能是最简单的方法。 一个共享不变的值对象可以从持久化存储中获取，此时可以通过标准类型的「领域服务」或「工厂」来获取值对象。我们应该为每组标准类型创建一个领域服务或工厂(比如一个服务处理电话号码类型，一个服务处理邮寄地址类型，另一个服务处理货币类型)，服务或工厂将按需从持久化存储中获取标准类型，而客户方代码并不知道这些标准类型是来自数据库中的。另外，使用领域服务或工厂还使得我们可以加入不同的缓存机制，由于值对象在数据库中是只读的，并且在整个系统中是不变的，缓存机制也将变得更加简单安全。 总的来说，建议尽量使用枚举来表示标准类型，即便你认为某个标准类型更像一种状态模式。 实现通常来说，值对象至少包含两个构造函数，第一个构造函数接受用于构建对象状态的所有属性参数，称为主构造函数。该构造函数调用私有的 setter 方法初始化默认的对象状态，该私有的 setter 方法向我们展示了一种自委派性。 只有主构造函数才能使用自委派性来设置属性值，除此之外，其他任何方法都不能使用 setter 方法。由于所有的 setter 方法都是私有的，消费方是没有机会调用这些方法的，这是保持值对象不变性的两个重要因素。 第二个构造函数用于将一个值对象复制到另一个新的值对象，即复制构造函数。它将构造过程委派给主构造函数，先从原对象中取出各个属性，再将这些属性作为参数传给主构造函数。 复制构造函数对于测试来说是非常重要的，测试对象时，我们希望验证值对象的不变性，通过复制构造函数创建一个原实例的副本，验证两者的相等性。 持久化值对象以下着重讨论如何持久化包含值对象的聚合实例。聚合的读取和保存通过资源库完成。 有时，值对象需要以实体的身份进行持久化。换句话说，某个值对象实例会单独占据一张表中的某条记录，而该表也是专门为这个值对象类型而设计的，它甚至拥有自己的主键列。当面临「对象 - 关系阻抗失配」时，考虑以下几个问题： 我当前所建模的概念表示领域中的一个东西呢，还是只是用于描述和度量其他东西？ 如果该概念起描述作用，那么它是否满足值对象的几个特征？ 将该概念建模成实体是不是只是持久化上的考虑？ 将该概念建模成实体是不是因为它拥有唯一标识，我们关注的是对象实例的个体性，并且需要在其整个生命周期中跟踪其变化？ 我们不应该使持久化机制影响到对值对象的建模。无论使用什么技术来完成数据建模，数据库实体，主键，引用完整性和索引都不能用来驱动你对领域概念的建模。 单个值对象当实体包含单个值对象，值对象的属性需要和包含它的实体保存在一张数据表中时，其列名最好采用与数据库一致的形式，例如：123BusinessPriority.Ratings.Benefit=&gt;business_priority_ratings_benefit 值对象集合序列化到单个列中将一个 List 或 Set 的值对象保存在单个列中需要考虑以下问题： 列宽：有些对象集合可以包含任意多个元素，但数据库的列宽是有限制的。 查询：如果需要对该集合中的元素进行查询，无法用 SQL 语句实现，但从一个集合中查询一个或多个属性是比较少见的情况。 序列化器和反序列化器：需要自定义类型来实现序列化器和反序列化器，这只是增加了工作量。 使用数据库实体保存多个值对象我们不能因为某个概念非常符合数据库实体而将其建模成领域模型中的实体。有时，是对象 - 关系阻抗失配需要我们采用这种方法，但这绝非 DDD 原则。要实现这种方案，我们可以采用「层超类型」，或又名「委派身份标识(主键)」。下面的例子使用了两层层超类型：123456789public abstract class IdentifiedDomainObject: ISerializable&#123; private long _id = -1; protected long Id &#123; get =&gt; this._id; set =&gt; this._id = value; &#125;&#125; 接下来定义另一层层超类型，该层超类型是值对象专属的：1234public abstract class IdentifiedValueObject: IdentifiedDomainObject&#123;&#125; 虽然 IdentifiedValueObject 什么也不做，但它显式地表明了建模意图。IdentifiedValueObject 还应该有另外一个专属于实体的抽象子类 Entity。现在，每一个值对象类型都可以方便地获得一个隐藏的委派主键，我们可以自由地将其映射成数据库实体，而在领域模型中将其建模成值对象。 委派标识主要用于数据建模，其没有领域模型含义，这里更多是说明当实体包含值对象集合并且需要对其进行查询时如何对它们进行持久化，这样的值对象在数据库中会有一张单独的表，但这并不代表他们就是领域模型中的实体。 ORM 与枚举状态对象参考 《实现领域驱动设计》 P230]]></content>
      <categories>
        <category>架构与模式</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>ddd</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《实现领域驱动设计》读书笔记(4) - 战术建模之实体]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A8%A1%E5%BC%8F%2FDDD%2Fnote-ddd-tactical-entity%2F</url>
    <content type="text"><![CDATA[系列大纲: 《实现领域驱动设计》读书笔记 本文大纲: 唯一标识 委派标识 标识稳定性 实体及其本质特征 贫血领域模型 强类型实体标识 模型所扮演的角色 不变条件 验证 跟踪变化 当我们需要考虑一个对象的个性特征，或需要区分不同的对象时，我们引入实体这个领域概念。一个实体是一个唯一的东西，并且可以在相当长的时间内持续地变化。「唯一的身份标识」和「可变性特征」将实体和值对象区分开来。 常年进行 .NET 生态开发的人很容易把实体等同于 Entity Framework 中的 Entity，认为 Entity 就是数据库模型的对象映射，然而书中所说的实体完全是不同的概念，是面向业务领域的模型，纯数据模型而不具备任何行为的实体被称作「贫血领域模型」 唯一标识以下是常用的创建实体身份标识的策略，从简单到复杂依次为： 用户提供一个或多个初始唯一值作为程序输入，程序应该保证这些初始值是唯一的 程序内部通过某种算法自动生成身份标识 程序依赖于持久化存储，比如数据库来生成唯一标识 另一个限界上下文已经决定出了唯一标识，这作为程序的输入，用户可以在一组标识中进行选择 聚合根实体对象的唯一标识是全局唯一的，在同一个聚合中，一般实体的唯一标识只要和聚合内的其他实体区分开来即可。 将唯一标识的生成放在「资源库(Repository)」中是一种自然的选择 从数据库中获取标识比直接从应用程序中生成标识要慢得多，一种解决方法是将数据库序列缓存在应用程序中，比如缓存在资源库中。 有时，标识的生成和赋值时间对于实体来说是重要的，及早标识生成和赋值发生在持久化实体之前。延迟标识生成和赋值发生在持久化实体的时候 委派标识有些 ORM 工具通过自己的方式来处理对象的身份标识，如果我们自己的领域需要另外一种实体标识，此时两者将产生冲突。为了解决这个问题，需要使用两种标识，一种为领域所用，一种为 ORM 所用，在 Hibernate 中，被称为委派标识。委派标识与领域中的实体标识没有任何关系，委派标识只是为了迎合 ORM 创建的。 标识稳定性在多数情况下，我们都不应该修改实体的唯一标识，这样可以在实体的整个生命周期中保持标识的稳定性。 实体及其本质特征贫血领域模型过多拥有 getter 和 setter 方法而缺乏行为的模式可以概括为贫血领域模型。 强类型实体标识标识需要有特殊的类型还是可以使用简单的字符串？实体的唯一标识会用在很多地方，它可以用在不同限界上下文的所有实体上。在这种情况下，使用一个强类型的实体标识可以保证所有订阅方所持有的实体都能使用正确的标识。 这里所说的实体标识更多是指聚合根的实体标识？ 模型所扮演的角色在面向对象编程中，通常由接口来定义实现类的角色，在正确的设计情况下，一个类对于每一个它所实现的接口都存在一种角色。如果一个类没有显式的角色 - 即该类没有实现任何接口，那么默认情况下它扮演的即是本类的角色，也即，该类的公有方法表示该类的隐式接口。 不变条件不变条件是在整个实体生命周期中都必须保持事务一致性的一种状态，有时一个实体维护了一个或多个不变条件。如果实体的不变条件要求该实体所包含的所有对象都不能为 null，那么这些状态需要作为参数传递给构造函数，并且在相应的 setter 方法中对新值进行非 null 检查来确保一致性。 验证 自封装：无论从何处访问对象的状态，即使从对象内部访问数据，都必须通过 getter 和 setter 方法实现。 自封装首先为对象的实例变量和类变量提供了一层抽象。其次，我们可以方便地在对象中访问其所引用对象的属性。重要的是，自封装使验证变得非常简单。 验证的主要目的在于检查模型的正确性，我们将对模型进行三个级别的验证： 验证属性: 通过自封装的方式在 setter 方法中对属性进行验证 验证整体对象: 为了实现对整体对象的验证，可创建 Entity 层超类型，在其中定义 Validate 虚方法，实现类通过重写该方法按需调用验证逻辑，同时，由于验证逻辑的变化速度比实体本身还要快，所以应该将真正的验证逻辑委托给专门的验证类，实体在其 Validate 方法中使用这些验证类，从而使验证逻辑与实体解耦。 验证组合对象: 关注点从单个实体是否合法转向多个实体的是组合是否全部合法，包括一个或多个聚合实例。最好的方式是把这样的验证过程创建成一个领域服务，该领域服务通过资源库读取需要验证的聚合实例，然后对每个实例进行验证，可以是单独验证，也可以和其他聚合实例一起验证。 跟踪变化领域专家可能会关心发生在模型中的一些重要事件，此时就需要对实体的一些特殊变化进行跟踪了。跟踪变化最实用的方法是「领域事件」和「事件存储」。可以为领域专家所关心的所有状态改变都创建单独的事件类型，事件的名字和属性表明发生了什么样的事件。当命令操作执行完后，系统发出这些领域事件，订阅方接收发生在模型上的所有事件。接收到事件后，订阅方将事件保存在事件存储中。]]></content>
      <categories>
        <category>架构与模式</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>ddd</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《实现领域驱动设计》读书笔记(3) - 战略建模之架构]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A8%A1%E5%BC%8F%2FDDD%2Fnote-ddd-strategic-archietecture%2F</url>
    <content type="text"><![CDATA[系列大纲: 《实现领域驱动设计》读书笔记 本文大纲: 分层架构 六边形架构(端口与适配器架构，洋葱架构) 依赖倒置原则 面向服务架构(Service-Oriented Architecture, SOA) REST(Representational State Transfer) RESTful HTTP 服务端的关键方面 REST 和 DDD 命令与查询职责分离 - CQRS 客户端和查询处理器 查询模型 事件驱动架构 管道和过滤器 长时处理过程(Saga) 事件源(EventSource) 数据网织和基于网格的分布式计算 分层架构分层架构的一个重要原则是: 每层只能与位于其下方的层发生耦合。「严格分层架构」只允许某层与直接位于其下方的层发生偶尔，而「松散分层架构」则允许任意上方层与任意下方层发生耦合。由于「用户界面层」和「应用服务层」经常需要与基础设施打交道，许多系统都是基于「松散分层架构」的。 在分层架构中，领域的核心域通常只位于架构的其中一层，「用户界面层」和「应用服务层」均位于其上。 根据笔者的理解，用户界面层对应所有对系统产生消费行为的客户端，可能是人也可能是其他系统，用户界面是应用层的直接消费方。 有人认为既然用户界面需要对用户输入进行验证，那么它就应该包含业务逻辑。事实上，用户界面进行的验证和领域模型的验证是不同的，在用户界面中使用的只是数据的渲染和展现，而领域模型的验证的关注点却跟一致性有关，此时可以使用展现模型将用户界面与领域模型解耦。 「应用服务」位于应用层中，「应用服务」和「领域服务」的职责是不同的，后续的文章专门针对两者进行了讨论。 领域逻辑不应该出现在应用服务中，应用服务可以用于控制持久化事务和安全认证，或者向其他系统发送基于事件的消息通知，另外还可以用于创建邮件以发送给用户。应用服务本身并不处理业务逻辑，但它是领域模型的直接消费者，它主要用于协调领域对象的操作，应用服务是很轻量的。比如聚合。同时，应用服务是表达用例和用户故事的主要手段。因此，应用服务通常的用途是: 接收来自用户界面的输入参数，再通过资源库获取到聚合实例，然后执行相应的操作，例如:1234567891011121314@Transactionalpublic void CommitBacklogItemToSprint(string tenantId, string backlogItemId, string sprintId)&#123; // construct wanted id objects. var tenantId = new TenantId(tenantId); var backlogItemId = new BacklogItemId(backlogItemId); // get backlogItem object from repository. var backlogItem = backlogItemRepository.BacklogItemOfId(tenantId, backlogItemId); var sprintId = new SprintId(sprintId); var sprint = sprintRepository.SprintOfId(tenantId, sprintId); // call the commit method from backlogItem. backlogItem.CommitTo(sprint);&#125; 上述代码很好的诠释了前文提及的关于应用服务「协调领域对象的操作」的功能 如果应用服务比上述功能复杂许多，这通常意味着领域逻辑已经泄露到应用服务中了，此时的领域模型将变成「贫血领域模型」。因此，最佳实践是将应用服务做成很薄的一层。 六边形架构(端口与适配器架构，洋葱架构)依赖倒置原则 高层模块不应该依赖于低层模块，两者都应该依赖于抽象 抽象不应该依赖于细节，细节应该依赖于抽象。 当传统的分层架构引入了依赖倒置原则，会发现已经不存在分层的概念了，无论是高层还是低层都依赖于抽象，好像把整个分层架构推平了。在六边形架构中，不同的消费者通过「对等」的方式与系统交互，当需要新增消费者时，只需添加一个新的适配器将客户输入转化成能被系统 API 所理解的参数就行了。同时，系统输出，例如「图形界面」，「持久化」和「消息」等都可以通过不同方式实现，并且是可替换的，对于每种特定的输出，都有一个新的适配器负责完成相应的转化功能。 六边形架构提倡用「内部区域」和「外部区域」来看待整个系统，在外部区域中，不同的客户代码提交输入，内部系统用于获取持久化数据，并对程序输出进行存储，或在中途将输出转发到另外的地方(比如消息)。 依据笔者理解，端口和适配器的意思是，将系统想象成一般的计算机，HTTP 协议和 AMPQ 协议以及用户界面可看作不同的端口，而适配器则负责将来自这些协议的数据转化成系统 API 能够理解的数据。 在使用六边形架构时，我们应该根据用例来设计应用程序，而不是根据需要支持的客户数目来设计。任何客户都可能向不同的端口发出请求，但是所有的适配器都将使用相同的 API。 应用程序位于六边形架构的「内部区域」，公共 API 通过「应用服务」暴露给外部区域，而如前文所述，应用服务是领域模型的直接消费者，所有的输入都将委派给内部的领域对象。 我们可以将资源库的实现看作是持久化适配器，该适配器用于访问先前存储的聚合实例，或者保存新的实例，我们可以通过不同的方式实现资源库，如关系型数据库，文档型数据库以及内存数据库，他们分别对应着不同的适配器，但服务于同一种端口——持久化，即同一个端口可以有多种适配器。 六边形架构的好处在于易于测试，整个应用程序和领域模型可以在没有客户和存储机制的条件下进行设计开发。基于六边形架构，可以扩展为 SOA，REST，事件驱动架构，CQRS 架构或者数据网织或基于网格的分布式缓存，还有可能 Map-Reduce 这种分布式并行处理方式。 面向服务架构(Service-Oriented Architecture, SOA)服务的设计原则如下: 服务契约: 通过契约文档，服务阐述自身的目的与功能 松耦合: 服务将依赖关系最小化 服务抽象: 服务只发布契约，而向消费方隐藏内部逻辑 服务重用性: 一种服务可以被其他服务重用 服务自治性: 服务自行控制环境与资源以保持独立性，这有助于保持服务的一致性和可靠性 服务无状态性: 服务负责消费者的状态管理，但不能与服务的自治性发生冲突 服务可发现性: 消费方可以通过服务元数据来查找服务和理解服务 服务组合性: 一种服务可以由其他服务组合而成，而不管其他服务的大小和复杂性如何 这些原则可以与六边形架构结合起来，此时服务边界位于最左侧，而领域模型位于中心位置，消费方可以通过 REST，SOAP 和消息机制获取服务。 业务服务可以由任意数量的技术服务来提供，技术服务可以是 REST 资源，SOAP 接口或消息类型。业务服务强调业务战略，即如何对业务和技术进行整合。 REST(Representational State Transfer)REST 既不是使用 HTTP 直接发送 XML/JSON，也不是将 URI 的查询参数传递给方法。REST 是一种架构风格，架构风格之于架构就像设计模式之于设计一样，它将不同架构实现共有的东西抽象出来，使得我们在谈论架构时不至于陷入技术细节中。分布式系统架构存在多种架构风格，包括客户端-服务器架构风格和分布式对象(例如远程过程调用)风格。REST 是 Web 架构的一种架构风格，和其他技术一样，我们可以通过不同的方式来使用 Web 协议，有些使用方式符合设计者的初衷，而有些则不然。例如，你可以使用关系型数据库管理系统(RDBMS)创建表，列，外键关联，视图和约束等，你也可以只创建一张包含两列的表，一列表示‘键’，一列表示‘值’，然后将序列化之后的对象保存在值列中。此时，你依然在使用 RDBMS，但你却使用不到多少 RDBMS 的功能，如查询，组合，排列和分组等。 同理，Web 协议既可以按照它的设计初衷为人所用——此时便是一种遵循 REST 架构风格的方式——也可以通过一种不遵循其设计初衷的方式为人所用。因此，当我们没有足够充分的理由使用 REST 风格的 HTTP 所带来的好处时，采用另一种分布式系统架构可能是合适的，就像在保存拥有唯一键的数值时，NoSQL 键值对存储方式是一种更好的选择一样。 RESTful HTTP 服务端的关键方面「资源」是关键的概念，系统的设计者将决定哪些有意义的「东西」可以暴露给外界，并且给这些「东西」一个唯一的身份标识。通常来说，每种资源都拥有一个 URI，每个 URI 都需要指向某个资源。 另一个关键方面是「无状态通信」，消息是自描述的，例如，HTTP 请求本身便包含了服务端所需要的全部信息，服务端可以使用其本身的状态来辅助通信，重要的是: 我们不能依靠请求本身来创建一个隐式上下文环境(会话)。无状态通信保证了不同请求之间的相互独立性，这在很大程度上提高了系统的可伸缩性。 如果将资源看作对象，那么每一个对象都支持相同的接口，可以调用的方法是一个固定的集合，它们全都可以用 HTTP 动作表示，其中最重要的有 GET，PUT，POST 和 DELETE。这也是将 REST 与其他架构风格区别开来的关键。虽然乍一看这些方法将会转化成 CRUD 操作，但通常我们所创建的资源并不表示任何持久化实体，而是封装了某种行为，当调用 HTTP 动词对应的操作时，实际上是在调用这些行为。 依据笔者理解，对象化的资源并不代表任何领域模型中的实体，而是根据某一项业务操作抽象出来的资源块，其中包括用以展示的数据和具有一致性边界的行为。 在 HTTP 规范中，每种 HTTP 方法都有一个明确的定义，比如 GET 方法只能用于「安全」的操作: 它可能完成一些客户并没有要求的动作行为 它总是读取数据 它可能被缓存起来 最后，通过使用 HATEOAS(Hypermedia as Engine of Application State)，REST 服务的消费方可以沿着某种路径发现应用程序可能的状态变化。简单来讲，就是单个资源并不独立存在，不同资源是相互链接在一起的，对于服务器来说，这意味着在返回中包含对其他资源的链接，由此消费方便可通过这些链接访问到相应的资源。 REST 和 DDD不建议将领域模型直接暴露给外界，这样会使系统接口变得非常脆弱，领域模型的任何改变都会导致系统接口的改变。要将 DDD 与 RESTful HTTP 合并起来使用，我们有两种方式。 第一种方法是为系统接口层单独创建一个限界上下文，再在此上下文中通过适当的策略来访问核心模型，这是一种经典的方法，它将系统接口看作一个整体，通过资源抽象将系统功能暴露给外界，而不是通过服务或远程接口。这种方法让核心域和系统接口之间完成了解耦。 另一种方法用于需要使用标准媒体类型的时候。如果某种媒体类型并不用于支持单个系统接口，而是用于一组相似的客户端-服务器交互场景，此时可以创建一个领域模型来处理每一种媒体类型。这种方法本质上为 DDD 中的共享内核或发布语言。 这里提到的媒体类型表示 MIME type。 通常来讲，添加新资源并在已有资源中创建到新资源的链接是非常简单的，要添加新的格式也同样如此。另外，基于 REST 的系统也是非常容易理解，系统被分为很多较小的资源块，每一个资源块都可以独立测试和调试。HTTP 设计本身以及 URI 成熟的重写与缓存机制使得 RESTful HTTP 成为一种不错的架构选择，该架构具有很好的松耦合性和可伸缩性。 命令与查询职责分离 - CQRS从「资源库」中查询所有需要显示的数据是困难的，特别是在需要显示来自不同聚合类型与实例的数据时，领域越复杂，这种困难越大。一种被软件系统广泛采用的做法是使用「数据传输对象(Data Transfer Object, DTO)」，即从不同的资源库中获取聚合实例，然后再将它们组装成 DTO。 然而，查询这些数据所带来的性能消耗可能会随着数据量增大而显著降低，另外一种办法是使用 「CQRS(Command-Query Responsibility Segregation)」。CQRS 是将紧缩(Stringent)对象(或组件)设计原则和命令-查询分离(CQS)应用在架构模式中的结果。 一个方法要么是执行某种动作命令，要么是返回数据的查询，而不能两者皆是。换句话说，问题不应该对答案进行修改。一个方法只有在具有参考透明性的时候才能返回数据，此时该方法不会产生副作用。 [Bertrand Meyer] 在对象层面，这意味着: 如果一个方法修改了对象的状态，该方法便是一个命令(Command)，它不应该返回数据，在 Java 和 C# 中，这样的方法应该声明为 void 如果一个方法返回了数据，该方法便是一个查询(Query)，此时它不应该通过直接或间接的手段修改对象的状态，在 Java 或 C# 中，这样的方法应该以其返回的数据类型进行声明 在领域模型中，我们通常会看到同时包含命令和查询的聚合，也经常在资源库中看到不同的查询方法，这些方法对对象属性进行过滤。但在 CQRS 中，我们忽略这些常态的情形，而是通过另一种方式来查询用于显示的数据。 假设，一个聚合不再有查询方法，只有命令方法，资源库也将变成只有 Add() 或 Save() 方法(分别支持创建和更新操作)，同时只有一个查询方法，如 FromId()，这个唯一的查询方法以聚合 ID 作为参数，然后返回该聚合实例。资源库不能使用其他方法来查询聚合，比如对属性进行过滤等。在将所有查询方法移除之后，我们将此时的模型称为「命令模型(Command Model)」，但我们仍然需要向用户显示数据，为此我们将创建第二个模型，该模型专门用于优化查询，称之为「查询模型(Query Model)」。 你可能会认为: 这种架构风格需要大量的额外工作，我们解决了一些问题，同时带来了另外的问题，而且我们需要编写更多的代码。但无论如何，不要急于否定这种架构，在某些情况下，新增的复杂性是合理的。 因此，领域模型被一分为二，最终得到如下图所示的系统组件: 客户端和查询处理器客户端可以是 Web 浏览器，也可以是桌面应用程序，它们将使用运行在服务器端的一组查询处理器。查询处理器表示一个只知道如何向数据库执行基本查询并将查询结果以某种格式返回的简单组件。 查询模型查询模型是一种非规范化数据模型，它并不反映领域行为，只是用于数据显示。如果数据模型是 SQL 数据库，那么每个数据库表视图便是一种数据显示视图，它可以包含很多列，甚至是所有数据的超集，表视图可以通过多张表进行创建，此时每张表代表整个显示数据的一个逻辑子集。 todo.. 事件驱动架构事件驱动架构不见得必须与六边形架构一同使用，但引入六边形架构有助于理解事件驱动架构。 管道和过滤器考虑以下地 shell 命令，它便是一种最简单的管道和过滤器:1cat phone_numbers.txt | grep 303 | wc -l 这行 Linux 命令用于在 phone_numbers.txt 文件中统计含有电话区号 “303” 的所有文本行的数量: cat 命令向标准输出流输出 phone_numbers.txt 文件中的内容。通常来说，标准输出流与终端相连，但其后连接了 “|” 符号，输出会通过管道转向下一个命令工具。 grep 命令从标准输出流中读取数据，即从 cat 命令输出的内容，并匹配含有 “303” 的所有行，并将查找到的结果通过管道转向下一个命令工具。 最后，wc 命令读取标准输出流，即 grep 命令的输出结果，其参数 -l 表示统计所读行的数量。由于在 wc 命令之后没有另外的管道了，此时的输出直接显示在终端。 每个工具都接收一个数据集，对其进行处理，再输出另一个数据集，每一个命令都充当着过滤器的作用。在整个过滤过程完成之后，输入数据和输出数据可能完全不一样了，本例中，原始输入数据是一个文本文件，最终输出的只有一个数字 “3”。 领域模型中发布领域事件的组件便可类比这里的过滤器，而领域事件则可类比为管道。 长时处理过程(Saga)todo.. 事件源(EventSource)事件源是指: 对于某个聚合上的每次命令操作，都有至少一个领域事件发布出去，该领域事件描述了操作的执行结果。每一个领域事件都将被保存到「事件存储」中，每次从资源库中获取某个聚合时，我们将根据发生在该聚合上的历史事件来重建该聚合实例，事件的作用顺序与它们的产生顺序相同。 随着时间推移，发生在聚合实例上的事件越来越多，那么，重放这些成百上千的事件会对那些操作繁忙的模型造成影响，为了避免这种瓶颈，我们可以通过聚合状态「快照」的方式来进行优化。可以创建一个聚合内存状态的快照，此时的快照反应了聚合在事件存储历史中某个事件发生后的状态。为了达到这样的目的，我们需要利用该事件及其发生前的所有事件来重建聚合实例，之后对聚合状态进行序列化，再把序列化之后的快照保存在事件存储中。这样，便可通过聚合快照来实例化某个聚合，接着再重放比快照更新的事件来修改聚合的状态，直至读取时发生在聚合上的最后一个事件。 创建快照所需的前置事件数量临界值可以由团队确立，例如，发现某个聚合在接收到 50 个事件之后为其创建快照可以获得最佳性能，那么 50 就是其临界值。 事件通常以二进制的方式保存在事件存储中，这使得事件源不能用于查询操作。事实上，为事件源所设计的资源库只有一个接受聚合 ID 的查询方法，因此需要另外的方法来支持查询，通常将 CQRS 和事件源一同使用。 数据网织和基于网格的分布式计算todo..]]></content>
      <categories>
        <category>架构与模式</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>ddd</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《实现领域驱动设计》读书笔记(2) - 战略建模]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A8%A1%E5%BC%8F%2FDDD%2Fnote-ddd-strategic%2F</url>
    <content type="text"><![CDATA[系列大纲: 《实现领域驱动设计》读书笔记 本文大纲: 通用语言(Ubiquitous Language) 领域和子域 核心域 支撑子域 通用子域 问题空间(Problem Space)和解决方案空间(Solution Space) 限界上下文(Bounded Context) 上下文映射图 通用语言(Ubiquitous Language)通用语言是团队成员间能够互相理解的语言，它统一了开发团队和领域专家之间的术语，从而提高了沟通效率。通用语言不同于「统一建模语言(UML, Unified Modelling Language)」，UML 是开发人员之间的语言。 通用语言的几点注意： 限界上下文和通用语言存在一对一的关系 只有当团队工作在独立的限界上下文中时，通用语言才是 “通用”的 虽然我们只工作在一个限界上下文中，但我们可能经常会与其他限界上下文打交道，这时可以通过上下文映射图来对这些限界上下文进行集成，每个限界上下文都有自己的通用语言，而有时语言间的术语可能有重叠的部分 领域和子域在 DDD 中，一个领域可能包含多个限界上下文，通常一个限界上下文对应一个子域。“领域”一词承载了太多含义，领域既可以表示整个业务系统，也可以表示其中的某个核心域或支撑子域，一个模型在一个子域应该表达出清晰的含义。 例如，在一个电子商务系统中，至少要向买家展示不同类别的产品，允许买家下单和付款，还需要安排物流。这个特定的领域可以划分为「产品目录(Product Catalog)」，「订单(Order)」，「发票(Invoicing)」和「物流(Shipping)」。子域不一定会很大，可以简单到只包含一套算法，这套算法对业务系统来说可能非常重要，但并不包含在核心域之中。在正确实施 DDD 的情况下，这种简单的子域可以以「模块(Module)」的形式从核心域中分离出来。 核心域通常代表了促进业务成功的核心功能，核心域应该配备最好的领域专家和开发团队。 支撑子域如果一个限界上下文对应着业务的某些重要方面，但却不是核心，那么它便是一个「支撑子域」。 通用子域如果一个子域被用于整个业务系统，那么这个子域便是「通用子域」。例如在众多系统中都有的「认证与授权子系统」通常就是一个通用子域。 问题空间(Problem Space)和解决方案空间(Solution Space) 问题空间是领域的一部分，对问题空间的开发将产生一个新的核心域，对问题空间的评估应该同时考虑已有子域和额外所需子域。因此，问题空间是核心域和其他子域的组合。 解决方案空间包括一个或多个限界上下文，因为限界上下文即是一个特定的解决方案。 在我们实施某个解决方案之前，我们需要对问题空间和解决方案空间进行评估，首先回答以下问题： 这个战略核心域的名字是什么？ 它的目标是什么？ 它包含哪些概念？ 它的支撑子域和通用子域是什么？ 如何安排项目人员？ 你能组建一支合适的团队吗？ 解决方案空间在很大程度上受到现有系统和技术的影响。我们应该根据限界上下文仔细考虑以下问题： 有哪些软件资产是已经存在的，它们可以重用吗？ 哪些资产是需要创建的，或者从别处获得？ 这些资产是如何集成在一起的？ 还需要什么样的集成？ 假设已经有了现有资产和那些需要被创建的资产，我们还需要做些什么？ 核心域和那些支撑项目的成功几率如何？会不会出现由于其中一个失败而导致整个项目失败的可能？ 有哪些地方我们使用了完全不同的术语？ 限界上下文之间在哪些地方存在概念重叠？ 这些重叠的概念在不同的限界上下文之间是如何映射和翻译的？ 哪些限界上下文包含了核心域中的概念，其中使用了哪些战术建模工具？ 限界上下文(Bounded Context)限界上下文采用 模型+上下文 的形式来命名。 同一个概念在不同的限界上下文中的关注点是不一样的，例如在一个电子商务系统中，”顾客” 这个概念在订单系统上下文中，其关注点有先前购买情况，忠诚度，可买产品，折扣和物流方式，而在下单时，”顾客” 的上下文包括名字，产品寄送地址，订单总价和一些付款术语。所以 “顾客” 在这个例子中并没有一个清晰的含义。类似的问题其实是脱离了不同限界上下文中协作概念的关注点，在不同的限界上下文中，”顾客” 扮演了不同的协作概念，例如在产品目录上下文中，”顾客” 可以用 “浏览者” 表示，而在订单上下文中，”顾客” 以 “购买者” 表示。”顾客” 一词本身包含了太多可能的角色，不同的角色有不同的职责。在不同的限界上下文中，不同角色充当了 “顾客” 的某种角色，进而使得单一限界上下文该角色的含义清晰，并与其他限界上下文的关注点得以分离。 限界上下文是一个显式的语义边界，领域模型便存在于这个边界之内。领域模型把通用语言表达成软件模型，创建边界的原因在于，每一个模型概念，包括它的属性和行为，在边界之内都具有特殊的含义。限界上下文并不旨在创建单一的项目资产，它并不是一个单独的组件，文档或者框图。因此，它并不是一个 jar 或者 dll，但这些可以用来表示限界上下文。 在上下文边界之外，我们通常不会使用该上下文之内的对象实例，但是不同上下文中彼此关联的对象可能共享一些状态。 一个限界上下文并不是只包含领域模型。模型自然是限界上下文的一等公民，但它并不局限于此。它通常标定了一个系统，一个应用程序或者一个业务服务。有时一个限界上下文包含的内容可能比较少，例如，一个通用子域可能只包含领域模型。 当模型驱动着数据库 Schema 的设计时，数据库 Schema 也应该位于该模型所处的上下文边界之内。因为数据库 Schema 是由建模团队设计，开发并维护的。这也意味着数据库中表和列的名字应该和模型的名字保持一致。另一方面，如果数据库 Schema 已经存在，或者另有一个专门的数据建模团队要求有别于模型的数据库 Schema 设计，此时的 Schema 便不能和模型位于同一个限界上下文中了。 如果「用户界面(UI)」被用于渲染模型，并且驱动着模型的行为设计时，同样，该用户界面也应该属于模型所在的上下文边界之内。但是，这并不表示我们应该在用户界面中对领域进行建模，因为这样将导致「贫血领域对象」或者任何试图将领域概念带到领域模型之外的举措。 通常情况下，一个系统/应用程序的使用者并不只是人，还可能是另外的计算机系统。系统中有可能存在诸如 Web 服务之类的组件，或者使用 REST 资源来与模型交互，在所有可能的情形下，这些面向服务的组件都应该位于上下文边界之内。 用户界面和面向服务的端点都会将操作委派给「应用服务(Application Service)」，应用服务包含了不同类型的服务，比如安全和事务管理等。对于模型来说，应用服务扮演的是一种门面模式(Facade)。同时，应用服务还具有任何管理功能，它将来自用例流(Use Case Flow)的请求转换成领域逻辑的执行流。应用服务也是位于上下文边界之内的。 限界上下文主要用来封装通用语言和领域对象，但同时它也包含了那些为领域模型提供交互手段和辅助功能的内容。需要注意的是，对于架构中的每个组件，我们都应该将其放在适当的地方。限界上下文可以包含「模块(Module)」，「聚合(Aggregate)」，「领域事件(Domain Event)」和「领域服务(Domain Service)」。限界上下文应该足够大，以表达它所对应的整套通用语言。 上下文映射图上下文映射图表示了不同限界上下文之间是如何集成的，任何两个限界上下文可能存在某种模式： 合作关系(Partnership): 两个团队各自负责自己的上下文，在接口的演化上进行合作以同时满足两个系统的需求 共享内核(Shared Kernel): 两个上下文对模型和代码的共享产生一种紧密的依赖性，需要为共享的部分指定一个显式的边界，并保持共享内核的最小化。在没有与另一个团队协商的情况下，共享内核是不能改变的。应该引入一种持续集成机制来保证共享内核与通用语言的一致性。 客户方-供应方开发(Customer-Supplier Development): 两个上下文处于上-下游关系，上游团队独立于下游团队完成开发，下游团队的开发可能会受到很大的影响。因此在上游团队的计划中，应该顾及下游团队的需求。 遵奉者(Confirmist): 在存在上-下游关系的两个团队中，上游团队完全不考虑下游团队的需求，而下游团队只能盲目地使用上游团队的模型。 防腐层(Anticorruption Layer): ACL，当两个上下文不是合作，共享内核或者客户-供应方关系时，翻译将变得复杂。下游团队需要根据自己的领域模型创建一个单独的层，该层作为上游系统的代理提供功能。防腐层通过已有的接口与其他系统交互，在防腐层内部，在自己的模型和他方的模型之间进行翻译转换。如果翻译过于复杂，并且需要大量的数据复制和同步，从而使得翻译前后的模型存在很大的相似度，那么你可能过多地使用了外部上下文中的数据，导致自己的模型混淆不清了。 开放主机服务(Open Host Service): OHS，定义一种协议，其他系统通过该协议来访问该系统，协议是公开的，这样任何想与这个系统集成的人都可以使用该协议。通常来讲，我们可以将开发主机服务看成是远程过程调用 (Remote Procedure Call) 的 API。同时，它也可以通过消息机制实现。 发布语言(Published Language): PL，在两个限界上下文之间翻译模型需要一种公用的语言。此时你应该使用一种发布出来的共享语言来完成集成交流。发布语言通常与开放主机服务一起使用，常见的发布语言使用 XML Schema。在使用 REST 服务时，可以使用 XML 和 JSON，也可以使用 Google Proto Buffer 来表示。使用 REST 的好处是每个客户端都可以指明使用哪种语言，同时还可以指明资源的展现方法。 另谋他路(SeperateWay): 如果两套系统之间没有任何显著的关系，那么他们是完全解耦的，集成总是昂贵的。 大泥球(Big Ball of Mud): 当我们检查已有系统时，经常会发现系统中存在混杂在一起的模型，它们之间的边界非常模糊，此时应该为整个系统绘制一个边界，然后将其归纳在大泥球范围之列。 系统间集成经常依赖于 RPC。RPC 与编程语言中的过程调用非常相似。和在相同进程中的过程调用不同的是，RPC 更容易产生有损性能的时间延迟，并有可能导致调用彻底失败。虽然 REST 并不是真正意义上的 RPC，但它却具有与 RPC 相似的特征。如果系统所依赖的状态已经存在于本地，那么系统将获得更大的自治性。DDD 的做法是在本地创建一些由外部模型翻译而成的领域对象，这些对象保留着本地模型所需的最小状态集。为了初始化这些对象，我们只需要有限的 RPC 调用或 REST 请求。然而，要与远程模型保持同步，最好的方式是在远程系统中采用面向消息的通知机制(例如 RabbitMQ)。消息通知可以通过服务总线进行发布，也可以采用消息队列或者 REST。]]></content>
      <categories>
        <category>架构与模式</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>ddd</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《实现领域驱动设计》读书笔记(1) - 总览]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A8%A1%E5%BC%8F%2FDDD%2Fnote-ddd-overview%2F</url>
    <content type="text"><![CDATA[系列大纲: 《实现领域驱动设计》读书笔记 本文大纲: 总览 领域模型 什么是领域模型？ 前言面向对象编程的博大精深至今只浅尝一二，开发实践中一直希望能够持久向好的设计方向上靠而不过度设计。2015 年时第一次接触到领域驱动设计，当时看了 《实现领域驱动设计》这本书，初看时晦涩难懂，许多概念与当时的理解存在很大偏差，一度不理解为何要那样设计。在经过了几个项目的实战之后，如今再翻出来看，对书中总结的思考方式和方法论有了一番新的体会。现在看来，领域驱动设计之所以难以理解，在于其方法论的概念是抽象于任何语言和技术实现的，长期工作于一线的开发人员要转换思维去理解类似「值对象」，「标准类型」等这样的概念，需要一些时间。 一谈到「领域驱动设计」或者 DDD，总是很难通过一句话来概括，它既不是设计模式，也不代表任何技术实现，仅仅是面向对象程序设计的一种方法论，其中涵盖了在任何软件系统中可能涉及的方方面面。 领域模型什么是领域模型？领域模型是关于某个特定业务领域的软件模型。通常，领域模型通过对象来实现，这些对象同时包含了「数据」和「行为」，并且表达了准确的业务含义 ——《领域驱动设计》 全书由高层视角深入到实现的细枝末节来组织章节，其索引大致为： 战略建模 通用语言(Ubiquitous Language) 领域，子域和核心域 限界上下文(Bounded Context) 上下文映射图(Context Mapping) 架构(Archiecture) 战术建模 实体(Entity) 值对象(Value Object) 领域服务(Domain Service) 领域事件(Domain Event) 模块(Module) 聚合(Aggregate) 工厂(Factory) 资源库(Repository) 集成限界上下文(Integrating Bounded Contexts) 应用程序 这些不同的内容会在后续的笔记中一一提到，只取我觉得有价值的部分记录下来。设计一个系统时所需要的所有建模工具都能在以上这些概念中找到，并且针对这些应用场景提出了指导意见和最佳实践。]]></content>
      <categories>
        <category>架构与模式</category>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>ddd</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAuth 2.0 和 OpenID Connect]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Foauth2-and-openid-connect%2F</url>
    <content type="text"><![CDATA[参考资料: OAuth 2.0 Debugger OpenID Connect Debugger Youtube Video - OAuth 2.0 and OpenID Connect OAuth 2.0 简介在 2010 年以前，大多数的身份认证场景都在想办法解决一个问题，如何让用户在不输入密码的前提下授权第三方访问用户的数据？例如使用 Google，Facebook，微信帐号授权登录操作，这就是 OAuth 诞生的背景。 2010 年以前的 Identity 案例 Simple Login: 表单和 cookies SAML 协议: 一个帐号，可在多个系统中登录 Mobile app login: ??? Delegated Authorization: ??? OAuth 2.0 用于委托授权假设用户 A 有一个 Google 帐号，A 信任 Google。现在 A 想要访问 Yelp 网站，Yelp 网站支持使用 Google 帐号进行授权登录，当 A 点击某个 Connect to Google 按钮开始，OAuth Flow 便开始了: 用户 A 点击 Connect to Google 按钮 重定向至 Google Authorization Service，该服务让用户 A 首先验证其在 Google 的身份，或体现为帐号密码输入框，用户 A 将用户密码给了 Google 而不是 Yelp，这令用户 A 觉得安全一些。 成功登录后，Google Authorization Service 询问用户 A 说 Yelp 想要访问他的 Google 个人资料(Profile)和联系清单(Contacts)，是否同意授权？ 用户 A 选择否，授权中止；用户 A 选择是，Google 将用户重定向至之前的登录页面，并成功获取到用户 A 在 Google 的个人资料。 OAuth 2.0 术语对照 Resource Owner(用户 A，数据的主人) Client(Yelp，想要获得访问用户 A 数据授权的机构) Authorization Server(授权服务器，形如 accounts.google.com) Resource Server(用于取得用户 A 资料的 Google Profile API 或 Google Contacts API) Authorization Grant(用户 A 的授权处理过程) Redirect URI(Authorization Server 需要知道用户同意授权后的回调地址) Access Token(Client 最终用来向 Resource Server 获取用户数据的凭证) Authorization Code 模式 Yelp 在 Connect to Google 按钮的链接上填写 Google Authorization Service 定义的格式，并在链接中携带回调 Uri，Scope，Grant Type 及其他配置信息 用户 A 在 Google Authorization Service 返回的页面输入帐号密码成功登录并同意授权 Google Authorization Service 向浏览器返回一个带有 code 查询参数的 uri 并执行重定向。 该 uri 指向 Yelp 的服务端，服务端使用该 code 并结合从 Google 获取的 API Secret 再次访问 Google Authorization Service 换取一个 access token，code。 在得到 access token 之后，Yelp 服务端利用该 token 并结合 API Secret 向 Google Profile API 请求用户 A 的数据。 ScopeScope 是 Authorization Server 系统中预定义的细粒度权限等级，此处 Yelp 仅仅想要读取 Profile 信息，但它不能删除 Contacts 信息，所以读取和修改 Profile 就是不同的 Scope。Yelp 作为客户端请求授权时需要提供它想要获得授权的 Scope，以让 Google Authorization Service 在询问用户时向其展示 Yelp 想要访问哪些数据。用户同意授权之后 Google Authorization Service 返回给 Yelp 的 code 最终换取的 access token 也就被限制在这些 Scope 中。 Q&amp;A: 为何不直接返回 access token 而是首先返回 code，再去交换 access token？Back Channel 和 Front Channel Back Channel: 由系统的后台服务向另一个系统的 API 发起的请求，被认为是 Back Channel，由于开发人员对后台程序代码具有完全的控制权，且外部人员无法轻易取得访问服务器的权限，所以这种方式具有更高的安全性。 Front Channel: 浏览器，由于浏览器是一个开放的代码执行平台，任何人都可以通过 View Source 查看源代码，其主要数据和信息都用于展示，所以比起 Back Channel，敏感数据不应该走 Front Channel。 浏览器向 Google 发起的授权请求的所有查询参数和 url 都是公开透明的，而 Google Authorization Service 返回的也是一个包含 code 查询参数的 url，它最终由浏览器执行重定向操作。如果此时有人拦截了该 code，并试图先于 Yelp 换取 access token 就会发生数据泄漏。但实际情况是，浏览器在获取到包含 code 的 uri 后，该 uri 指向了 Yelp 的某个后台服务，该后台服务再使用该 code 并结合存储在服务端后台的 API Secret 一起去换取 access token。即便有人拦截了 code 的值，在不知道这个 Secret 的前提下，是无法换取 access token 的，所以换取 token 这一步操作走 Back Channel。由 Access Token 请求用户数据也必须由服务端走 Back Channel，且请求类型为 POST。 OAuth 2.0 Grant Type Authorization Code(front channel + back channel): 上述讨论的场景，首先返回 code，再以 code 去后台交换 token Implicit(front channel only): 单页应用的场景，无后台服务端，与 Code 模式的区别在于，直接返回 token，不走 Back Channel，这种方式有 token 被盗的风险，app 要确保 token 的安全。 Resource owner password credentials(back channel only): 不推荐 Client credentials(back channel only): 常见于 machine to machine 或 service to service 的场景，即服务端保存了用户的认证凭证。 OpenID Connect 和 OAuth 2.0 结合用于认证OAuth 在设计之初并不是为了做认证，因为 OAuth 没有提供获取用户信息的标准方式，OAuth 被用于用户认证是业界对其滥用的结果。而 OpenID Connect 则是基于 OAuth 2.0 的扩展认证协议，OpenID Connect 对 OAuth 2.0 做了以下扩展: ID Token: 包含了用户的 ID 或其他代表该用户信息的 token 用户信息接口用于获取详细的用户信息 标准化的 Scope 集合 标准化实现 OpenID Connect 的流程中 Authorization Server 在返回中增加了 ID Token，而 ID Token 以 JWT(JSON Web Token) Token 被熟知，它以标准化的方式将一系列信息加密，并在网络中传输。JWT 由三部分组成: Header: 包含用户身份的基本信息，如 Username，Email 等。 Payload: 有时又称作 Claims，该值可由应用程序解码得到类似，哪个用户登录了，何时登录的，过期时间是什么时候等 Signature: 用于验证 JWT 没有被修改。 总结以下情况使用 OAuth 2.0: 为请求 API 的客户端分配权限 获取其他系统的用户数据 以下情况使用 OpenID Connect: 登录用户 使系统内用户在其他系统中可访问 应用场景举例: 带有后台服务的网站应用程序(例如 MVC): OpenID Connect，前台请求认证服务，换取 Access Token 和 ID Token，客户端将其以 Cookie 存入浏览器本地以保持状态，这样的优势在于，前后端不再依赖不同平台的 Session 实现，更好的前后端解耦。 本地移动应用: 客户端请求认证服务，换取 Access Token 和 ID Token，将其存储于与设备安全相关的内容栈上，例如 Keychain。 SPA: 前台请求认证服务，以 Implict 模式返回 Access Token 和 Id Token，并确保其安全。]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 应用 - 视图组件]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-application-viewcomponent%2F</url>
    <content type="text"><![CDATA[参考资料: View components in ASP.NET Core 本文大纲: 前言 创建视图组件 视图组件方法 视图查找路径 调用视图组件 使用标签的方式调用视图组件 从控制器中调用视图组件 前言ASP.NET Core 只有两种组件作为路由系统的入口点: Views: 以 Controller 为基础返回的视图 Razor Pages: 直接返回 Razor Page 两种组件都可以在其内部渲染 Partial 视图，但任何一种组件都不能直接渲染另一种，一个可选的方案是，View Component(下称「视图组件」)。 视图组件不会使用模型绑定系统，仅仅依赖于传递给它的数据，它: 仅渲染一块内容而非整个 Response 依然保有职责分离的特性 可以包含参数及业务逻辑 可以从模板页面从调用 视图组件用于封装可重用的，但分部视图无法胜任的渲染逻辑，例如: 动态导航菜单 标签云(需访问数据库) 购物车 最近发布的文章 博客的侧边内容 根据用户登录状态展示的登录面板，已登录或未登录 视图组件由两部分组成，继承自 ViewComponent 的类和对应的视图。 创建视图组件有以下方式可以创建视图组件: 从 ViewComponent 类型派生 使用 [ViewComponent] 特性标记一个类型，或一个派生自被 [ViewComponent] 特性标记的类型 创建一个以 ViewComponent 为后缀的类型 视图组件类型必须为公开的，非嵌套，非抽象类。视图组件的名称是其类型名称移除 ViewComponent 的部分，该名称也可通过 ViewComponentAttribute.Name 进行显式指定，视图组件类型: 完全支持依赖注入 不同于控制器的生命周期，所以无法对其应用过滤器 视图组件方法视图组件将逻辑定义在 InvokeAsync 方法中，该方法返回一个 IViewComponentResult 类型的实例，方法参数由调用者传递而非来自模型绑定系统，视图组件不会直接处理请求。通常，视图组件初始化一个模型，并调用 View 方法将该模型传递至视图。视图模型的方法: 定义一个 InvokeAsync 方法，该方法返回一个 IViewComponentResult 类型的实例 初始化一个模型，并调用 View 方法将该模型传递至视图 方法参数由调用者传递而非来自模型绑定系统 并非一个 HTTP 请求的入口点，它们通常由视图调用 由方法重载传递不同的参数 视图查找路径运行时从以下路径查找视图: Views/&lt;controller_name&gt;/Components/&lt;view_component_name&gt;/&lt;view_name&gt; Views/Shared/Components/&lt;view_component_name&gt;/&lt;view_name&gt;: 官方推荐的组织方式 调用视图组件编写以下代码调用视图组件:123@Component.InvokeAsync("Name of view component", *anonymous type containing parameters*)@await Component.InvokeAsync("PriorityList", new &#123; maxPriority = 4, isDone = true &#125;) 第二行代码表示，调用名为 PriorityList 的视图组件，匿名类型中的两个参数将传递给类型中的 InvokeAsync 方法。 使用标签的方式调用视图组件形如:12&lt;vc:priority-list max-priority="2" is-done="false"&gt;&lt;/vc:priority-list&gt; vc 元素用来调用视图组件，以 Pascal 风格命名的类型和方法将被转换为对应的小写 kebab case 风格。注意，为了以标签形式使用视图组件，必须使用 @addTagHelper 注册包含视图组件的程序集，例如，包含视图组件的程序集名为 MyWebApp，那么将以下声明加入 _ViewImports.cstml 文件:1@addTagHelper *, MyWebApp 从控制器中调用视图组件视图组件通常是由视图调用的，但也可以直接从控制器方法中调用它们，控制器方法通过返回一个 ViewComponentResult 实例来使用视图组件，例如:1234public IActionResult IndexVC()&#123; return ViewComponent("PriorityList", new &#123; maxPriority = 3, isDone = false &#125;);&#125; 关于视图组件用法案例，参考: Walkthrough: Creating a simple view component]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 应用 - 验证]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-application-validation%2F</url>
    <content type="text"><![CDATA[参考资料: Model validation in ASP.NET Core MVC 本文大纲: 验证特性 Required 特性的注意事项 模型状态(Model State) 手动验证 自定义验证 客户端验证 为动态表单添加验证 为动态控件添加验证 IClientModelValidator 远程验证 验证特性ASP.NET Core 内置了一系列用于验证的 attribute(以下使用特性代替) 使得开发人员可以声明式地将它们应用于任何类型或属性。 1234567891011121314151617181920212223242526public class Movie&#123; public int ID &#123; get; set; &#125; [StringLength(60, MinimumLength = 3)] [Required] public string Title &#123; get; set; &#125; [Display(Name = "Release Date")] [DataType(DataType.Date)] public DateTime ReleaseDate &#123; get; set; &#125; [Range(1, 100)] [DataType(DataType.Currency)] public decimal Price &#123; get; set; &#125; [RegularExpression(@"^[A-Z]+[a-zA-Z""'\s-]*$")] [Required] [StringLength(30)] public string Genre &#123; get; set; &#125; [RegularExpression(@"^[A-Z]+[a-zA-Z""'\s-]*$")] [StringLength(5)] [Required] public string Rating &#123; get; set; &#125;&#125; [CreditCard]: 属性匹配信用卡格式 [Compare]: 在一个模型中验证两个属性 [EmailAddress]: 属性匹配电子邮件格式 [Phone]: 属性匹配电话号码格式 [Range]: 属性值匹配一个范围 [RegularExpression]: 模式匹配 [Required]: 要求属性必须有值 [StringLength]: 属性匹配字符串长度 [Url]: 属性匹配 url 格式 DataType 特性提供格式化不提供验证。 所有继承自 ValidationAttribute 的类型均支持模型验证，查看 System.ComponentModel.DataAnnotations获取更多有用的验证类型。 当内置验证特性无法满足要求时，可通过新建继承自 ValidationAttribute 的自定义验证类或使模型实现 IValidatableObject 接口进行扩展。 Required 特性的注意事项首先，值类型继承 Required 特性，当它们作为属性时，无需为它们指定 Required 特性，应用程序服务端将不会检查值类型的 Required 特性。 MVC 模型绑定系统与验证系统无关，如果模型绑定器发现某个不能为空的类型的值缺失或以空白填充，模型绑定器将忽略这些值。BindRequired 特性用于保证传递至后台的数据是完整的，当应用于一个属性时，模型绑定器要求该属性必须有值，当应用一个类型时，模型绑定器要求该类型下的所有属性值都必须有值。 表单数据对应的模型属性有 Required 特性时，客户端验证同样要求该表单数据有值，对值类型属性未被标记未 Required 特性的做 Required 检查，Required 同样可以控制客户端验证的错误消息。 ASP.NET Core 模型验证在客户端和服务端(以免用户禁用了 Javascript)同时启用，表单数据在客户端验证通过之前不会发送到服务端。 模型状态(Model State)MVC 会持续验证字段直到达到错误数量的最大值(默认值 200)，可以通过以下代码来改变该值:1services.AddMvc(options =&gt; options.MaxModelValidationErrors = 50); 调用 ModelState.IsValid 方法将评估应用到模型上的所有验证特性 手动验证模型绑定和验证完成之后，可能想要重复其中的某些部分，例如，想要通过模型的属性计算出另外一个值，可通过调用 ModelState.TryValidateModel 方法来手动验证模型。 自定义验证创建继承自 ValidationAttribute 的类型并重写 IsValid 方法，该方法接受两个参数: Object: 即将要被验证的目标值 ValidationContext: 获取与验证相关的其他信息，包括被模型绑定器已经绑定好的模型本身及其他属性 1234567891011121314151617181920public class ClassicMovieAttribute : ValidationAttribute, IClientModelValidator&#123; private int _year; public ClassicMovieAttribute(int Year) &#123; _year = Year; &#125; protected override ValidationResult IsValid(object value, ValidationContext validationContext) &#123; Movie movie = (Movie)validationContext.ObjectInstance; if (movie.Genre == Genre.Classic &amp;&amp; movie.ReleaseDate.Year &gt; _year) &#123; return new ValidationResult(GetErrorMessage()); &#125; return ValidationResult.Success; &#125; 这种方法通常用于扩展通用的验证特性，如果某种验证规则仅仅适用于某个类型，那么让该类型继承 IValidateObject 接口并实现 Validate 方法可能是更好的选择。123456789public IEnumerable&lt;ValidationResult&gt; Validate(ValidationContext validationContext)&#123; if (Genre == Genre.Classic &amp;&amp; ReleaseDate.Year &gt; _classicYear) &#123; yield return new ValidationResult( $"Classic movies must have a release year earlier than &#123;_classicYear&#125;.", new[] &#123; "ReleaseDate" &#125;); &#125;&#125; 客户端验证客户端验证通过在 html 中引用特定的 .js 文件来实现:12&lt;script src="https://cdn.bootcss.com/jquery-validate/1.17.0/jquery.validate.min.js"&gt;&lt;/script&gt;&lt;script src="https://cdn.bootcss.com/jquery-validation-unobtrusive/3.2.9/jquery.validate.unobtrusive.min.js"&gt;&lt;/script&gt; jQuery Unobtrusive Validation 是一个基于 jQuery Validate 插件的自定义微软前端库，两者 .js 均依赖 jquery 库。 MVC Tag Helpers 和 HTML helpers 读取模型属性的验证特性在表单中渲染 HTML5 data-attributes，MVC 同时为内置和自定义验证特性创建 data- 属性。然后 jQuery Unobtrusive Validation 将 data- 属性转换为 jQuery Validate 能够理解的数据，并将服务端的验证逻辑「复制」到前端，并在 HTML 中使用 tag helpers 展示相同的验证错误信息: 1234567&lt;div class="form-group"&gt; &lt;label asp-for="ReleaseDate" class="col-md-2 control-label"&gt;&lt;/label&gt; &lt;div class="col-md-10"&gt; &lt;input asp-for="ReleaseDate" class="form-control" /&gt; &lt;span asp-validation-for="ReleaseDate" class="text-danger"&gt;&lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 以上代码将被渲染为:12345678910111213141516&lt;form action="/Movies/Create" method="post"&gt; &lt;div class="form-horizontal"&gt; &lt;h4&gt;Movie&lt;/h4&gt; &lt;div class="text-danger"&gt;&lt;/div&gt; &lt;div class="form-group"&gt; &lt;label class="col-md-2 control-label" for="ReleaseDate"&gt;ReleaseDate&lt;/label&gt; &lt;div class="col-md-10"&gt; &lt;input class="form-control" type="datetime" data-val="true" data-val-required="The ReleaseDate field is required." id="ReleaseDate" name="ReleaseDate" value="" /&gt; &lt;span class="text-danger field-validation-valid" data-valmsg-for="ReleaseDate" data-valmsg-replace="true"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; input 标签并没有指定 type，MVC 会根据应用于属性之上的 [DataType] 特性来设置 input 标签的 type 属性，[DataType] 基类并不做任何验证，但当对属性应用 [DataType] 的子类 [EmailAddress] 时，jQuery Validation 会使用其他内容重写并展示消息。 为动态表单添加验证jQuery Unobtrusive Validation 仅在页面第一次加载时传递验证逻辑和参数给 jQuery Validate，动态生成的表单内容将不会自动继承验证。相反，我们可以在创建新的表单内容后告知 jQuery Unobtrusive Validation 立即执行该动作，以下代码展示了通过 AJAX 动态添加的表单元素如何设置验证:1234567891011121314$.get(&#123; url: "https://url/that/returns/a/form", dataType: "html", error: function(jqXHR, textStatus, errorThrown) &#123; alert(textStatus + ": Couldn't add form. " + errorThrown); &#125;, success: function(newFormHTML) &#123; var container = document.getElementById("form-container"); container.insertAdjacentHTML("beforeend", newFormHTML); var forms = container.getElementsByTagName("form"); var newForm = forms[forms.length - 1]; $.validator.unobtrusive.parse(newForm); &#125;&#125;) $.validator.unobtrusive.parse() 方法接收一个 jQuery 选择器，该方法告诉 jQuery Unobtrusive Validation 转换该选择器内表单的所有的 data- 属性。 为动态控件添加验证同时，我们也同样可以为新动态生成的控件如 &lt;input/&gt; 和 &lt;select/&gt; 添加验证规则，我们不能直接传递控件的选择器，因为表单已经存在了，我们需要首先移除现有的验证规则，然后重新转换整个表单:1234567891011121314$.get(&#123; url: "https://url/that/returns/a/control", dataType: "html", error: function(jqXHR, textStatus, errorThrown) &#123; alert(textStatus + ": Couldn't add control. " + errorThrown); &#125;, success: function(newInputHTML) &#123; var form = document.getElementById("my-form"); form.insertAdjacentHTML("beforeend", newInputHTML); $(form).removeData("validator") // Added by jQuery Validate .removeData("unobtrusiveValidation"); // Added by jQuery Unobtrusive Validation $.validator.unobtrusive.parse(form); &#125;&#125;) IClientModelValidator通过实现 IClientModelValidator 可以为前端验证添加自定义逻辑:12345678910111213public void AddValidation(ClientModelValidationContext context)&#123; if (context == null) &#123; throw new ArgumentNullException(nameof(context)); &#125; MergeAttribute(context.Attributes, "data-val", "true"); MergeAttribute(context.Attributes, "data-val-classicmovie", GetErrorMessage()); var year = _year.ToString(CultureInfo.InvariantCulture); MergeAttribute(context.Attributes, "data-val-classicmovie-year", year);&#125;实现该接口的特性类可以为生成的字段添加 HTML 属性，上述类型在客户端的生成的 HTML 如下:123456&lt;input class="form-control" type="datetime" data-val="true" data-val-classicmovie="Classic movies must have a release year earlier than 1960." data-val-classicmovie-year="1960" data-val-required="The ReleaseDate field is required." id="ReleaseDate" name="ReleaseDate" value="" /&gt; Unobtrusive Validation 利用这些 data- 属性来展示错误消息，而 jQuery 本身并不认识这些新增的属性，为了使其生效，需要为 jQeury 的 validator 对象添加逻辑，如下代码所示:1234567891011121314151617181920212223$(function () &#123; jQuery.validator.addMethod('classicmovie', function (value, element, params) &#123; // Get element value. Classic genre has value '0'. var genre = $(params[0]).val(), year = params[1], date = new Date(value); if (genre &amp;&amp; genre.length &gt; 0 &amp;&amp; genre[0] === '0') &#123; // Since this is a classic movie, invalid if release date is after given year. return date.getFullYear() &lt;= year; &#125; return true; &#125;); jQuery.validator.unobtrusive.adapters.add('classicmovie', [ 'element', 'year' ], function (options) &#123; var element = $(options.form).find('select#Genre')[0]; options.rules['classicmovie'] = [element, parseInt(options.params['year'])]; options.messages['classicmovie'] = options.message; &#125;);&#125;(jQuery)); 现在，jQuery 已经获得了足够得信息来执行自定义得 Javascript 验证并展示错误信息。 远程验证远程验证非常适合传递客户端数据进行服务端上下文的验证，一个典型的应用场景是检查用户名或 Email 是否已经存在。实现远程验证需要两个步骤: 必须对模型属性进行 [Remote] 注解，该类型提供了多个重载来告知客户端 Javascript 应该调用哪个远程方法。以下代码演示了该注解: 12[Remote(action: "VerifyEmail", controller: "Users")]public string Email &#123; get; set; &#125; 将验证代码放置在 [Remote] 注解指定的方法中。 jQuery Validate 要求该方法的返回内容必须是 JSON 字符串，且对验证通过的元素返回 “true”，对未通过验证的元素返回 “false”, “undefined” 或 null，如果返回一个字符串值，该值将被作为自定义错误消息。 以下的 VerifyEmail() 方法遵循了这些规则:12345678910[AcceptVerbs("Get", "Post")]public IActionResult VerifyEmail(string email)&#123; if (!_userRepository.VerifyEmail(email)) &#123; return Json($"Email &#123;email&#125; is already in use."); &#125; return Json(true);&#125;[Remote] 特性的 AdditionalFields 属性用于组合验证，例如，如果 User 模型包含两个额外的属性 FirstName 和 LastName，可能同样需要验证数据库中没有其他已知用户拥有相同的姓名，演示代码如下:1234[Remote(action: "VerifyName", controller: "Users", AdditionalFields = nameof(LastName))]public string FirstName &#123; get; set; &#125;[Remote(action: "VerifyName", controller: "Users", AdditionalFields = nameof(FirstName))]public string LastName &#123; get; set; &#125;用于验证的 VerifyName 方法必须定义两个参数用于分别接收 FirstName 和 LastName:12345678910[AcceptVerbs("Get", "Post")]public IActionResult VerifyName(string firstName, string lastName)&#123; if (!_userRepository.VerifyName(firstName, lastName)) &#123; return Json(data: $"A user named &#123;firstName&#125; &#123;lastName&#125; already exists."); &#125; return Json(data: true);&#125;如果需要利用 [Remote] 特性同时验证两个以上的参数，将它们以 , 分隔开传递给 AdditionalFields 属性:12[Remote(action: "VerifyName", controller: "Users", AdditionalFields = nameof(FirstName) + "," + nameof(LastName))]public string MiddleName &#123; get; set; &#125; 像所有特性参数一样，AdditionalFields 只接受字面值表达式，因此无法传递变量值。]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 应用 - 国际化与本土化]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-application-localization%2F</url>
    <content type="text"><![CDATA[参考资料: Globalization and localization in ASP.NET Core 本文大纲: 国际化和本土化 IStringLocalizer 和 IStringLocalizer IHtmlLocalizer 和 IHtmlLocalizer IStringLocalizerFactory 视图本土化 数据注解实现本土化 数据注解使用统一资源定位 为支持的语言文化提供本土化资源 SupportedCultures 和 SupportedUICultures 资源文件 语言文化的回退(fallback)行为 Visual Studio 生成资源文件 添加对其他语言文化的支持 实现语言文化选择的机制 配置本土化 本土化中间件 QueryStringRequestCultureProvider CookieRequestCultureProvider Accept-Language HTTP Header 使用自定义提供器 程序化设置语言文化 国际化与本土化术语 参考文档: Globalization and localization in ASP.NET Core 国际化和本土化「国际化(Globalization)」 表示将 app 设计为支持不同语言文化的过程。 「本土化(Localization)」 表示将 app 调整为支持某个特定语言文化的过程。 要实现本土化，至少需要: 提供本土化的 app 的内容 提供支持语言文化的本土化 Resources 为每一个 request 实现选择语言文化的策略 IStringLocalizer 和 IStringLocalizerIStringLocalizer 和 IStringLocalizer&lt;T&gt; 在框架层面提供了对本土化的支持，该接口使用 ResourceManager 和 ResourceReader 在运行时提供特定语言文化的资源，该接口公开一个索引器来查询本土化的字符串。该接口不需要在开发时设置中立语言并为不同的字符串提供默认值，示例代码如下: 12345678910111213141516171819202122using Microsoft.AspNetCore.Mvc;using Microsoft.Extensions.Localization;namespace Localization.StarterWeb.Controllers&#123; [Route("api/[controller]")] public class AboutController : Controller &#123; private readonly IStringLocalizer&lt;AboutController&gt; _localizer; public AboutController(IStringLocalizer&lt;AboutController&gt; localizer) &#123; _localizer = localizer; &#125; [HttpGet] public string Get() &#123; return _localizer["About Title"]; &#125; &#125;&#125; 以上代码中，IStringLocalizer&lt;AboutController&gt; 的实例由容器注入，如果在对应的资源中找不到 ‘About Title’ 的值，将直接返回该键，这里的 ‘About Title’ 是字面值，当有其他语言文化的资源文件被创建并提供了针对该字面值的本土化版本后，这里将返回相应的本土化字符串。这样在开发时无需预先定义资源文件并提供默认值。但遇到字面值较长的情况时，仍然建议使用传统方式来实现本土化——即为各个资源事先定义键，再在各个资源文件中提供对应的翻译版本。 IHtmlLocalizer 和 IHtmlLocalizerIHtmlLocalizer 和 IHtmlLocalizer 支持 HTML 化的资源，该接口对格式化的参数进行编码，但 HTML 部分不会编码，示例如下:1234567891011121314151617181920212223using System;using Microsoft.AspNetCore.Http;using Microsoft.AspNetCore.Localization;using Microsoft.AspNetCore.Mvc;using Microsoft.AspNetCore.Mvc.Localization;namespace Localization.StarterWeb.Controllers&#123; public class BookController : Controller &#123; private readonly IHtmlLocalizer&lt;BookController&gt; _localizer; public BookController(IHtmlLocalizer&lt;BookController&gt; localizer) &#123; _localizer = localizer; &#125; public IActionResult Hello(string name) &#123; ViewData["Message"] = _localizer["&lt;b&gt;Hello&lt;/b&gt;&lt;i&gt; &#123;0&#125;&lt;/i&gt;", name]; return View(); &#125;注意上述代码引用了 Microsoft.AspNetCore.Localization 和 Microsoft.AspNetCore.Mvc.Localization 命名空间，只有 name 参数被 HTML 编码。 IStringLocalizerFactoryIStringLocalizerFactory 演示了一种更加底层的方法，示例如下:123456789101112131415161718&#123; public class TestController : Controller &#123; private readonly IStringLocalizer _localizer; private readonly IStringLocalizer _localizer2; public TestController(IStringLocalizerFactory factory) &#123; var type = typeof(SharedResource); var assemblyName = new AssemblyName(type.GetTypeInfo().Assembly.FullName); _localizer = factory.Create(type); _localizer2 = factory.Create("SharedResource", assemblyName.Name); &#125; public IActionResult About() &#123; ViewData["Message"] = _localizer["Your application description page."] + " loc 2: " + _localizer2["Your application description page."];以上代码演示了 IStringLocalizerFactory.Create 方法的两种重载，从演示中可以看出，SharedResource 被放置到了另一个程序集中，通过这种方式可以将资源与控制器等分离。 你可以以控制器，区域为单位对字符串资源进行划分，或者不划分直接使用一个包含所有资源的类型。一些开发人员喜欢以 Startup 类来承载共享字符串资源。 下面的示例使用了两个不同的 IStringLocalizer&lt;T&gt; 实例:123456789101112131415161718public class InfoController : Controller&#123; private readonly IStringLocalizer&lt;InfoController&gt; _localizer; private readonly IStringLocalizer&lt;SharedResource&gt; _sharedLocalizer; public InfoController(IStringLocalizer&lt;InfoController&gt; localizer, IStringLocalizer&lt;SharedResource&gt; sharedLocalizer) &#123; _localizer = localizer; _sharedLocalizer = sharedLocalizer; &#125; public string TestLoc() &#123; string msg = "Shared resx: " + _sharedLocalizer["Hello!"] + " Info resx " + _localizer["Hello!"]; return msg; &#125; 视图本土化IViewLocalizer 接口为视图提供本土化字符串，ViewLocalizer 类型为该接口的默认实现，以下代码展示了如何使用该类型:1234567891011@using Microsoft.AspNetCore.Mvc.Localization@inject IViewLocalizer Localizer@&#123; ViewData["Title"] = Localizer["About"];&#125;&lt;h2&gt;@ViewData["Title"].&lt;/h2&gt;&lt;h3&gt;@ViewData["Message"]&lt;/h3&gt;&lt;p&gt;@Localizer["Use this area to provide additional information."]&lt;/p&gt;IViewLocalizer 的默认实现基于 View 的文件名查找资源，该类型没有提供设置共享资源文件的选项，并且该类型实现了 IHtmlLocalizer 接口，要在 View 中使用共享资源文件，注入 IHtmlLocalizer&lt;T&gt;:123456789101112@using Microsoft.AspNetCore.Mvc.Localization@using Localization.StarterWeb.Services@inject IViewLocalizer Localizer@inject IHtmlLocalizer&lt;SharedResource&gt; SharedLocalizer@&#123; ViewData["Title"] = Localizer["About"];&#125;&lt;h2&gt;@ViewData["Title"].&lt;/h2&gt;&lt;h1&gt;@SharedLocalizer["Hello!"]&lt;/h1&gt; 数据注解实现本土化数据注解的字符串内容默认与 IStringLocalizer&lt;T&gt; 协同实现本土化，因为已经设置了 ResourcesPath = &quot;Resources&quot;，RegiserViewModel 中 Attribute 查询消息的依据为: Resources/ViewModels.Account.RegisterViewModel.fr.resx Resources/ViewModels/Account/RegisterViewModel.fr.resx123456789101112131415161718public class RegisterViewModel&#123; [Required(ErrorMessage = "The Email field is required.")] [EmailAddress(ErrorMessage = "The Email field is not a valid email address.")] [Display(Name = "Email")] public string Email &#123; get; set; &#125; [Required(ErrorMessage = "The Password field is required.")] [StringLength(8, ErrorMessage = "The &#123;0&#125; must be at least &#123;2&#125; characters long.", MinimumLength = 6)] [DataType(DataType.Password)] [Display(Name = "Password")] public string Password &#123; get; set; &#125; [DataType(DataType.Password)] [Display(Name = "Confirm password")] [Compare("Password", ErrorMessage = "The password and confirmation password do not match.")] public string ConfirmPassword &#123; get; set; &#125;&#125; 数据注解使用统一资源定位如前文所述，数据注解将以所在类型文件名称为依据查询对应的资源，以下代码展示了如何为不同的类型统一查询资源的地址:12345678public void ConfigureServices(IServiceCollection services)&#123; services.AddMvc() .AddDataAnnotationsLocalization(options =&gt; &#123; options.DataAnnotationLocalizerProvider = (type, factory) =&gt; factory.Create(typeof(SharedResource)); &#125;);&#125;SharedResource 对应于存储字符串资源的类型，通过这种方式，数据注解将使用该类型作为查询依据。 为支持的语言文化提供本土化资源SupportedCultures 和 SupportedUICulturesASP.NET CORE 允许指定两个接收 CultureInfo 值的属性: SupportedCultures: 该属性决定与语言文化有关的输出结果，如日期，时间，数字和货币等格式；该值还决定文本的排序规则，大小写转换和字符串比较，有关服务器如何获取语言文化信息，参见 CultureInfo.CurrentCulture。 SupportedUICultures: 该属性决定 ResourceMangaer 该从哪一个 .resx 文件读取对应资源的翻译版本，.NET 的每一个线程都包含一个 CurrentCulture 和 CurrentUICulture 属性，ASP.NET CORE 在渲染与语言文化相关的功能时会检查它们。 资源文件非默认语言的字符串资源字典为单独的 .resx 资源文件。资源文件的默认命名约定为类型的完全限定名去掉程序集部分，例如，LocalizationWebSite.Web.dll 程序集的类型 LocalizationWebSite.Web.Startup 的法语版本资源文件的名称为 Startup.fr.resx，其中 fr 为语言文化编码，LocalizationWebsite.Web.Controllers.HomeController 的对应资源文件会命名为 Controllers.HomeController.fr.resx。如果目标类型命名空间与所在程序集不同，则命名时以完全限定名开头。 在 Startup.cs 类型中的 ConfigureServices 方法中将 ResourcesPath 设置为 “Resources”，那么 HomeController 对应的法语版本资源文件的相对路径为 Resources/Controllers.HomeController.fr.resx。另外，可以通过文件夹组织资源文件，如果不设置 ResourcesPath，将在项目根目录下查找 .resx 文件，使用 . 语法还是 / 路径作为约定取决于如何组织资源文件。 在 Razor 视图中应用资源文件基于相同的模式，假设 ResourcesPath 设置为 “Resources”，那么 Views/Home/About.cshtml 视图的法语版本资源文件就应该定位在: Resources/Views/Home/About.fr.resx Resources/Views.Home.About.fr.resx 如果不指定 ResourcesPath，那么系统将在与视图相同的文件夹下查找资源文件。 语言文化的回退(fallback)行为本土化机制首先从目标的语言文化开始查找，如果没有找到，它首先还原为目标语言文化的父级文化，即 CultureInfo.Parent，如果父级语言文化也未能找到，则返回默认资源文件中的值，如果默认资源文件也没有，则直接返回键。 Visual Studio 生成资源文件如果通过 Visuall Studio 新建一个默认资源文件(不带任何语言编码的 .resx 文件)，Visuall Studio 会为该文件生成一个类，为每一个新添加的键生成一个对应的属性，这不是 ASP.NET CORE 推荐的方式，我们希望每次新建资源文件时都带上语言编码，这样 Visuall Studio 便不会为其生成类型。 添加对其他语言文化的支持除了默认语言，每一个语言文化都需要一个唯一的资源文件，新建带有语言文化编码的资源文件是支持新语言文件的必须步骤。 实现语言文化选择的机制配置本土化本土化由 ConfigureServices 方法配置:12345services.AddLocalization(options =&gt; options.ResourcesPath = "Resources");services.AddMvc() .AddViewLocalization(LanguageViewLocationExpanderFormat.Suffix) .AddDataAnnotationsLocalization(); AddLocalization: 将本土化服务添加到服务容器，以上代码同时设置了 ResourcesPath。 AddViewLocalization: 添加对视图文件的本土化支持，在此例中，视图本土化基于视图文件名称的后缀。例如，Index.fr.cshtml 文件中的 “fr” 以示区别。 AddDataAnnotationsLocalization: 对数据注解本土化的支持，以 IStringLocalizer 抽象实现。 本土化中间件请求的当前语言文化由本土化中间件进行设置，该中间件在 Configure 方法中启用。该中间件必须在任何试图检查请求语言文化数据的中间件(例如，app.UseMvcWithDefaultRoute())启用之前启用。123456789101112131415161718192021222324252627var supportedCultures = new[]&#123; new CultureInfo(enUSCulture), new CultureInfo("en-AU"), new CultureInfo("en-GB"), new CultureInfo("en"), new CultureInfo("es-ES"), new CultureInfo("es-MX"), new CultureInfo("es"), new CultureInfo("fr-FR"), new CultureInfo("fr"),&#125;;app.UseRequestLocalization(new RequestLocalizationOptions&#123; DefaultRequestCulture = new RequestCulture(enUSCulture), // Formatting numbers, dates, etc. SupportedCultures = supportedCultures, // UI strings that we have localized. SupportedUICultures = supportedCultures&#125;);app.UseStaticFiles();// To configure external authentication, // see: http://go.microsoft.com/fwlink/?LinkID=532715app.UseAuthentication();app.UseMvcWithDefaultRoute();UseRequestLocalization 初始化一个 RequestLocalizationOptions 对象，每次有请求到达时，系统会遍历 RequestLocalizationOptions 中的 RequestCultureProvider 列表，第一个匹配请求语言文化的提供器将被应用。RequestLocalizationOptions 类型包含了以下默认的提供器: QueryStringRequestCultureProvider CookieRequestCultureProvider AcceptLanguageHeaderRequestCultureProvider 默认提供器遍历列表从最具体到最抽象，后文将讨论如何改变遍历顺序抑或添加一个自定义的提供器。如果任何提供器都不能匹配请求的语言文化，那么采用 DefaultRequestCulture。 QueryStringRequestCultureProvider一些网站应用会在查询字符串中携带代表语言文化的参数，该方法易于调试和测试。默认情况下，QueryStringRequestCultureProvider 作为第一个本土化提供器注册到 RequestCultureProvider 列表，那么在查询字符串中携带 culture 和 ui-culture 参数则会被解析为对应的语言文化，例如:1http://localhost:500/?culture=es-MX&amp;ui-culture=es-MX 如果只传递其中一个值，那么 culture 和 ui-culture 都将被设置为该值。 CookieRequestCultureProvider部署到生产环境中运行网站应用程序通常会使用 ASP.NET Core culture cookie 来实现本土化，使用 MakeCookieValue 方法创建 cookie。 CookieRequestCultureProvider.DefaultCookieName 返回默认的 cookie 名称，该默认值为 .AspNetCore.Culture。该值的格式为 c=%LANGCODE%|uic=%LANGCODE%，其中 c 代表 culture，uic 代表 ui-culture，例如:1c=en-UK|uic=en-US 如果只传递其中一个值，那么 culture 和 ui-culture 都将被设置为该值。 Accept-Language HTTP HeaderAccept-Language header 在多数浏览器中都可以修改，它最初是用来指定用户的语言，该值指示浏览器应该将何种语言包含在请求头中，该值通常继承自操作系统。从浏览器发送的请求头中检查用户的偏好语言通常不是一种可靠的办法，应用程序应该提供另一种机制允许用户设置偏好语言。 使用自定义提供器假设想要将用户偏好的语言保存在数据库中，可通过编写一个自定义提供器来查询这些值，以下代码展示了如何添加一个自定义本土化提供器:1234567891011121314151617181920private const string enUSCulture = "en-US";services.Configure&lt;RequestLocalizationOptions&gt;(options =&gt;&#123; var supportedCultures = new[] &#123; new CultureInfo(enUSCulture), new CultureInfo("fr") &#125;; options.DefaultRequestCulture = new RequestCulture(culture: enUSCulture, uiCulture: enUSCulture); options.SupportedCultures = supportedCultures; options.SupportedUICultures = supportedCultures; options.RequestCultureProviders.Insert(0, new CustomRequestCultureProvider(async context =&gt; &#123; // My custom request culture logic return new ProviderCultureResult("en"); &#125;));&#125;);使用 RequestLocalizationOptions 来添加和移除本土化提供器，Insert 方法的第一个参数为索引值，该值同时决定了顺序。 程序化设置语言文化国际化与本土化术语即使所有的计算机以数字编码的方式存储文本，但不同的系统却使用不同的数字编码来存储相同的文本。RFC 4646 格式提供了 &lt;languagecode2&gt;-&lt;country/regioncode2&gt; 格式的参考，&lt;languagecode2&gt; 为语言码，&lt;country/regioncode2&gt; 为子文化码，例如 en-US 和 en-AU 分别代表美国英语和澳大利亚英语。RFC 4646 是 ISO 639 代表语言的两位小写语言码与 ISO 3166 代表国家和地区的两位大写文化码的组合。]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 框架基础 - 管道与中间件]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-fundamentals-pipelines%2F</url>
    <content type="text"><![CDATA[参考资料: 采用管道处理请求 管道如何处理请求 管道如何创建 中间件究竟是什么 本文大纲: 概述 一个简单的 Hello World 应用 管道的构成 定制管道 服务器 HttpApplication HostingApplication KestrelServer ServerAddressesFeature WebHost WebHostOptions 构建管道 WebHostBuilder 几个常用的扩展方法 HttpContext FeatureCollection DefaultHttpContext HttpContextFactory ApplicationBulder ApplicationBuilderFactory 中间件类型 中间件类型注册 概述HTTP 协议自身的特性决定了任何一个 Web 应用的工作方式都是监听、接收并处理 HTTP 请求，并最终对请求予以响应。HTTP 请求处理是管道式设计典型的应用场景，ASP.NET Core 根据一个具体的 HTTP 请求构建一个管道，接收到的 HTTP 请求消息像水一样流入这个管道，组成这个管道的各个环节依次对它作相应的处理。整个请求处理完成后的结果同样转变成消息逆向流入这个管道进行处理，并最终变成回复给客户端的 HTTP 响应。 一个简单的 Hello World 应用首先来看一个简单的 .NET Core 应用程序: 1234567891011public class Program&#123; public static void Main() &#123; new WebHostBuilder() .UseKestrel() .Configure(app =&gt; app.Run(async context=&gt; await context.Response.WriteAsync("Hello World"))) .Build() .Run(); &#125;&#125; WebHost 对象可以看成是 Web 应用的宿主，启动 Web 应用本质上就是启动 WebHost 宿主对象。WebHostBuilder 负责创建 WebHost 对象，它的 Build 方法创建并返回相应的 WebHost。 Configure 方法注册到 WebHostBuilder 上的委托对象(委托类型为 Action&lt;IApplicationBuilder&gt;)用于定制管道的逻辑。调用 WebHost 的扩展方法 Run 启动应用程序时，用于监听，接收，处理和响应 HTTP 请求的管道也随之被建立。 管道的构成HTTP 请求处理流程始于对请求的监听，终于对请求的响应，这两项工作均由同一个对象来完成，我们称之为 「服务器(Server)」 。尽管 ASP.NET Core 的请求处理管道可以任意定制，但是该管道必须有一个 Server，Server 是整个管道的「水龙头」。在上述的 Hello World 应用中，在 Build 一个 WebHost 之前，首先调用了扩展方法 UseKestrel，该方法就是为后续构建的管道注册一个名为 KestrelServer 的「服务器」。 调用 WebHost 的 Start 方法(调用 WebHost 的扩展方法 Run 时，它的 Start 方法会被自动调用)之后，定制的管道会被构建出来，管道的服务器将绑定到一个预设的端口(KestrelServer 默认采用 5000 作为监听端口)开始监听请求。HTTP 请求一旦抵达，服务器将其标准化并分发给管道后续的节点。管道中位于服务器之后的节点称为「中间件(Middleware)」。每个中间件都具有各自独立的功能，例如有专门实现路由功能的中间件，有专门实施用户认证的中间件。所谓的管道定制体现在根据具体的需求选择对应的中间件组成最终的请求处理管道。下图揭示了由一个服务器和一组中间件构成的请求处理管道:一个基于 ASP.NET Core 的应用程序通常是根据某个框架开发的，而框架本身就是通过某个或多个「中间件」构建出来的。ASP.NET Core MVC 就是典型的基于 ASP.NET Core 的开发框架，它定义了一个叫做「路由」的中间件实现了请求地址与 Controller/Action 之间的映射，并在此基础实现了激活 Controller，执行 Action 以及呈现 View 等一系列的功能。所以应用程序可以视为某个中间件的一部分，如果一定要将它独立出来，整个请求处理管道将呈现出如下图所示的结构: 定制管道在上述的 Hello World 程序中，调用扩展方法 UseKestrel 注册 KestrelServer 服务器之后，还调用 WebHostBuilder 的 Configure 的扩展方法注册了一个类型为 Action&lt;IApplicationBuilder&gt; 的委托对象。注册这个委托对象的目的在于对构建的管道定制请求处理逻辑，即为管道注册中间件。这个委托对象调用 ApplicationBuilder 的 Run 扩展方法注册了一个中间件来为每个请求响应一个「Hello World」字符串。1public static IWebHostBuilder Configure(this IWebHostBuilder hostBuilder, Action&lt;IApplicationBuilder&gt; configureApp) 除了调用 WebHostBuilder 的 Configure 方法来注册一个 Action&lt;IApplicationBuilder&gt; 类型的委托，注册中间件定义管道的逻辑更多地还是定义在一个单独的类型中。由于管道的定制总是在应用程序启动(Startup)的时候进行，一般称这个用于定制管道的类型为「启动类型」，并在大部分情况下会直接命名为 Startup。按照约定，通过注册中间件定制管道的操作会实现在名为 Configure 的方法中，方法的第一个参数必须是一个 IApplicationBuilder 接口的实例，后续可定义任意数量和类型的参数，当 ASP.NET Core 框架调用该方法的时候，会以依赖注入的方式提供这些参数的值。启动类型可以通过调用 WebHostBuilder 的扩展方法 UseStartup&lt;T&gt; 来指定，如下面的代码与前面演示的示例是完全等效的。12345678910111213141516171819public class Startup&#123; public void Configure(IApplicationBuilder app) &#123; app.Run(async context =&gt; await context.Response.WriteAsync("Hello World")); &#125;&#125;public class Program&#123; public static void Main() &#123; new WebHostBuilder() .UseKestrel() .UseStartup&lt;Startup&gt;() .Build() .Run(); &#125;&#125;在真正的项目开发中，我们会利用 ApplicationBuilder 注册相应的中间件进而构建一个符合需求的请求处理管道。如下所示，我们除了按照上面的方式调用扩展方法 UseMvc 注册了支撑 MVC 框架的中间件(实际上是一个实现路由的中间件)之外，还调用了其它的扩展方法注册了相应的中间件实现了对静态文件的访问(UseStaticFiles)，错误页面的呈现(UseExceptionHandler)以及基于 ASP.NET Identity Framework 的认证(UseIdentity):1234567891011public class Startup&#123; public void Configure(IApplicationBuilder app) &#123; app.UseExceptionHandler("/Home/Error"); app.UseStaticFiles(); app.UseIdentity(); app.UseMvc(); &#125;&#125; 服务器服务器是 ASP.NET Core 管道的第一个节点，它负责请求的监听和接收，并最终完成对请求的响应。服务器是所有实现了 IServer 接口的类型及其对象的统称。IServer 接口定义了一个只读属性 Features 返回描述自身特性集合的 IFeatureCollection 对象，Start 方法用于启动服务器。12345public interface IServer&#123; IFeatureCollection Features &#123; get; &#125; void Start&lt;TContext&gt;(IHttpApplication&lt;TContext&gt; application); &#125; Start 方法一旦执行，服务会马上开始监听工作。任何 HTTP 请求抵达，该方法接收一个 HttpApplication 对象创建一个上下文，并在此上下文中完成对请求的所有处理操作。当完成了对请求的处理任务之后，HttpApplication 对象会自行负责回收释放由它创建的上下文。 HttpApplicationASP.NET Core 请求处理管道由一个服务器和一组有序排列的中间件组合而成。如果在此之上作进一步抽象，将后者抽象成一个 HttpApplication 对象，该管道就成了一个 Server 和 HttpApplication 的组合。Server 将接收到的 HTTP 请求转发给 HttpApplication 对象，后续的请求完全由它来负责。HttpApplication 从服务器获得请求之后，会使用注册的中间件对请求进行处理，并最终将请求递交给应用程序。HttpApplication 针对请求的处理在一个执行上下文中完成，这个上下文为对单一请求的整个处理过程定义了一个边界。描述 HTTP 请求的 HttpContext 是这个执行上下文中最核心的部分，除此之外，我们还可以根据需要将其他相关的信息定义其中，所以 IHttpApplication&lt;TContext&gt; 接口采用泛型来表示定义这个上下文的类型。一个 HttpApplication 对象在接收到 Server 转发的请求之后完成三项基本的操作，即「创建上下文」，「在上下文中处理请求」以及「请求处理完成之后释放上下文」，这些操作通过三个方法来完成。123456public interface IHttpApplication&lt;TContext&gt;&#123; TContext CreateContext(IFeatureCollection contextFeatures); Task ProcessRequestAsync(TContext context); void DisposeContext(TContext context, Exception exception);&#125; CreateContext 和 DisposeContext 方法分别体现了执行上下文的创建和释放，CreateContext 方法的参数 contextFeatures 表示描述原始上下文的特性集合。在此上下文中针对请求的处理实现在另一个方法 ProcessRequestAsync 中。 HostingApplication在 ASP.NET Core 中，HostingApplication 类型是 IHttpApplication&lt;Context&gt; 默认实现类，它创建的执行上下文有如下定义:123456public struct Context&#123; public HttpContext HttpContext &#123; get; set; &#125; public IDisposable Scope &#123; get; set; &#125; public long StartTimestamp &#123; get; set; &#125;&#125; 该类型封装了一个 HttpContext 对象，后者是真正描述当前 HTTP 请求的上下文，承载着核心的上下文信息。除此之外，Context 还定义了 Scope 和 StartTimestamp 两个属性，两者与日志记录和事件追踪有关，前者用来将针对同一请求的多次日志记录关联到同一个上下文区限(见日志区限；后者表示请求开始处理的时间戳，如果在完成请求处理的时候记录下当时的时间戳，就可以计算出整个请求处理所花费的时间。 12345678public class HostingApplication : IHttpApplication&lt;HostingApplication.Context&gt;&#123; public HostingApplication(RequestDelegate application, ILogger logger, DiagnosticSource diagnosticSource, IHttpContextFactory httpContextFactory); public Context CreateContext(IFeatureCollection contextFeatures); public void DisposeContext(Context context, Exception exception); public Task ProcessRequestAsync(Context context);&#125; HostingApplication 的构造函数依赖一个 RequestDelegate 的委托对象，该对象由 IApplicationBuilder 注册的中间件生成，HttpContextFactory 用以创建 HttpContext 对象:123456789101112131415public class HostingApplication : IHttpApplication&lt;HostingApplication.Context&gt;&#123; private readonly RequestDelegate _application; private readonly DiagnosticSource _diagnosticSource; private readonly IHttpContextFactory _httpContextFactory; private readonly ILogger _logger; public HostingApplication(RequestDelegate application, ILogger logger, DiagnosticSource diagnosticSource, IHttpContextFactory httpContextFactory) &#123; _application = application; _logger = logger; _diagnosticSource = diagnosticSource; _httpContextFactory = httpContextFactory; &#125;&#125;logger 和 diagnosticSource 是与日志记录有关的参数。HostingApplication 对 CreateContext，ProcessRequestAsync 和 DisposeContext 有如下实现:12345678910111213141516171819202122public Context CreateContext(IFeatureCollection contextFeatures)&#123; //省略其他实现代码 return new Context &#123; HttpContext = _httpContextFactory.Create(contextFeatures), Scope = ..., StartTimestamp = ... &#125;;&#125; public Task ProcessRequestAsync(Context context)&#123; Return _application(context.HttpContext);&#125; public void DisposeContext(Context context, Exception exception)&#123; //省略其他实现代码 context.Scope.Dispose(); _httpContextFactory.Dispose(context.HttpContext);&#125; CreateContext 直接利用私有字段 _httpContextFactory 创建一个 HttpContext 对象并将其赋值给 Context 的同名属性。 ProcessRequestAsync 方法则使用 HttpContext 传入 RequestDelegate 委托。 DisposeContext 方法执行时 Context 属性的 Scope 会率先被释放，此后 调用 IHttpContextFactory.Dispose 方法释放 HttpContext 对象。 KestrelServer跨平台是 ASP.NET Core 一个显著的特性，而 KestrelServer 是目前微软推出的唯一一个能够真正跨平台的服务器。KestrelServer 基于 KestrelEngine 的网络引擎实现对请求的监听，接收和响应。KetrelServer 之所以可以跨平台，在于 KestrelEngine 是在 libuv 跨平台网络库上开发的。 libuv 是基于 Unix 系统针对事件循环和事件模型的网络库 libev 开发的。libev 不支持 Windows，有人在 libev 之上创建了一个抽象层以屏蔽平台之间的差异，这个抽象层就是 libuv。libuv 在 Windows 平台上使用 IOCP 的形式实现，到目前为止，libuv 已经支持更多除 Unix 和 Windows 以外的平台了，如 Linux(2.6)、MacOS 和 Solaris (121以及之后的版本)。下图揭示了 libuv 针对 Unix 和 Windows 的跨平台实现原理。 以下是 KestrelServer 类型的定义:123456789public class KestrelServer : IServer&#123; public IFeatureCollection Features &#123; get; &#125; public KestrelServerOptions Options &#123; get; &#125; public KestrelServer(IOptions&lt;KestrelServerOptions&gt; options, IApplicationLifetime applicationLifetime, ILoggerFactory loggerFactory); public void Dispose(); public void Start&lt;TContext&gt;(IHttpApplication&lt;TContext&gt; application);&#125; 除了实现接口 IServer 定义的 Features 属性之外，KestrelServer 还包含一个类型为 KestrelServerOptions 的只读属性 Options。这个属性表示 KestrelServer 的配置信息，构造函数通过输入参数 IOptions&lt;KestrelServerOptions&gt; 对其进行初始化，这里同样采用 Options 模式。例如可以通过一个 JSON 文件来配置 KestrelServer:12345&#123; "noDelay" : false, "shutdownTimeout" : "00:00:10", "threadCount" : 10&#125; 构造函数的另外两个参数 - IApplicationLifetime 与与应用的生命周期管理有关， ILoggerFactory 则用于创建记录日志的 Logger。 通常，通过调用 WebHostBuilder 的 UseKestrel 扩展方法来注册 KestrelServer。UseKestrel 方法有两个重载，其中一个接收一个类型为 Action&lt;KestrelServerOptions&gt; 的参数，通过赋值该参数直接完成对 KestrelServer 的配置。代码如下:12345public static class WebHostBuilderKestrelExtensions&#123; public static IWebHostBuilder UseKestrel(this IWebHostBuilder hostBuilder); public static IWebHostBuilder UseKestrel(this IWebHostBuilder hostBuilder, Action&lt;KestrelServerOptions&gt; options);&#125;由于服务器负责监听，接收和响应请求，它是影响整个 Web 应用响应能力和吞吐量最大的因素之一，为了更加有效地使用服务器，可以根据具体的网络负载状况对其作针对性的设置。现在来看看 KestrelServerOptions 类型的定义:123456789public class KestrelServerOptions&#123; //省略其他成员 public int MaxPooledHeaders &#123; get; set; &#125; public int MaxPooledStreams &#123; get; set; &#125; public bool NoDelay &#123; get; set; &#125; public TimeSpan ShutdownTimeout &#123; get; set; &#125; public int ThreadCount &#123; get; set; &#125;&#125; ServerAddressesFeatureKestrelServer 默认采用 http://localhost:5000 作为监听地址，服务器的监听地址可以显式指定，其通过 IServerAddressesFeature 提供支持。服务器接口 IServer 中定义了一个类型为 IFeatureCollection 的只读属性 Features，它表示当前服务器的特性集合，ServerAddressesFeature 作为一个重要的特性，就包含在这个集合中。该接口只有一个唯一的只读属性返回服务器的监听地址列表。ASP.NET Core 默认使用 ServerAddressesFeature 类型实现 IServerAddressesFeature 接口，定义如下:123456789public interface IServerAddressesFeature&#123; ICollection&lt;string&gt; Addresses &#123; get; &#125;&#125; public class ServerAddressesFeature : IServerAddressesFeature&#123; public ICollection&lt;string&gt; Addresses &#123; get; &#125;&#125;WebHost 通过依赖注入创建的服务器的 Features 属性中会默认包含一个 ServerAddressesFeature 对象。WebHost 会将显式指定的地址(一个或者多个)添加到该对象的监听地址列表中。地址列表其作为配置项保存在一个 Configuration 对象上，配置项对应的 Key 为 urls，可以通过 WebHostDefaults 的静态只读属性 ServerUrlsKey 返回这个 Key。123456new WebHostBuilder() .UseSetting(WebHostDefaults.ServerUrlsKey, "http://localhost:3721/") .UseKestrel() .UseStartup&lt;Startup&gt;() .Build() .Run(); WebHost 的配置最初来源于创建它的 WebHostBuilder，WebHostBuilder 提供了一个 UseSettings 方法来设置某个配置项的值。对监听地址的显式设置，最直接的编程方式是调用 WebHostBuilder 的扩展方法 UseUrls，该方法的实现逻辑与上面完全一致:12345public static class WebHostBuilderExtensions&#123; public static IWebHostBuilder UseUrls(this IWebHostBuilder hostBuilder, params string[] urls) =&gt;hostBuilder.UseSetting(WebHostDefaults.ServerUrlsKey, string.Join(ServerUrlsSeparator, urls)) ; &#125; WebHostASP.NET Core 管道是由作为应用程序宿主的 WebHost 对象创建出来的。应用的启动和关闭是通过启动或者关闭对应 WebHost 的方式实现的。IWebHost 接口定义了如下三个基本成员:123456public interface IWebHost : IDisposable&#123; void Start(); IFeatureCollection ServerFeatures &#123; get; &#125; IServiceProvider Services &#123; get; &#125;&#125;Start 方法用于启动宿主程序。编程中通常会调用它的一个扩展方法 Run 来启动 WebHost，Run 方法会在内部调用 Start 方法。当 WebHost 启动后，服务器立即开始监听请求。IWebHost 接口的默认实现类是 WebHost，它总是由一个 WebHostBuilder 对象创建，WebHost 的构造函数依赖 4 个参数:12345678910111213141516171819202122232425262728293031323334353637public class WebHost : IWebHost&#123; private IServiceCollection _appServices; private IServiceProvider _hostingServiceProvider; private WebHostOptions _options; private IConfiguration _config; private ApplicationLifetime _applicationLifetime; public IServiceProvider Services &#123; get; private set; &#125; public IFeatureCollection ServerFeatures &#123; get &#123; return this.Services.GetRequiredService&lt;IServer&gt;()?.Features; &#125; &#125; public WebHost(IServiceCollection appServices, IServiceProvider hostingServiceProvider, WebHostOptions options, IConfiguration config) &#123; _appServices = appServices; _hostingServiceProvider = hostingServiceProvider; _options = options; _config = config; _applicationLifetime = new ApplicationLifetime(); appServices.AddSingleton&lt;IApplicationLifetime&gt;(_applicationLifetime); &#125; public void Dispose() &#123; _applicationLifetime.StopApplication(); (this.Services as IDisposable)?.Dispose(); _applicationLifetime.NotifyStopped(); &#125; public void Start() &#123; &#125;&#125; appServices 从直接注册到 WebHostBuilder 上的服务而来 hostingServiceProvider 是由 appServices 创建的IServiceProvider。 只读属性 Services 返回一个 ServiceProvider 对象，其利用构造函数传入的 ServiceCollection 对象创建。 只读属性 ServerFeatures 返回服务器的特性集合，而服务器本身使用 ServiceProvider 获得 Dispose 方法释放服务器对象，并利用 ApplicationLifetime 发送相应的信号。 WebHostOptions一个 WebHostOptions 对象为构建的 WebHost 对象提供一些预定义的选项，这些选项很重要，它们决定了由 WebHost 构建的管道进行内容加载以及异常处理等方面的行为。以下是其类型定义:12345678910111213public class WebHostOptions&#123; public string ApplicationName &#123; get; set; &#125; public bool DetailedErrors &#123; get; set; &#125; public bool CaptureStartupErrors &#123; get; set; &#125; public string Environment &#123; get; set; &#125; public string StartupAssembly &#123; get; set; &#125; public string WebRoot &#123; get; set; &#125; public string ContentRootPath &#123; get; set; &#125; public WebHostOptions() public WebHostOptions(IConfiguration configuration) &#125;可以将这些选项定义在配置中，并利用 Options 模式创建一个 WebHostOptions 对象。 构建管道Start 方法真正启动 WebHost:123456789101112131415161718192021222324252627282930313233public void Start()&#123; //注册服务 IStartup startup = _hostingServiceProvider.GetRequiredService&lt;IStartup&gt;(); this.Services = startup.ConfigureServices(_appServices); //注册中间件 Action&lt;IApplicationBuilder&gt; configure = startup.Configure; configure = this.Services.GetServices&lt;IStartupFilter&gt;().Reverse().Aggregate(configure, (next, current) =&gt; current.Configure(next)); IApplicationBuilder appBuilder = this.Services.GetRequiredService&lt;IApplicationBuilder&gt;(); configure(appBuilder); //为服务器设置监听地址 IServer server = this.Services.GetRequiredService&lt;IServer&gt;(); IServerAddressesFeature addressesFeature = server.Features.Get&lt;IServerAddressesFeature&gt;(); if (null != addressesFeature &amp;&amp; !addressesFeature.Addresses.Any()) &#123; string addresses = _config["urls"] ?? "http://localhost:5000"; foreach (string address in addresses.Split(';')) &#123; addressesFeature.Addresses.Add(address); &#125; &#125; //启动服务器 RequestDelegate application = appBuilder.Build(); ILogger logger = this.Services.GetRequiredService &lt;ILogger&lt;MyWebHost&gt;&gt;(); DiagnosticSource diagnosticSource = this.Services.GetRequiredService&lt;DiagnosticSource&gt;(); IHttpContextFactory httpContextFactory = this.Services.GetRequiredService&lt;IHttpContextFactory&gt;(); server.Start(new HostingApplication(application, logger, diagnosticSource, httpContextFactory)); //对外发送通知 _applicationLifetime.NotifyStarted();&#125; 注册服务: Start 方法首先通过 ServiceProvider 获取 Startup 的实例，并调用 ConfigureServices 注册所有服务。 注册中间件: 使用 ServiceProvider 获取所有注册的 StartupFilter，并结合之前提取的 Startup 对象创建一个注册中间件的委托(Action&lt;IApplicationBuilder&gt;)。从 ServiceProvider 获取 ApplicationBuilder 对象作为参数传入该委托，完成中间件的注册。 设置服务器监听地址: 使用 ServiceProvider 提取注册在 WebHostBuilder 上的服务器对象，从该对象的 Features 属性中提取 IServerAddressesFeature 对象，从配置中提取显式指定的监听地址，将其逐个加入到 IServerAddressesFeature 的 Addresses 集合中。如果没有任何显式指定的监听地址，那么默认值为 http://localhost:5000。 启动服务器: 准备就绪的服务器由 IServer.Start 方法启动，该方法接收一个 HttpApplication&lt;TContext&gt; 作为参数，创建该接口的默认实现者 HostingApplication 需要 4 个参数: RequestDelegate: 中间件链表，通过 IApplicationBuilder.Build 获取。 Logger: 日志记录器，通过 ServiceProvider 获取。 DiagnosticSource: 通过 ServiceProvider 获取。 HttpContextFactory: Http 上下文工厂，通过 ServiceProvider 获取。 发布通知: 服务器成功启动之后，向外发送通知 WebHostBuilderWebHostBuilder 是 WebHost 的创建者，IWebHostBuilder 接口除了定义用来创建 WebHost 的核心方法 Build 之外，还定义了其他一些方法:123456789public interface IWebHostBuilder&#123; IWebHost Build(); IWebHostBuilder ConfigureServices(Action&lt;IServiceCollection&gt; configureServices); IWebHostBuilder UseLoggerFactory(ILoggerFactory loggerFactory); IWebHostBuilder ConfigureLogging(Action&lt;ILoggerFactory&gt; configureLogging); string GetSetting(string key); IWebHostBuilder UseSetting(string key, string value);&#125;ASP.NET Core 有两种注册服务的途径，一种是将服务注册实现在启动类的 ConfigureServices 方法中，另一种就是调用 IWebHostBuilder 的 ConfigureServices 方法。前者实际上是在 WebHost 启动时提取 Startup 对象调用其 ConfigureServices 进行注册，而 IWebHostBuilder.ConfigureServices 直接将服务提供给创建的 WebHost。 UseLoggerFactory 设置一个默认的 ILoggerFactory 对象，ConfigureLogging 则对 ILoggerFactory 进行配置，具体参见日志系统。 IWebHostBuilder 的默认实现类型时 WebHostBuilder，以下代码展示了除 Build 方法以外的其他成员的实现:1234567891011121314151617181920212223242526272829303132333435363738public interface IWebHostBuilderpublic class WebHostBuilder : IWebHostBuilder&#123; private List&lt;Action&lt;ILoggerFactory&gt;&gt; _configureLoggingDelegates = new List&lt;Action&lt;ILoggerFactory&gt;&gt;(); private List&lt;Action&lt;IServiceCollection&gt;&gt; _configureServicesDelegates = new List&lt;Action&lt;IServiceCollection&gt;&gt;(); private ILoggerFactory _loggerFactory = new LoggerFactory(); private IConfiguration _config = new ConfigurationBuilder().AddEnvironmentVariables("ASPNETCORE_").Build(); public IWebHostBuilder ConfigureLogging(Action&lt;ILoggerFactory&gt; configureLogging) &#123; _configureLoggingDelegates.Add(configureLogging); return this; &#125; public IWebHostBuilder ConfigureServices(Action&lt;IServiceCollection&gt; configureServices) &#123; _configureServicesDelegates.Add(configureServices); return this; &#125; public string GetSetting(string key) &#123; return _config[key]; &#125; public IWebHostBuilder UseLoggerFactory(ILoggerFactory loggerFactory) &#123; _loggerFactory = loggerFactory; return this; &#125; public IWebHostBuilder UseSetting(string key, string value) &#123; _config[key] = value; return this; &#125; ...&#125;默认创建了一个 Configuration 类型的字段 _config 表示应用使用的配置，它默认采用环境变量(用于筛选环境变量的前缀为ASPNETCORE_)作为配置源，GetSetting 和 UseSetting 方法都在内部操作这个字段。另一个字段 _loggerFactory 表示默认使用的 ILoggerFactory，UseLoggerFactory 方法指定的 LoggerFactory 用来对这个字段进行赋值。ConfigureLogging 和 ConfigureServices 仅仅将传入的委托对象保存在一个集合中。 Build 方法实现创建 WebHost 对象并注册必要的服务，以下列出这些服务的不完全列表: 用于注册服务和中间件的 Startup 对象。 用来创建 Logger 的 LoggerFactory 对象 构建中间件链表的 ApplicationBuilder 对象 创建 HTTP 上下文的 HttpContextFactory 对象 用户实现诊断功能的 DiagnosticSource 对象 用来保存承载环境的 HostingEnvironment 对象 以下代码展示了 Build 方法的实现:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class WebHostBuilder : IWebHostBuilder&#123; private List&lt;Action&lt;ILoggerFactory&gt;&gt; _configureLoggingDelegates = new List&lt;Action&lt;ILoggerFactory&gt;&gt;(); private List&lt;Action&lt;IServiceCollection&gt;&gt; _configureServicesDelegates = new List&lt;Action&lt;IServiceCollection&gt;&gt;(); private ILoggerFactory _loggerFactory = new LoggerFactory(); private IConfiguration _config = new ConfigurationBuilder().AddInMemoryCollection().Build(); public IWebHost Build() &#123; //根据配置创建WebHostOptions WebHostOptions options = new WebHostOptions(_config); //注册服务IStartup IServiceCollection services = new ServiceCollection(); if (!string.IsNullOrEmpty(options.StartupAssembly)) &#123; Type startupType = StartupLoader.FindStartupType(options.StartupAssembly, options.Environment); if (typeof(IStartup).GetTypeInfo().IsAssignableFrom(startupType)) &#123; services.AddSingleton(typeof(IStartup), startupType); &#125; else &#123; services.AddSingleton&lt;IStartup&gt;(_ =&gt; new ConventionBasedStartup(StartupLoader.LoadMethods(_, startupType, options.Environment))); &#125; &#125; //注册ILoggerFactory foreach (var configureLogging in _configureLoggingDelegates) &#123; configureLogging(_loggerFactory); &#125; services.AddSingleton&lt;ILoggerFactory&gt;(_loggerFactory); //注册服务IApplicationBuilder，DiagnosticSource和IHttpContextFactory services .AddSingleton&lt;IApplicationBuilder&gt;(_ =&gt; new ApplicationBuilder(_)) .AddSingleton&lt;DiagnosticSource&gt;(new DiagnosticListener("Microsoft.AspNetCore")) .AddSingleton&lt;IHttpContextFactory, HttpContextFactory&gt;() .AddOptions() .AddLogging() .AddSingleton&lt;IHostingEnvironment, HostingEnvironment&gt;() .AddSingleton&lt;ObjectPoolProvider, DefaultObjectPoolProvider&gt;(); //注册用户调用ConfigureServices方法设置的服务 foreach (var configureServices in _configureServicesDelegates) &#123; configureServices(services); &#125; //创建MyWebHost return new WebHost(services, services.BuildServiceProvider(), options, _config); &#125; &#125; 几个常用的扩展方法除了使用 GetSetting 和 UseSetting 方法来以键值对的形式来获取和设置配置项，还可以通过 UseConfiguration 扩展方法直接指定一个 IConfiguration 对象作为参数，该对象会原封不动的拷贝至内部的配置项中，其内部依旧是调用了 UseSettings 方法来实现的。1234public static class HostingAbstractionsWebHostBuilderExtensions&#123; public static IWebHostBuilder UseConfiguration(this IWebHostBuilder hostBuilder, IConfiguration configuration);&#125;WebHostBuilder 在创建 WebHost 的时候需要一个 WebHostOptions 对象，为了方便设置 WebHostOptions 的配置项，ASP.NET Core 定义了一系列扩展方法，这些方法最终也是通过 UseSettings 方法。1234567891011public static class HostingAbstractionsWebHostBuilderExtensions&#123; public static IWebHostBuilder CaptureStartupErrors(this IWebHostBuilder hostBuilder, bool captureStartupErrors); public static IWebHostBuilder UseContentRoot(this IWebHostBuilder hostBuilder, string contentRoot); public static IWebHostBuilder UseEnvironment(this IWebHostBuilder hostBuilder, string environment); public static IWebHostBuilder UseStartup(this IWebHostBuilder hostBuilder, string startupAssemblyName); public static IWebHostBuilder UseWebRoot(this IWebHostBuilder hostBuilder, string webRoot); public static IWebHostBuilder UseUrls(this IWebHostBuilder hostBuilder, params string[] urls); public static IWebHostBuilder UseServer(this IWebHostBuilder hostBuilder, IServer server); public static IWebHostBuilder UseUrls(this IWebHostBuilder hostBuilder, params string[] urls); &#125; HttpContext对于管道来说，请求的接收者和最终响应者都是服务器，服务器接收到请求之后会创建与之对应的「原始上下文」，请求的响应也通过这个「原始上下文」来完成。 但对于建立在管道上的应用程序来说，它们不需要关注管道究竟采用了何种类型的服务器，更不会关注由这个服务器创建的「原始上下文」。ASP.NET Core 定义了 HttpContext 抽象类来描述当前请求的上下文，对当前上下文的抽象解除了管道对具体服务器类型的依赖，这使得可以为 ASP.NET Core 应用程序自由地选择寄宿(Hosting)方式，而不是像传统的 ASP.NET 应用一样只能寄宿在 IIS 中。抽象的 HttpContext 为请求处理提供了标准化的方式，这使得位于管道中的中间件与具体的服务器类型进行了解耦，中间件只要遵循标准来实现其自身的逻辑即可。HttpContext 包含了当前请求的所有细节，可以直接利用它完成对请求的响应:12345678910111213141516public abstract class HttpContext&#123; public abstract IFeatureCollection Features &#123; get; &#125; public abstract HttpRequest Request &#123; get; &#125; public abstract HttpResponse Response &#123; get; &#125; public abstract ConnectionInfo Connection &#123; get; &#125; public abstract WebSocketManager WebSockets &#123; get; &#125; public abstract AuthenticationManager Authentication &#123; get; &#125; public abstract ClaimsPrincipal User &#123; get; set; &#125; public abstract IDictionary&lt;object, object&gt; Items &#123; get; set; &#125; public abstract IServiceProvider RequestServices &#123; get; set; &#125; public abstract CancellationToken RequestAborted &#123; get; set; &#125; public abstract string TraceIdentifier &#123; get; set; &#125; public abstract ISession Session &#123; get; set; &#125; public abstract void Abort();&#125; 当需要中止对请求的处理时，可通过为 RequestAborted 属性设置一个 CancellationToken 对象将终止通知发送给管道。如果需要对整个管道共享一些与当前上下文相关的数据，可以将它保存在 Items 属性表示的字典中。RequestServices 属性返回一个 IServiceProvider 对象，该对象为中间件提供注册的服务实例，只要相应的服务事先注册到指定的服务接口上，就可以利用这个 IServiceProvider 来获取对应的服务对象。 表示请求和响应的 HttpRequest 和 HttpResponse 同样是抽象类:123456789101112131415161718192021222324252627282930313233343536373839404142public abstract class HttpRequest&#123; public abstract QueryString QueryString &#123; get; set; &#125; public abstract Stream Body &#123; get; set; &#125; public abstract string ContentType &#123; get; set; &#125; public abstract long? ContentLength &#123; get; set; &#125; public abstract IRequestCookieCollection Cookies &#123; get; set; &#125; public abstract IHeaderDictionary Headers &#123; get; &#125; public abstract string Protocol &#123; get; set; &#125; public abstract IQueryCollection Query &#123; get; set; &#125; public abstract IFormCollection Form &#123; get; set; &#125; public abstract PathString Path &#123; get; set; &#125; public abstract PathString PathBase &#123; get; set; &#125; public abstract HostString Host &#123; get; set; &#125; public abstract bool IsHttps &#123; get; set; &#125; public abstract string Scheme &#123; get; set; &#125; public abstract string Method &#123; get; set; &#125; public abstract HttpContext HttpContext &#123; get; &#125; public abstract bool HasFormContentType &#123; get; &#125; public abstract Task&lt;IFormCollection&gt; ReadFormAsync(CancellationToken cancellationToken = default(CancellationToken));&#125;public abstract class HttpResponse&#123; public abstract HttpContext HttpContext &#123; get; &#125; public abstract int StatusCode &#123; get; set; &#125; public abstract IHeaderDictionary Headers &#123; get; &#125; public abstract Stream Body &#123; get; set; &#125; public abstract long? ContentLength &#123; get; set; &#125; public abstract string ContentType &#123; get; set; &#125; public abstract IResponseCookies Cookies &#123; get; &#125; public abstract bool HasStarted &#123; get; &#125; public abstract void OnCompleted(Func&lt;object, Task&gt; callback, object state); public virtual void OnCompleted(Func&lt;Task&gt; callback); public abstract void OnStarting(Func&lt;object, Task&gt; callback, object state); public virtual void OnStarting(Func&lt;Task&gt; callback); public virtual void Redirect(string location); public abstract void Redirect(string location, bool permanent); public virtual void RegisterForDispose(IDisposable disposable);&#125; FeatureCollection在 ASP.NET Core 管道式处理设计中，特性是一个非常重要的概念，它是实现抽象化的 HttpContext 的途径，不同类型的服务器在接收到请求时会创建一个「原始上下文」，接下来服务器将「原始上下文」的操作封装成一系列标准的特性对象(IFeature)，进而封装成一个 FeatureCollection 对象，当调用 DefaultHttpContext 相应的属性和方法时，其内部又借助封装的特性对象去操作「原始上下文」。 当原始上下文被创建出来之后，服务器会将它封装成一系列标准的特性对象，HttpContext 正是对这些特性对象的封装。这些特性对象对应的类型均实现了某个预定义的标准接口，接口定义了相应的属性来读写原始上下文中描述的信息，还定义了相应的方法来操作原始上下文。HttpContext 的 Features 属性返回这组特性对象的集合，类型为 IFeatureCollection，该接口用于描述某个对象所具有的一组特性，我们可以将其视为一个 Dictionary&lt;Type, object&gt; 对象，字典的 Value 代表特性对象，Key 则表示该对象的注册类型(特性描述对象的具体类型，具体类型的基类或者接口)。调用 Set 方法来注册特性对象，而 Get 方法则根据指定的注册类型得到对应的特性对象。12345678public interface IFeatureCollection : IEnumerable&lt;KeyValuePair&lt;Type, object&gt;&gt;, IEnumerable&#123; object this[Type key] &#123; get; set; &#125; bool IsReadOnly &#123; get; &#125; int Revision &#123; get; &#125; TFeature Get&lt;TFeature&gt;(); void Set&lt;TFeature&gt;(TFeature instance);&#125; 特性对象的注册和获取也可以通过的索引器来完成。如果 IsReadOnly 属性返回 True，便不能注册新的特性或修改已经注册的特性。只读属性 Revision 可视为 IFeatureCollection 对象的版本，注册新特性或修改现有的特性都将改变这个属性的值。 IFeatureCollection 的默认实现类型是 FeatureCollection:123456public class FeatureCollection : IFeatureCollection&#123; //其他成员 public FeatureCollection(); public FeatureCollection(IFeatureCollection defaults);&#125;FeatureCollection 类型的 IsReadOnly 总是返回 False，如果调用无参构造函数，它的 Revision 默认返回 0。如果调用第二个构造函数，其 Revision 属性将延续传入参数的 IFeatureCollection.Revision 的值，并采用递增来修改其值。 DefaultHttpContextASP.NET Core 使用 DefaultHttpContext 类型作为 HttpContext 的默认实现，原始上下文由「特性集合」来创建 HttpContext 的策略就体现在该类型上。DefaultHttpContext 的构造函数如下:1234public class DefaultHttpContext : HttpContext&#123; public DefaultHttpContext(IFeatureCollection features);&#125;无论是组成管道的中间件还是建立在管道上的应用程序，都统一采用 DefaultHttpContext 对象来获取请求信息，并利用它完成对请求的响应。针对 DefaultHttpContext 的调用(属性或方法)最终都转发给具体服务器创建的「原始上下文」，构造函数接收的 FeatureCollection 对象所代表的特性集合是这两个上下文对象进行沟通的唯一渠道。定义在 DefaultHttpContext 中的所有属性几乎都具有一个对应的特性，这些特性又都对应一个接口。下表列出了部分特性接口以及 DefaultHttpContext 对应的属性: 接口 属性 描述 IHttpRequestFeature Request 获取描述请求的基本信息 IHttpResponsetFeature Response 控制对请求的响应 IHttpAuthenticationFeature AuthenticationManger/User 提供用户认证的 AuthenticationHandler 对象和表示当前用户的 ClaimsPrincipal 对象 IHttpConnectionFeature Connection 提供描述当前 HTTP 连接的基本信息。 IItemsFeature Items 提供客户代码存放关于当前请求的对象容器。 IHttpRequestLifetimeFeature RequestAborted 传递请求处理取消通知和中止当前请求处理。 IServiceProvidersFeature RequestServices 提供根据服务注册创建的 ServiceProvider。 ISessionFeature Session 提供描述当前会话的 Session 对象。 IHttpRequestIdentifierFeature TraceIdentifier 为追踪日志(Trace)提供针对当前请求的唯一标识。 IHttpWebSocketFeature WebSockets 管理 WebSocket 其中最重要的两个接口为表示请求和响应的 IHttpRequestFeature 和 IHttpResponseFeature。这两个接口分别与抽象类 HttpRequest 和 HttpResponse 具有一致的定义。 1234567891011121314151617181920212223public interface IHttpRequestFeature&#123; string Protocol &#123; get; set; &#125; string Scheme &#123; get; set; &#125; string Method &#123; get; set; &#125; string PathBase &#123; get; set; &#125; string Path &#123; get; set; &#125; string QueryString &#123; get; set; &#125; string RawTarget &#123; get; set; &#125; IHeaderDictionary Headers &#123; get; set; &#125; Stream Body &#123; get; set; &#125;&#125;public interface IHttpResponseFeature&#123; int StatusCode &#123; get; set; &#125; string ReasonPhrase &#123; get; set; &#125; IHeaderDictionary Headers &#123; get; set; &#125; Stream Body &#123; get; set; &#125; bool HasStarted &#123; get; &#125; void OnCompleted(Func&lt;object, Task&gt; callback, object state); void OnStarting(Func&lt;object, Task&gt; callback, object state);&#125; DefaultHttpContext 对象中表示请求和响应的 Request 和 Response 属性就是分别提取 HttpRequestFeature 和 HttpResponseFeature 特性创建出 DefaultHttpRequest 和 DefaultHttpResponse 对象，它们分别继承自 HttpRequest 和 HttpResponse。以下是伪代码的实现:1234567891011121314151617public class DefaultHttpRequest : HttpRequest&#123; public IHttpRequestFeature RequestFeature &#123; get; &#125; public DefaultHttpRequest(DefaultHttpContext context) &#123; this.RequestFeature = context.HttpContextFeatures.Get&lt;IHttpRequestFeature&gt;(); &#125; public override Uri Url &#123; get &#123; return this.RequestFeature.Url; &#125; &#125; public override string PathBase &#123; get &#123; return this.RequestFeature.PathBase; &#125; &#125;&#125; 1234567891011121314151617181920212223242526public class DefaultHttpResponse : HttpResponse&#123; public IHttpResponseFeature ResponseFeature &#123; get; &#125; public override Stream OutputStream &#123; get &#123; return this.ResponseFeature.OutputStream; &#125; &#125; public override string ContentType &#123; get &#123; return this.ResponseFeature.ContentType; &#125; set &#123; this.ResponseFeature.ContentType = value; &#125; &#125; public override int StatusCode &#123; get &#123; return this.ResponseFeature.StatusCode; &#125; set &#123; this.ResponseFeature.StatusCode = value; &#125; &#125; public DefaultHttpResponse(DefaultHttpContext context) &#123; this.ResponseFeature = context.HttpContextFeatures.Get&lt;IHttpResponseFeature&gt;(); &#125;&#125; HttpContextFactory在服务器接收到请求时，它并不是直接利用原始上下文来创建 HttpContext 对象，而是通过 HttpContextFactory 来创建。IHttpContextFactory 接口除了定义创建 HttpContext 对象的 Create 方法之外，还定义了一个 Dispose 方法来释放指定的 HttpContext 对象。 HttpContextFactory 类是该接口的默认实现者，由它的 Create 方法创建并返回的是一个 DefaultHttpContext 对象:123456789101112public interface IHttpContextFactory&#123; HttpContext Create(IFeatureCollection featureCollection); void Dispose(HttpContext httpContext);&#125;public class HttpContextFactory : IHttpContextFactory&#123; //省略其他成员 public HttpContext Create(IFeatureCollection featureCollection); public void Dispose(HttpContext httpContext);&#125; 以上涉及的类型和接口和所在的命名空间： 类型或接口 命名空间 HttpContext Microsoft.AspNetCore.Http HttpRequest Microsoft.AspNetCore.Http HttpResponse Microsoft.AspNetCore.Http DefaultHttpRequest Microsoft.AspNetCore.Http.Internal DefaultHttpResponse Microsoft.AspNetCore.Http.Internal IHttpRequestFeature Microsoft.AspNetCore.Http.Features IHttpResponseFeature Microsoft.AspNetCore.Http.Features 以及它们之间的 UML 关系图: ApplicationBulder创建 WebHost 的 WebHostBuilder 提供了一个用于管道定制的 Configure 方法，它利用 ApplicationBuilder 参数进行中间件的注册。中间件在请求处理流程中体现为一个类型为 Func&lt;RequestDelegate，RequestDelegate&gt; 的委托对象，RequestDelegate 相当于一个 Func&lt;HttpContext, Task&gt; 对象，它体现了针对 HttpContext 所进行的某项操作，进而代表某个中间件针对请求的处理过程。那为何我们不直接用一个 RequestDelegate 对象来表示一个中间件，而将它表示成一个 Func&lt;RequestDelegate，RequestDelegate&gt; 对象呢？ 在多数情况下，具体的请求处理需要注册多个不同的中间件，这些中间件按照注册时间的顺序进行排列构成了管道。对于单个中间件来说，在它完成了自身的请求处理任务之后，需要将请求传递给下一个中间件作后续的处理。Func&lt;RequestDelegate，RequestDelegate&gt; 中作为输入参数的 RequestDelegate 对象代表一个委托链，体现了后续中间件对请求的处理。当某个中间件将自身实现的请求处理任务添加到这个委托链中，新的委托链将作为这个 Func&lt;RequestDelegate，RequestDelegate&gt; 对象的返回值。以上图为例，如果用一个 Func&lt;RequestDelegate，RequestDelegate&gt; 来表示中间件 B，那么作为输入参数的 RequestDelegate 对象代表的是中间件 C 对请求的处理操作，而返回值则代表 B 和 C 先后对请求的处理操作。如果一个 Func&lt;RequestDelegate，RequestDelegate&gt; 代表第一个从服务器接收请求的中间件(比如 A)，那么执行该委托对象返回的 RequestDelegate 实际上体现了整个管道对请求的处理。 现在，来看看 IApplicationBuilder 接口的定义:12345678910public interface IApplicationBuilder&#123; IServiceProvider ApplicationServices &#123; get; set; &#125; IFeatureCollection ServerFeatures &#123; get; &#125; IDictionary&lt;string, object&gt; Properties &#123; get; &#125; RequestDelegate Build(); IApplicationBuilder New(); IApplicationBuilder Use(Func&lt;RequestDelegate, RequestDelegate&gt; middleware);&#125; Use 方法实现对中间件的注册，而 Build 方法则将所有注册的中间件转换成一个 RequestDelegate 对象。除了这两个核心方法，IApplicationBuilder 接口还定义了三个属性，其中 ApplicationServices 返回根据最初服务注册生成的 ServiceProvider 对象，而 ServerFeatures 属性返回的 FeatureCollection 对象是描述 Server 的特性集合。字典类型的 Properties 属性供用户存储任意自定义的属性，而 New 方法会根据自己「克隆」出一个新的 ApplicationBuilder 对象，这两个 ApplicationBuilder 对象应用具有相同的属性集合。 从编程便利性考虑，很多预定义的中间件类型都具有对应的用来注册的扩展方法，比如 UseStaticFiles 注册处理静态文件请求的中间件。 ApplicationBuilder 类型是 IApplicationBuilder 的默认实现者，其定义了一个 List&lt;Func&lt;RequestDelegate, RequestDelegate&gt;&gt; 属性来存放所有注册的中间件，Use 方法只需要将指定的中间件添加到这个列表即可，而 Build 方法只需要逆序调用这些中间件对应的 Func&lt;RequestDelegate, RequestDelegate&gt; 对象就能得需要的 RequestDelegate 对象。值得一提的是，Build 方法在中间件链条的尾部添加了一个额外的中间件，该中间件会负责将响应状态码设置为 404，如果没有注册任何对请求作最终响应的中间件(这样的中间件将不会试图调用后续中间件)，整个管道会回复一个状态码为 404 的响应。123456789101112131415161718192021222324public class ApplicationBuilder : IApplicationBuilder&#123; private IList&lt;Func&lt;RequestDelegate, RequestDelegate&gt;&gt; middlewares = new List&lt;Func&lt;RequestDelegate, RequestDelegate&gt;&gt;(); public RequestDelegate Build() &#123; RequestDelegate app = context =&gt; &#123; context.Response.StatusCode = 404; return Task.FromResult(0); &#125;; foreach (var component in middlewares.Reverse()) &#123; app = component(app); &#125; return app; &#125; public IApplicationBuilder Use(Func&lt;RequestDelegate, RequestDelegate&gt; middleware) &#123; middlewares.Add(middleware); return this; &#125;&#125; ApplicationBuilderFactoryIApplicationBuilderFactory 是 ASP.NET Core 用来创建 IApplicationBuilder 的工厂，如下面的代码片段所示，该接口定义了唯一个方法 CreateBuilder 接收 FeatureCollection 对象参数 来创建 IApplicationBuilder 对象，该 IFeatureCollection 对象正是承载与服务器相关特性的集合。ApplicationBuilderFactory 类型是该接口的默认实现者，当 CreateBuilder 方法被调用的时候，它会直接将构造时提供 ServiceProvider 对象和 serverFeatures 参数表示的 IFeatureCollection 对象来创建 ApplicationBuilder 对象。1234567891011121314151617181920public interface IApplicationBuilderFactory&#123; IApplicationBuilder CreateBuilder(IFeatureCollection serverFeatures);&#125; public class ApplicationBuilderFactory : IApplicationBuilderFactory&#123; private readonly IServiceProvider _serviceProvider; public ApplicationBuilderFactory(IServiceProvider serviceProvider) &#123; this._serviceProvider = serviceProvider; &#125; public IApplicationBuilder CreateBuilder(IFeatureCollection serverFeatures) &#123; return new ApplicationBuilder(_serviceProvider, serverFeatures); &#125;&#125; 中间件类型虽然中间件最终体现为一个类型为 Func&lt;RequestDelegate, RequestDelegate&gt; 的委托对象，但是大部分情况下都会将中间件定义成一个单独的类型。中间件类型不要求实现某个接口或继承某个基类，但要遵循几个必要的约定。现在通过 ContentMiddleware 类来看看一个合法的中间件类型应该如何定义。1234567891011121314151617181920public class ContentMiddleare&#123; public RequestDelegate _next; public byte[] _content; public string _contentType; public ContentMiddleare(RequestDelegate next, byte[] content, string contentType) &#123; _next = next; _content = content; _contentType = contentType; &#125; public async Task Invoke(HttpContext context, ILoggerFactory loggerFactory) &#123; loggerFactory.CreateLogger&lt;ContentMiddleare&gt;().LogInformation($"Write content (&#123;_contentType&#125;)"); context.Response.ContentType = _contentType; await context.Response.Body.WriteAsync(_content,0, _content.Length); &#125;&#125;ContentMiddleware 中间件将任何类型的内容响应给客户端，它的 _content 和 _contentType 两个字段分别代表响应内容和媒体类型(内容类型或者 MIME 类型)，它体现了一个典型中间件类型的定义规则或者约定: 应该定义为非静态类。 具有一个公共构造函数。这个构造函数的第一个参数类型必须为 RequestDelegate，代表对请求的后续操作(可以视为下一个注册的中间件) 针对请求的处理定义在一个名为 Invoke 的公共实例方法，其返回类型为 Task。该方法的第一个参数类型为 HttpContext，代表当前 HTTP 上下文。可以为这个方法定义任意数量和类型的额外参数，当这个方法被执行的时候，系统将会采用依赖注入的方式为这些参数赋值。 中间件类型注册中间件类型的注册可以通过调用 IApplicationBuilder 接口的扩展方法 UseMiddleware 和 UseMiddleware&lt;TMiddleware&gt; 来注册。除了指定中间件的类型之外，我们还需要按照顺序指定目标构造函数的全部或部分参数。不过构造函数的第一个参数 RequestDelegate 不需要提供，如果只指定了部分参数，缺失的参数将会通过 ServiceProvider 提供。12345public static class UseMiddlewareExtensions&#123; public static IApplicationBuilder UseMiddleware&lt;TMiddleware&gt;(this IApplicationBuilder app, params object[] args); public static IApplicationBuilder UseMiddleware(this IApplicationBuilder app, Type middleware, params object[] args);&#125;可以按照下面的方式来注册上面定义的 ContentMiddleware 中间件:123new WebHostBuilder() .Configure(app =&gt; app.UseMiddleware&lt;ContentMiddleare&gt;(File.ReadAllBytes("girl.png"),"image/png"))...]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 框架基础 - 日志系统]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-fundamentals-logging%2F</url>
    <content type="text"><![CDATA[参考资料: Logging in ASP.NET Core Logging with Logger Message .NET Core 的日志 本文大纲: 前言 日志模型三要素 创建 Logger 采用依赖注入来创建日志 日志类别(Category) 日志级别(LogLevel) 日志事件 ID(EventId) 日志消息模板(Message Template) 日志过滤 通过配置创建日志过滤规则 以编程方式创建日志过滤规则 日志过滤规匹配算法 日志提供器别名 默认最小日志级别 日志全局过滤器委托 日志区限(Log Scopes) 内置日志提供器 Console 提供器 Debug 提供器 EventSource 提供器 Windows EventLog 提供器 TraceSource 提供器 Azure App Service 提供器 LoggerMessage 模式 LoggerMessage.Define LoggerMessage.DefineScope 前言.NET Core提供了独立的日志模型使我们可以采用统一的 API 来完成针对日志记录的编程，我们同时也可以利用其扩展点对这个模型进行定制，比如可以将第三方日志提供器整合到我们的应用中。 日志模型三要素日志记录编程的核心对象: ILogger: 将日志消息写到对应的目的地(如文件，数据库等) ILoggerFactory: 创建组合式的 Logger，该 Logger 其实是对一组 Logger 的封装，自身并不提供日志写入功能，而是委托内部封装的 Logger 来写日志。 ILoggerProvider: 创建具有写入日志功能的 Logger。 LoggerFactory 可以注册多个 LoggerProvider 对象，在进行日志编程时，我们会利用 LoggerFactory 对象创建 Logger 来写日志，而该对象委托的内部 Logger 则由这些 LoggerProvider 提供。这三者的关系如下: 创建 Logger引入以下 Nuget Package 以实现原始的日志功能: Microsoft.Extensions.Logging.Abstractions: 引入 ILoggerFactory 和 ILogger 接口 Microsoft.Extensions.Logging: 引入 ILoggerFactory 的默认实现 LoggerFactory Microsoft.Extensions.Logging.Console: 引入 ConsoleLoggerProvider Microsoft.Extensions.Logging.Debug: 引入 DebugLoggerProvider System.Text.Encoding.CodePages: 由于 .NET Core 在默认情况下并不支持中文编码，需要在程序启动的时候显式注册一个支持中文编码的 EncodingProvider 首先创建 LoggerFactory 对象，然后通过 AddProvider 方法将一个 ConsoleLoggerProvider 和 DebugLoggerProvider 对象注册到 LoggerFactory 上，这两个 LoggerProvider 的构造函数接收一个 Func&lt;string, LogLevel, bool&gt; 类型的参数，该委托对象的两个输入参数分别代表日志消息的类型和等级，布尔类型的返回值决定创建的 Logger 是否会写入给定的日志消息。由于传入的委托对象总是返回 True，意味着所有级别的日志消息均会被这两个 LoggerProvider 创建的 Logger 对象写入对应的目的地。日志提供器注册完成之后，调用 LoggerFactory 的 CreateLogger 方法创建一个指定类别的 Logger 对象。12345678910111213141516171819class Program&#123; static void Main(string[] args) &#123; // 注册 EncodingProvider 实现对中文编码的支持 Encoding.RegisterProvider(CodePagesEncodingProvider.Instance); Func&lt;string, LogLevel, bool&gt; filter = (category, level) =&gt; true; ILoggerFactory loggerFactory = new LoggerFactory(); loggerFactory.AddProvider(new ConsoleLoggerProvider(filter, false)); loggerFactory.AddProvider(new DebugLoggerProvider(filter)); ILogger logger = loggerFactory.CreateLogger(nameof(Program)); int eventId = 3721; logger.LogInformation(eventId, $"升级到 .NET Core version 1.0.0"); logger.LogWarning(eventId, "并发量接近上限"); logger.LogError(eventId, "数据库连接失败(数据库：&#123;Database&#125;，用户名：&#123;User&#125;)", "TestDb", "sa"); &#125;&#125; 采用依赖注入来创建日志在 ASP.NET Core 应用中，总是以依赖注入的方式来获取相关的服务类型实例，ILoggerFactory 就是服务类型的一种。在创建 ServiceCollection 对象之后，调用 AddLogging() 向其注册日志服务，再从 ServiceCollection 对象中获取 ILoggerFactory 对象，调用 ILoggerFactory 的 AddConsole() 和 AddDebug() 扩展方法完成日志提供器向 ILoggerFactory 的注册。12345678var logger = new ServiceCollection() .AddLogging() // call this extension method to register logging service .BuildServiceProvider() // build service provider to get services .GetService&lt;ILoggerFactory&gt;() // get ILoggerFactory service .AddConsole() // register console logger provider to logger factory .AddDebug() // register debug logger provider to logger factory .CreateLogger(nameof(Program)); // create logger of category 'Program' 同一个 LoggerFactory 可以注册多个 LoggerProvider，当 LoggerFactory 创建出相应的 Logger 对象来写入日志时，日志消息实际上会分发给所有 LoggerProvider。而每条日志消息都携带了日志等级， LoggerProvider 通过其构造函数传入的 Func 委托来过滤不同等级的日志消息，这样就实现了一条日志消息只写入特定的日志提供器的目的地。 日志类别(Category)每一条日志消息都带有日志类别信息，在创建 ILogger 时可以指定类别，类别为任何字符串值，但按照惯例日志类别为类型的完全限定名，例如: “TodoApi.Controllers.TodoController”。 调用 ILoggerFactory.CreateLogger 时可以指定日志类别:1234567891011public class TodoController : Controller&#123; private readonly ITodoRepository _todoRepository; private readonly ILogger _logger; public TodoController(ITodoRepository todoRepository, ILoggerFactory logger) &#123; _todoRepository = todoRepository; _logger = logger.CreateLogger("TodoApi.Controllers.TodoController"); &#125;更多时候使用 ILogger&lt;T&gt; 则更简单:1234567891011public class TodoController : Controller&#123; private readonly ITodoRepository _todoRepository; private readonly ILogger _logger; public TodoController(ITodoRepository todoRepository, ILogger&lt;TodoController&gt; logger) &#123; _todoRepository = todoRepository; _logger = logger; &#125; 日志级别(LogLevel)日志级别由轻至重分别为: Trace = 0: 提供给发开人员用于跟踪和调试的信息，通常包含一些敏感数据，绝不能暴露给用户 Debug = 1: 在开发与调试阶段帮助开发人员分析调试的信息，这些消息通常是短期有效的信息，在部署环境中不会启用该级别 Information = 2: 记录应用程序的正常行为的日志级别，这些消息通常具有长期有效性。 Warning = 3: 记录应用程序运行期间不正常或意外事件的日志，这些行为不会导致应用程序崩溃但需要记录下来以供后续调查。 Error = 4: 记录无法被处理的错误及异常，这些消息指示在单一事务边界内失败，但不影响应用程序的其他部分 Critical = 5: 记录需要立即进行修正的致命错误，最高警戒级别 ASP.NET Core 将框架级别的事件日志以 Debug 级别日志分发给不同的日志提供器。 日志事件 ID(EventId)每记录一条日志，都可以为其指定事件 ID，事件 ID 用于将一系列相互关联的日志消息串起来，例如，将某件产品添加至购物车相关的日志的 ID 可为 1000，而与结账付款的事件 ID 可为 1001。日志事件 ID 以数据的形式将不同的日志进行逻辑分组，方便日后的查阅与分析。 日志消息模板(Message Template)在调用 ILogger.Log() 时，需要为每条日志消息提供消息模板，该消息模板不同于传统 C# 格式化字符串和最新的插值字符串，其中包含命名占位符而不是数字占位符，填充到占位符的顺序又不与其名称相对应，而是按照占位符的顺序，日志框架这样设计是为了让日志提供器能够实现语义化或结构化的日志存储。如果采用以下方式写入日志:123string p1 = "parm1";string p2 = "parm2";_logger.LogInformation("Parameter values: &#123;p2&#125;, &#123;p1&#125;", p1, p2);将会得到的输出结果为:1Parameter values: parm1, parm2 按照笔者的理解，许多日志提供器都采用了将占位符参数以字段的形式进行存储的功能，在消息模板中的占位符既是可以在消息输出中被替代的字符串，也是在结构化存储中的字段信息，当收集到大量的日志数据之后，通过结构化查询语句将大大提供分析效率。 日志过滤可以针对特定的提供器，或类别，或所有提供器或所有类别指定最小记录的日志级别，小于该级别的日志消息将不会分布至相应的日志提供器，同样，可通过将日志级别设置为 LogLevel.None 来忽略所有日志。 通过配置创建日志过滤规则ASP.NET Core 项目模板的代码调用 CreateDefaultBuilder 方法了，该方法默认注册了 Console 和 Debug 提供器，同时告知日志系统查询 Logging 配置块来加载日志配置。1234567891011121314151617181920212223public static void Main(string[] args)&#123; var webHost = new WebHostBuilder() .UseKestrel() .UseContentRoot(Directory.GetCurrentDirectory()) .ConfigureAppConfiguration((hostingContext, config) =&gt; &#123; var env = hostingContext.HostingEnvironment; config.AddJsonFile("appsettings.json", optional: true, reloadOnChange: true) .AddJsonFile($"appsettings.&#123;env.EnvironmentName&#125;.json", optional: true, reloadOnChange: true); config.AddEnvironmentVariables(); &#125;) .ConfigureLogging((hostingContext, logging) =&gt; &#123; logging.AddConfiguration(hostingContext.Configuration.GetSection("Logging")); logging.AddConsole(); logging.AddDebug(); &#125;) .UseStartup&lt;Startup&gt;() .Build(); webHost.Run();&#125;配置数据以日志提供器和类别为单位指定了最小日志级别，例如:123456789101112131415161718192021&#123; "Logging": &#123; "IncludeScopes": false, "Debug": &#123; "LogLevel": &#123; "Default": "Information" &#125; &#125;, "Console": &#123; "LogLevel": &#123; "Microsoft.AspNetCore.Mvc.Razor.Internal": "Warning", "Microsoft.AspNetCore.Mvc.Razor.Razor": "Debug", "Microsoft.AspNetCore.Mvc.Razor": "Error", "Default": "Information" &#125; &#125;, "LogLevel": &#123; "Default": "Debug" &#125; &#125;&#125; 以编程方式创建日志过滤规则考虑以下代码:123456WebHost.CreateDefaultBuilder(args) .UseStartup&lt;Startup&gt;() .ConfigureLogging(logging =&gt; logging.AddFilter("System", LogLevel.Debug) .AddFilter&lt;DebugLoggerProvider&gt;("Microsoft", LogLevel.Trace)) .Build(); 第一个 AddFilter 方法指示所有提供器的 “System” 类别最小日志级别为 Debug。 第二个 AddFilter 方法指示 Debug 日志提供器的 “Microsoft” 类别最小日志级别为 Trace。 日志过滤规匹配算法综合以上配置项和编程方式添加的过滤规则，其可以解释为:]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 框架基础 - 配置系统 Options 模式]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-fundamentals-configuration-options%2F</url>
    <content type="text"><![CDATA[参考资料: Options pattern in ASP.NET Core http://www.cnblogs.com/artech/p/new-config-system-01.html Options Github Source 本文大纲: Options 模式 配置绑定 扩展方法 AddOptions OptionsManager IConfigureOptions ConfigureOptions 扩展方法 Configure 创建 Options 对象 Options 模式在真实的项目中我们大多采用 Options 模式来使用配置，Options 是配置的逻辑结构在对象层面的体现，通常，可以将一个 Configuration 对象绑定为一个 Options 对象。这样的绑定称为「配置绑定」。 配置绑定Microsoft.Extensions.Configuration.Binder 包为 IConfiguration 接口定义了 Bind 扩展方法，该方法接收一个代表 Options 的 object 类型的参数，并将 Configuration 的配置数据绑定到该对象上。1234public static class ConfigurationBinder&#123; public static void Bind(this IConfiguration configuration, object instance);&#125; 配置绑定的目标类型可以是一个简单的基元类型，也可以是一个自定义数据类型，还可以是一个数组、集合或者字典类型。上述 Bind 方法在进行配置绑定的过程中会根据不同的目标类型采用不同的策略。 Options 模式是对依赖注入的应用，通过调用 IServiceCollection 扩展方法 AddOptions 添加 Options 模式的服务注册，再通过 Configure&lt;TOptions&gt; 扩展方法配置目标 Options 的 T 类型。消费方利用 ServiceProvider 得到一个类型为 IOptions&lt;TOptions&gt; 的服务对象后，读取 Value 属性得到由配置绑定生成的 TOptions 实例。1234567IConfiguration config = ...;FormatOptions options = new ServiceCollection() .AddOptions() .Configure&lt;FormatOptions&gt;(config.GetSection("Format")) .BuildServiceProvider() .GetService&lt;IOptions&lt;FormatOptions&gt;&gt;() .Value; 扩展方法 AddOptions当调用 IServiceCollection 的 AddOptions 时，该方法对 IOptions&lt;&gt; 接口注册一个服务，该服务的实现类型为 OptionsManager&lt;TOptions&gt; ，生命周期为 Singleton。配置绑定生成的 Options 对象最终都是通过 OptionsManager&lt;TOptions&gt; 创建的。12345public static IServiceCollection AddOptions(this IServiceCollection services)&#123; services.TryAdd(ServiceDescriptor.Singleton(typeof(IOptions&lt;&gt;), typeof(OptionsManager&lt;&gt;))); return services;&#125;以下是几个相关类型的定义: OptionsManager12345public class OptionsManager&lt;TOptions&gt; : IOptions&lt;TOptions&gt; where TOptions: class, new()&#123; public OptionsManager(IEnumerable&lt;IConfigureOptions&lt;TOptions&gt;&gt; setups); public virtual TOptions Value &#123; get; &#125;&#125; OptionsManager&lt;TOptions&gt; 类型的构造函数接受一个 IConfigureOptions&lt;TOptions&gt; 的集合，Options 对象的创建体现在 Value 属性上。该属性的实现非常简单，它先调用 TOptions 类型的默认无参构造函数(TOptions 代表的类型必须具有一个默认无参构造函数)创建一个空的 TOptions 对象，然后将其传递给构造函数中指定的ConfigureOptions&lt;TOptions&gt; 对象逐个进行转换处理。 IConfigureOptions1234public interface IConfigureOptions&lt;in TOptions&gt; where TOptions: class&#123; void Configure(TOptions options);&#125; IConfigureOptions&lt;TOptions&gt; 接口定义了一个唯一的 Configure 方法，该方法将 Options 对象作为输入参数。 ConfigureOptions123456789101112public class ConfigureOptions&lt;TOptions&gt;: IConfigureOptions&lt;TOptions&gt; where TOptions : class, new()&#123; public Action&lt;TOptions&gt; Action &#123; get; private set; &#125; public ConfigureOptions(Action&lt;TOptions&gt; action) &#123; this.Action = action; &#125; public void Configure(TOptions options) &#123; this.Action(options); &#125;&#125; IConfigure&lt;TOptions&gt; 的默认实现类型 ConfigureOptions&lt;TOptions&gt; 在其构造函数中接收一个 Action&lt;TOptions&gt; 委托对象，再在 Configure 方法中调用该委托实现对 TOptions 的操作。 扩展方法 Configure123456789101112131415public static IServiceCollection Configure&lt;TOptions&gt;(this IServiceCollection services, string name, IConfiguration config, Action&lt;BinderOptions&gt; configureBinder) where TOptions : class&#123; if (services == null) &#123; throw new ArgumentNullException(nameof(services)); &#125; if (config == null) &#123; throw new ArgumentNullException(nameof(config)); &#125; return services.AddSingleton&lt;IConfigureOptions&lt;TOptions&gt;&gt;(new NamedConfigureFromConfigurationOptions&lt;TOptions&gt;(name, config, configureBinder));&#125; 在调用 IServiceCollection 的 Configure 方法时，其内部注册了一个 IConfigureOptions&lt;TOptions&gt; 接口的单例服务，其实际类型为 NamedConfigureFromConfigurationOptions&lt;TOptions&gt;，该类型最终继承自 ConfigureOptions&lt;TOptions&gt; 类型，并且在其构造函数中声明了一个匿名方法作为 Action&lt;TOptions&gt; 的参数传入 NamedConfigureFromConfigurationOptions&lt;TOptions&gt; 中，最终实现配置绑定。12public NamedConfigureFromConfigurationOptions(string name, IConfiguration config, Action&lt;BinderOptions&gt; configureBinder) : base(name, options =&gt; config.Bind(options, configureBinder)) 创建 Options 对象Options 编程模式以两个注册到 ServiceCollection 的服务为核心，这两个服务对应的服务接口分别是: IOptions&lt;TOptions&gt;: 直接提供最终绑定了配置数据的 Options 对象 IConfigureOptions&lt;TOptions&gt;: 在 Options 对象返回之前对它实施相应的初始化工作。 这个两个服务分别通过扩展方法 AddOptions 和 Configure 方法注册到指定的 ServiceCollection 中，服务的真实类型分别是 OptionsManager&lt;TOptions&gt; 和 NamedConfigureFromConfigurationOptions&lt;TOptions&gt;，后者派生于 ConfigureOptions&lt;TOptions&gt;。下图所示的 UML 体现了 Options 模型中涉及的这些接口／类型以及它们之间的关系。]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 框架基础 - 配置系统]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-fundamentals-configuration%2F</url>
    <content type="text"><![CDATA[参考资料: Configuration in ASP.NET Core http://www.cnblogs.com/artech/p/new-config-system-01.html 本文大纲: 前言 从编程角度认识配置系统 从设计角度认识配置系统 配置数据的转换 Configuration 对象 ConfigurationProvider 对象 ConfigurationSource 对象 ConfigurationBuilder 对象 对象关系图 同步 Configuration 的更改 前言配置 API 提供了统一的方式以键值对的形式来读取和设置配置项，配置项在运行时从多个配置源读取信息，并以一个多层级的字典表树来存储这些值。配置源支持以下提供器: 文件格式(INI, JSON 和 XML) 命令行参数 环境变量 内存对象 Secret Manager 存储 Azure Key Vault 自定义配置源提供器 任何一个配置项的值都映射到一个字符串键，框架内置了实现类型将配置项映射到一个 POCO 对象。Options 模式使用 Options 类型代表一组关联的设置项。 从编程角度认识配置系统从编程角度来看，开发人员主要用到了以下三个对象 Configuration: 客户代码最终使用的包含配置项的对象 ConfigurationBuilder: 构建 Configuration 的对象 ConfigurationSource: 配置源对象 读取配置时，根据配置的定义方式创建相应的 ConfigurationSource 对象，并将其注册到创建的 ConfigurationBuilder 对象上，后者利用注册的这些 ConfigurationSource 提供最终的 Configuration 对象。 IConfiguration, IConfigurationSource 和 IConfigurationBuilder 接口分别代表这些对象的抽象，三者均定义在 Microsoft.Extensions.Configuration.Abstractions 包中，默认实现定义在 Microsoft.Extensions.Configuration 包中。 虽然大部分情况下配置从整体来说都具有结构化的层次关系，但是「原子」配置项都以最简单的「键-值对」的形式来体现，并且键和值通常都是字符串。 1234var configBuilder = new ConfigurationBuilder();var configuration = configBuilder .Add(new MemoryConfigurationSource &#123; InitialData = source &#125;) .Build(); 这里首先创建了一个 ConfigurationBuilder 对象，然后将一个 MemoryConfigurationSource 对象注册到它上面，随后调用 IConfigurationBuilder.Build 方法得到一个 IConfiguration 对象。 真实项目中涉及的配置大都具有结构化的层次，Configuration 对象同样具有这样的结构，结构化配置具有一个配置树，一个 Configuration 对象对应这棵树的某个节点，而整棵配置树也可由根节点对应的 Configuration 来表示，以键值对体现的原子配置项对应配置树中不具有子节点的「叶子节点」。 从设计角度认识配置系统配置具有多种原始来源，如内存对象，物理文件，数据库或其他自定义存储介质。如果采用物理文件来存储配置数据，我们还可以选择不同的文件格式(JSON, XML 和 INI)。因此配置的原始数据结构是不确定的，配置模型的最终目的在于提取原始的配置数据并将其转换成一个 Configuration 对象以对客户代码提供统一的编程模型。 配置数据的转换配置从原始结构向逻辑结构的转换需要一种「中间结构」——数据字典，整棵配置树的所有节点都会转换成基于字典的中间结构，最终再完成到 Configuration 对象的转换，父子级节点之间以 : 进行连接。 一个 Configuration 对象具有树形层次结构的意思不是说该类型具有对应的数据成员(字段或属性)定义，而是它提供的 API 「在逻辑上体现出树形层次结构」，配置树是一种逻辑结构。 Configuration 对象一个 Configuration 对象表示配置树的某个配置节点，表示根节点的对象与表示其它配置节点的对象是不同的，所以配置模型采用 IConfigurationRoot 接口来表示根节点，根节点以外的其他配置节点则用 IConfigurationSection 接口表示，这两个接口都继承自 IConfiguration。下图为我们展示了由一个 ConfigurationRoot 对象和一组 ConfigurationSection 对象构成的配置树。下面的代码展示了 IConfigurationRoot 接口的定义，该接口仅定义了一个 Reload 方法实现对配置数据的重新加载。ConfigurationRoot 对象表示配置树的根，也代表整棵配置树，如果它被重新加载，意味着整棵配置树的所有配置数据均被重新加载。1234public interface IConfigurationRoot : IConfiguration&#123; void Reload();&#125;非根配置节点的 IConfigurationSection 接口具有如下三个属性: Key: 只读，用来唯一标识多个具有相同父节点的 ConfigurationSection 对象 Path 表示当前配置节点在配置树中的路径，该路径由多个 Key 值组成，并采用冒号(:)分隔纵深节点。Path 和 Key 的值体现了当前配置节在整个配置树中的位置。 Value: 表示当前 IConfigurationSection 配置节点的值。只有配置树的叶子节点对应的ConfigurationSection 对象的 Value 属性才有值，非叶子节点对应的 ConfigurationSection 对象仅表示存放子配置节点的逻辑容器，它们的 Value 为 Null。值得一提的是，这个 Value 属性并不是只读的，而是可读可写的，但是写入的值不会被持久化，因为配置树只是逻辑结构，而非物理结构。所以一旦配置树被重新加载，写入的值将会丢失。123456public interface IConfigurationSection : IConfiguration&#123; string Path &#123; get; &#125; string Key &#123; get; &#125; string Value &#123; get; set; &#125;&#125; 现在来看看 IConfiguration 接口的定义: 12345678public interface IConfiguration&#123; IEnumerable&lt;IConfigurationSection&gt; GetChildren(); IConfigurationSection GetSection(string key); IChangeToken GetReloadToken(); string this[string key] &#123; get; set; &#125;&#125; GetChildren: 返回 ConfigurationSection 的集合，表示所有从属于它的配置节点 GetSection: 根据指定的 key 返回一个具体的子配置节点。key 参数与当前配置对象的 Path 属性的值进行组合以确定目标配置节点所在的路径。 GetReloadToken: 返回当配置重新加载时进行回调的 IChangeToken 对象，有关 IChangeToken 详见后文。 以下示例通过不同的 key 值获得相同配置节点的值:12345678910111213141516171819Dictionary&lt;string, string&gt; source = new Dictionary&lt;string, string&gt;&#123; ["A:B:C"] = "ABC"&#125;;IConfiguration root = new ConfigurationBuilder() .Add(new MemoryConfigurationSource &#123; InitialData = source &#125;) .Build(); IConfigurationSection section1 = root.GetSection("A:B:C");IConfigurationSection section2 = root.GetSection("A:B").GetSection("C");IConfigurationSection section3 = root.GetSection("A").GetSection("B:C"); Debug.Assert(section1.Value == "ABC");Debug.Assert(section2.Value == "ABC");Debug.Assert(section3.Value == "ABC"); Debug.Assert(!ReferenceEquals(section1, section2));Debug.Assert(!ReferenceEquals(section1, section3)); Debug.Assert(null != root.GetSection("D"));虽然上述代码得到的 ConfigurationSection 对象均指向配置树的同一个节点，但是它们并非同一个对象。当调用 GetSection 方法时，无论配置树是否存在一个与指定路径匹配的配置节点，它总是会创建一个 ConfigurationSection 对象。 IConfiguration 的索引器执行与 GetSection 方法相同的逻辑。 ConfigurationProvider 对象虽然每种不同类型的配置源都具有一个对应的 ConfigurationSource 类型，但对原始数据的读取并不由 ConfigurationSource 实现，而是委托一个对应的 ConfigurationProvider 对象来完成。不同配置类型的 ConfigurationSource 由不同的 ConfigurationProvider 实现读取。ConfigurationProvider 将配置数据从原始结构转换为数据字典，因此定义在 IConfigurationProvider 接口中的方法大多为针对字典对象的操作:12345678public interface IConfigurationProvider&#123; void Load(); bool TryGet(string key, out string value); void Set(string key, string value); IEnumerable&lt;string&gt; GetChildKeys(IEnumerable&lt;string&gt; earlierKeys, string parentPath)&#125;配置数据通过调用 ConfigurationProvider 的 Load 方法完成加载。TryGet 方法获取由指定的Key 所标识的配置项的值。ConfigurationProvider 是只读的，ConfigurationProvider 只负责从持久化资源中读取配置数据，而不负责更新保存在持久化资源的配置数据，它的 Set 方法设置的配置数据只会保存在内存中。ConfigurationProvider 的 GetChildKeys 方法用于获取某个指定配置节点的所有子节点的 Key。 ConfigurationSource 对象ConfiurationSource 在配置模型中代表配置源，它通过注册到 ConfigurationBuilder 上为后者创建的 Configuration 提供原始的配置数据。由于原始配置数据的读取实现在相应的 ConfigurationProvider 中，所以 ConfigurationSource 的作用在于提供相应的 ConfigurationProvider。如下面的代码片段所示，该接口具有一个唯一的 Build 方法根据指定的 ConfigurationBuilder 对象提供对应的ConfigurationProvider。1234public interface IConfigurationSource&#123; IConfigurationProvider Build(IConfigurationBuilder builder);&#125; ConfigurationBuilder 对象ConfigurationBulder 在整个配置模型中处于一个核心地位，它是 Configuration 的创建者，IConfigurationBulder 接口定义了两个方法，其中 Add 方法用于注册 ConfigurationSource，最终的 Configuration 则通过 Build 方法创建，后者返回一个代表整棵配置树的ConfigurationRoot 对象。注册的 ConfigurationSource 保存在 Sources 属性表示的集合中，Properties 属性则以字典的形式存放任意的自定义数据。12345678public interface IConfigurationBuilder&#123; IEnumerable&lt;IConfigurationSource&gt; Sources &#123; get; &#125; Dictionary&lt;string, object&gt; Properties &#123; get; &#125; IConfigurationBuilder Add(IConfigurationSource source); IConfigurationRoot Build();&#125;配置系统提供了 ConfigurationBulder 类型作为 IConfigurationBulder 接口的默认实现者。 无论是 ConfigurationRoot 还是 ConfigurationSection，它们自身都没有维护任何数据。这句话有点自相矛盾，因为配置树仅仅是 API 在逻辑上所体现的数据结构，并不代表具体的配置数据也是按照这样的结构进行存储的。 对象关系图配置系统的四个核心对象之间的关系简单而清晰，可以通过一句话来概括: ConfigurationBuilder 利用注册的 ConfigurationSource 得到相应的 ConfigurationProvider，再调用 ConfigurationProvider 的 Load 方法读取原始配置数据并创建出相应的 Configuration 对象。下图所示的 UML 展示了配置模型涉及的主要接口/类型以及它们之间的关系: 同步 Configuration 的更改参考配置的同步机制是如何实现的？]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET Core 框架基础 - 文件系统]]></title>
    <url>%2FWeb%2FASP-NET-Core%2Faspnetcore-fundamentals-filesystem%2F</url>
    <content type="text"><![CDATA[参考资料: ASP.NET Core 的文件系统 File Providers in ASP.NET Core 本文大纲: 抽象的「文件系统」 FileProvider 抽象 文件系统的实现者 PhysicalFileProvider EmbeddedFileProvider CompositeFileProvider 监控变化 文件系统详解 FileInfo &amp; GetFileInfo 方法 DirectoryContents &amp; GetDirectoryContents 方法 ChangeToken 及 Watch 方法 路径前缀 「/」 对象关系图 抽象的「文件系统」ASP.NET Core 利用一个抽象化的 FileProvider 以统一的方式提供所需的文件。FileProvider 是所有实现了 IFileProvider 接口的类型的统称，FileProvider 是个抽象的概念，所以由它构建的也是一个抽象的文件系统。 这个文件系统采用目录的方式来组织和规划文件，这里所谓的目录和文件都是抽象的概念，并非对一个具体物理目录和文件的映射。文件系统的目录仅仅是文件的逻辑容器，而文件可能对应一个物理文件，也可能保存在数据库中，或者来源于网络，甚至有可能根本就不能存在，其内容需要在读取时动态生成。 一个 FileProvider 可以视为针对一个根目录的映射。目录除了可以存放文件之外，还可以包含多个子目录，所以目录/文件在整体上呈现出树形层细化结构。 FileProvider 抽象IFileProvider 接口提供了获取文件信息(IFileInfo)和目录信息的方法，并支持追踪变化并发送通知(IChangeToken)的功能。 IFileInfo 接口代表单独的文件信息或目录，其含有以下属性: Exists: 标识是否存在 IsDirectory: 标识是否为目录 Name: 描述「文件」的名称 Length: 以字节计算 LastModified: 上次修改的日期 CreateReadStream: 调用该方法来读取内容 文件系统的实现者IFileProvider 内置了三个实现类型: Physical: 访问真实的物理文件结构 Embedded: 访问嵌套于程序集内的文件 Composite: 组合来自于其他提供器的文件和目录访问 PhysicalFileProviderPhysicalFileProvider 实现了对访问物理文件系统的支持，其内部包裹了 System.IO.File 类型，该类型将所有可访问路径限制在一个根目录下，在初始化该类型时必须为其提供一个代表目录的路径参数。以下代码演示了如何创建一个 PhysicalFileProvider:123IFileProvider provider = new PhysicalFileProvider(applicationRoot);IDirectoryContents contents = provider.GetDirectoryContents(""); // the applicationRoot contentsIFileInfo fileInfo = provider.GetFileInfo("wwwroot/js/site.js"); // a file under applicationRoot EmbeddedFileProvider在 .NET Core 中，通过在 .csproj 文件中使用 &lt;EmbeddedResource&gt; 元素将文件嵌套至程序集中:123456&lt;ItemGroup&gt; &lt;EmbeddedResource Include="Resource.txt;**\*.js" Exclude="bin\**;obj\**;**\*.xproj;packages\**;@(EmbeddedResource)" /&gt; &lt;Content Update="wwwroot\**\*;Views\**\*;Areas\**\Views;appsettings.json;web.config"&gt; &lt;CopyToPublishDirectory&gt;PreserveNewest&lt;/CopyToPublishDirectory&gt; &lt;/Content&gt;&lt;/ItemGroup&gt; 向 EmbeddedFileProvider 类型的构造函数提供 Assembly 对象来创建它。1var embeddedProvider = new EmbeddedFileProvider(Assembly.GetEntryAssembly());嵌套资源没有「目录」的概念，不同命名空间的资源同样可以通过 . 语法来访问。EmbeddedFileProvider 类型的构造器接收一个可选的 baseNamespace 参数，指定该参数可以将调用 GetDirectoryContents 方法访问的范围限制在该命名空间下。 CompositeFileProviderCompositeFileProvider 组合多个 IFileProvider 对象并暴露一个针对不同 provider 的统一访问接口，创建 CompositeFileProvider 实例需要向其传递一个或多个 IFileProvider 对象。123var physicalProvider = _hostingEnvironment.ContentRootFileProvider;var embeddedProvider = new EmbeddedFileProvider(Assembly.GetEntryAssembly());var compositeProvider = new CompositeFileProvider(physicalProvider, embeddedProvider); 监控变化IFileProvider 包含一个 Watch 方法对监控文件和目录变化提供了支持，该方法接收一个路径参数，该参数可通过 globbing patterns 来指定多个文件。Watch 方法返回一个 IChangeToken 对象，该对象包含一个 HasChanged 属性和一个 RegisterChangeCallback 方法。RegisterChangeCallback 在指定路径的文件发送变化后被调用。 值得注意的是，每一个 IChangeToken 对象仅监控一次变化。单个 ChangeToken 对象的使命在于当绑定的数据源第一次发生变换时对外发送相应的信号，而不具有持续发送数据变换的能力。它具有一个 HasChanged 属性表示数据是否已经发生变化，而并没有提供一个让这个属性「复位」的方法。 如果需要对文件进行持续监控，需要在注册的回调中重新调用 FileProvider 的 Watch 方法，并利用新生成的 ChangeToken 再次注册回调。除此之外，考虑到 ChangeToken 的 RegisterChangeCallback 方法以一个 IDisposable 对象的形式返回回调注册对象，我们应该在对回调实施二次注册时调用第一次返回的回调注册对象的 Dispose 方法将其释放掉。 或者，可以使用定义在 ChangeToken 类型中如下两个方法 OnChange 方法来注册数据发生改变时自动执行的回调。这两个方法具有两个参数: Func&lt;IChangeToken&gt;: 用于创建 ChangeToken对象的委托对象 Action&lt;object&gt;/Action&lt;TState&gt;: 代表回调操作的委托对象 12345678910111213141516171819202122public static class ChangeToken&#123; public static IDisposable OnChange(Func&lt;IChangeToken&gt; changeTokenProducer, Action changeTokenConsumer) &#123; Action&lt;object&gt; callback = null; callback = delegate (object s) &#123; changeTokenConsumer(); changeTokenProducer().RegisterChangeCallback(callback, null); &#125;; return changeTokenProducer().RegisterChangeCallback(callback, null); &#125; public static IDisposable OnChange&lt;TState&gt;(Func&lt;IChangeToken&gt; changeTokenProducer, Action&lt;TState&gt; changeTokenConsumer, TState state) &#123; Action&lt;object&gt; callback = null; callback = delegate (object s) &#123; changeTokenConsumer((TState) s); changeTokenProducer().RegisterChangeCallback(callback, s); &#125;; return changeTokenProducer().RegisterChangeCallback(callback, state); &#125;&#125; 也可以使用 TaskCompletionSource 对象:123456789101112private static async Task MainAsync()&#123; IChangeToken token = _fileProvider.Watch("quotes.txt"); var tcs = new TaskCompletionSource&lt;object&gt;(); token.RegisterChangeCallback(state =&gt; ((TaskCompletionSource&lt;object&gt;)state).TrySetResult(null), tcs); await tcs.Task.ConfigureAwait(false); Console.WriteLine("quotes.txt changed");&#125; 基于 Docker 容器和网络共享的文件系统不会正确的发送改变通知，可通过设置 DOTNET_USE_POLLINGFILEWATCHER 环境变量为 1 或者 true 每 4 秒轮询文件改变。 文件系统详解FileProvider 的定义:123456public interface IFileProvider&#123; IFileInfo GetFileInfo(string subpath); IDirectoryContents GetDirectoryContents(string subpath); IChangeToken Watch(string filter);&#125; FileInfo &amp; GetFileInfo 方法可以通过向 GetFileInfo 传递一个子路径(通常为相对路径)来访问指定文件的信息，当调用这个方法的时候，无论指定的路径是否存在，该方法总是返回一个具体的 FileInfo 对象。即使指定的路径对应一个具体的目录，这个 FileInfo 对象的 IsDirectory 也总是返回 False（它的Exists属性也返回False）。 DirectoryContents &amp; GetDirectoryContents 方法调用 FileProvider 的 GetDirectoryContents 方法，目录内容通过该方法返回 DirectoryContents 对象来表示。一个 DirectoryContents 对象实际上表示一个 FileInfo 的集合，组成这个集合的所有 FileInfo 是对所有文件和子目录的描述。和 GetFileInfo 方法一样，不论指定的目录是否存在，GetDirectoryContents 方法总是会返回一个具体的 DirectoryContents 对象，它的 Exists 属性会帮助我们确定指定目录是否存在。1234public interface IDirectoryContents : IEnumerable&lt;IFileInfo&gt;&#123; bool Exists &#123; get; &#125;&#125; ChangeToken 及 Watch 方法目前仅 PhysicalFileProvider 类型提供了 Watch 方法的实现，它会委托一个 FileSystemWatcher 对象来完成最终的文件监控任务。Watch 方法的返回类型为 IChangeToken 接口，ChangeToken 可视为一个与某个数据进行关联，并在数据发生变化对外发送通知的令牌。如果关联的数据发生改变，它的 HasChanged 属性将变成 True。调用它的 RegisterChangeCallback 方法注册一个在数据发生改变时可以自动执行的回调方法。该方法以一个 IDisposable 对象的形式返回注册对象，原则上讲我们应该在适当的时机调用其 Dispose 方法注销回调的注册，以免内存泄漏。IChangeToken 接口的另一个属性 ActiveChangeCallbacks，它表示当数据发生变化时是否需要主动执行注册的回调操作。 路径前缀 「/」无论是调用 GetFileInfo，GetDirectoryContents 方法指定的目标文件和目录的路径，还是在调用 Watch 方法时指定筛选表达式，都是针对当前 FileProvider 根目录的相对路径。指定的这个路径可以采用 / 字符作为前缀，但是这个前缀是不必要的。 对象关系图文件系统还涉及到其他一些对象，如 DirectoryContents、FileInfo 和 ChangeToken。这些对象都具有对应的接口定义，下图所示的 UML 展示了涉及的这些接口以及它们之间的关系。]]></content>
      <categories>
        <category>Web</category>
        <category>ASP.NET Core</category>
      </categories>
      <tags>
        <tag>asp.net core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建 Ubuntu 16.04 LTS]]></title>
    <url>%2Flinux%2Flinux-setup-a-new-ubuntu-server%2F</url>
    <content type="text"><![CDATA[近年来云计算服务器越来越流行，在适当的时机准备一台拥有公网 ip 的云服务器以备不时之需。最近租用了一台预装 ubuntu 操作系统(这里选择 ubuntu 是因为笔者对其预装的工具集比较熟悉)的服务器之后，还要为这台服务器做一些额外的配置以使其能够在互联网环境中正常运行。很多云服务提供商都提供了适配不同需求的预装环境，但为了对服务器的搭建过程有一个直观的感觉和更多的控制权，决定亲手过一遍这个过程。 本篇文章涉及的所有指令细节可参考Linux 基础 - 用户管理 通常，配置一台裸机至少要完成以下几个步骤 新建群组 新建用户，并为其分配群组 配置 ssh 配置服务器安全策略 准备工作在开始配置之前，我们需要知道该服务器的以下信息： 服务器的公网 ip 地址 确认 22 端口打开 root 用户的初始密码 在 windows 系统下启动 PuTTy，使用 root 用户远程登录到该服务器 主机命名 hostname(可选的)如果不想修改云服务提供商默认分配的主机名，可跳过此步。 执行命令 hostname 显示当前主机名12$ hostnameVM-0-4 执行以下命令进行修改，修改完成后，再次执行命令以查看效果：123$sudo hostname &lt;your-new-hostname&gt;$ hostnameyour-new-hostname 新建群组 - groupadd执行如下命令新建一个带有 &lt;gid&gt; 的群组1$sudo groupadd &lt;your-new-group-name&gt; -g &lt;gid&gt; 新建用户 - useradd1$sudo useradd -u 2500 -m -c "FrostHe" -s -g sudo -G pango pango 该命令创建一个名为 pango 的用户，uid 为 2500，要求为该用户创建 Home 目录，使用预设值设置 shell 环境，加入初始群组 sudo，同时加入次要群组 pango。由于先前已经创建了群组，在创建该用户时就不会再创建与之同名的新的群组，而是将该用户加入到该群组下。 现在，新建用户 pango 还没有密码，设置密码之前是无法登录 shell 的，执行 passwd 来为新用户指定密码：123$sudo passwd pangoEnter new Unix Password:Retype new Unix Password: passwd 指令在不接参数时表示修改当前登录用户的密码 现在，退出 PuTTy 客户端，以新建用户名和密码登录，如果登录成功，则表明新用户创建无误。 配置服务器安全策略将云服务器的 22 端口暴露于互联网并允许 root 用户及一般用户以密码进行登录是不推荐的，特别是 root 帐号，一旦被攻击者破解那么服务器上的资源可任由其修改。为了使服务器免于这些危险，我们需要让这台服务器： 禁用 root 帐号密码登录，仅启用公钥认证 开启防火墙并限定端口 设置 ip 登录策略及 禁用密码登录并启用 ssh 公钥登录以新建用户登录系统，修改 sshd_config 文件：1234$sudo nano /etc/ssh/sshd_configPermitRootLogin noPublicAuthentication yesPasswordAuthentication no 该配置对 ssh 客户端远程登录作出限制：启用公钥认证并禁用密码认证，同时禁止 root 远程登录。 若要修改生效， sshd 进程需要重新读取该配置，但这会让已经通过密码登录的会话中断，并且在 public key 部署前没有任何机会重新进行远程连接，所以这一步放到最后来做。 接下来在新用户 Home 目录下的 .ssh/authorized_keys 文件中复制 openssh 格式的公钥值。 在 /etc/ssh/sshd_config 中有一行 AuthorizedKeyFiles，该行的默认值为 .ssh/authorized_keys，该项配置是 sshd 进程提取 public key 的依据，如果对该值进行了修改，那么这里新建的文件也必须要与之对应。 现在执行 sudo service sshd reload 以使配置生效。此时重新打开一个 PuTTy 客户端，使用新用户密码登录，将收到错误对话框：在 Putty 中设置对应的私钥路径，重试即可登录成功： 至此，一个基本的云服务器配置就完成了，有的云服务提供商推出了「安全组」功能，即从云端配置端口进出通道。更多安全配置查阅 为 Linux 系统配置 CSF。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>diy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为 Linux 系统配置 CSF]]></title>
    <url>%2Flinux%2Flinux-configure-csf%2F</url>
    <content type="text"><![CDATA[本文大纲: 功能 认证失败检测守护进程 进程追踪 目录监控 消息服务 端口涌流保护 Port Knocking 连接限制保护 Port/IP 地址重定向 UI 集成 IP 屏蔽列表 安装 ConfigServer Firewall 下载 解压缩 安装 基本配置 配置端口 任何主机 Apache FTP 服务器 邮件服务器 MySQL 服务器 进阶配置 应用更改 屏蔽与允许 ip 地址 屏蔽 ip 地址 允许 ip 地址 忽略 ip 地址 Config Server Firewall(CSF) 基于一般的防火墙功能(数据包过滤)进行了扩展，支持大多数 Linux 发行版本，其支持列表可在其官网找到。以下以 ubuntu 系统为例介绍 CSF 的使用。 对 CSF 的几乎所有指令操作都需要管理员权限，本篇文章仅涵盖 ipv4 的防火墙设置 - iptables。ipv6 对应的是 ip6tables。 功能认证失败检测守护进程CSF 每隔一段时间就会检查系统登录日志，对那些登录失败的请求尝试进行监控，在达到预设的失败次数后，CSF 将对发起这些请求的 ip 地址做预设的处理，失败次数和处理动作都是可自定义的。 以下程序对该功能兼容： Courier imap, Dovecot, uw-imap, Kerio openSSH cPanel, WHM, Webmail (cPanel servers only) Pure-ftpd, vsftpd, Proftpd Password protected web pages (htpasswd) Mod_security failures (v1 and v2) Suhosin failures Exim SMTP AUTH同时，可以自定义匹配 Regex 的登录文件来，对登录失败尝试的用户进行屏蔽。 进程追踪CSF 可检测异常行为的进程或异常开启端口的行为，在捕获到这些行为时可配置发送邮件给服务器管理员。 目录监控CSF 可监控 /temp 和其他相关的目录扫描恶意脚本，并在发现风险时发送邮件给服务器管理员。 消息服务启用该功能可让 CSF 在屏蔽客户端时传送一些有用的信息，但这样做可能会为黑客提供一些线索信息，继而增加服务器的风险。 端口涌流保护该设置提供端口涌流保护，比如 DDoS 攻击，可以为每个端口指定特定时间内允许最大的连接数。启用该功能可以极大降低黑客的野蛮攻击导致服务器宕机。找到一个适合的参数需要一些尝试，太过严格的流量限制会降低正常用户的体验。 Port Knocking参考 Port Knocking。 连接限制保护该功能可限制由单一 ip 地址对特定端口建立的连接数，这也是防止 DDoS 攻击的手段之一。 Port/IP 地址重定向可配置 CSF 将对特定 IP/Port 的连接重定向到另一个 IP/Port。注意，重定向之后，请求发起的客户端将变为服务器的 ip 地址，该功能与 NAT 不同。 UI 集成除了命令行工具之外，CSF 对 cPanel 和 Webmin 提供了 UI 集成。 IP 屏蔽列表CSF 可使用该功能从预设的源下载屏蔽的 ip 地址列表。 安装 ConfigServer Firewall下载从官网下载安装包到当前工作目录：1wget http://download.configserver.com/csf.tgz 解压缩下载下来的是一个压缩包，安装之前需要解压缩1tar -xzf csf.tgz 安装在进行安装之前，首先禁用其他防火墙脚本，例如 UFW，执行以下指令禁用 UFW：1$sudo ufw disable 导航到刚刚解压出来的文件夹 csf，执行安装脚本：1234cd csf$sudo sh install.sh...Installation Completed 安装完成后，查看指定的 iptables 是否可用：123456789101112131415$sudo perl /usr/local/csf/bin/csftest.plTesting ip_tables/iptable_filter...OKTesting ipt_LOG...OKTesting ipt_multiport/xt_multiport...OKTesting ipt_REJECT...OKTesting ipt_state/xt_state...OKTesting ipt_limit/xt_limit...OKTesting ipt_recent...OKTesting xt_connlimit...OKTesting ipt_owner/xt_owner...OKTesting iptable_nat/ipt_REDIRECT...OKTesting iptable_nat/ipt_DNAT...OKRESULT: csf should function on this server 出现以上报告内容则表明 CSF 已可在服务器上正常运行。 此时，通过远程登录连接到服务器的 ip 地址会被自动加入到白名单中，同时 SSH 占用的端口(即便是自定义的端口)也会放行，CSF 默认使用测试模式，该模式下所有的 iptables 规则将在 5 分钟后自动被移除。一旦确认所有配置已经就绪，应该由测试模式切换为工作模式。 基本配置通过编辑 /etc/csf/csf.conf 文件来配置 CSF，保存更改后，执行 csf -r 来使配置重新生效。1$sudo nano /etc/csf/csf.conf 配置端口配置文件中默认开启的端口如下：1234567TCP_IN = "20,21,22,25,53,80,110,143,443,465,587,993,995"TCP_OUT = "20,21,22,25,53,80,110,113,443"UDP_IN = "20,21,53"UDP_OUT = "20,21,53,113,123" 这些端口代表的默认服务如下： Port 20: FTP 数据传输 Port 21: FTP 控制 Port 22: 安全 shell (SSH) Port 25: 简单邮件传输协议 (SMTP) Port 53: 域名系统 (DNS) Port 80: 超文本传输协议 (HTTP) Port 110: 邮局协议 v3 (POP3) Port 113: 身份协议 Port 123: 网络时间协议 (NTP) Port 143: 因特网信息访问协议 (IMAP) Port 443: 超文本安全传输协议 (HTTPS) Port 465: URL Rendesvous Directory for SSM (Cisco) Port 587: 简单邮件传输协议 (SMTP) Port 993: 因特网安全信息访问协议 (IMAPS) Port 995: 安全邮局协议 v3 (POP3S) 以下是针对常见场景开启的服务端口： 任何主机1234TCP_IN: 22,53TCP_OUT: 22,53,80,113,443UPD_IN: 53UPD_OUT: 53,113,123 Apache1TCP_IN: 80,443 FTP 服务器1234TCP_IN: 20,21TCP_OUT: 20,21UPD_IN: 20,21UPD_OUT:20,21 邮件服务器12TCP_IN: 25,110,143,587,993,995TCP_OUT: 25,110 MySQL 服务器12TCP_IN: 3306TCP_OUT: 3306 进阶配置CSF 提供了大量的可配置项，最常用的列表如下： ICMP_IN: 设置为 1 时将允许外部主机 ping，设为 0 则禁止任何请求 ICMP_IN_LIMIT: 特定时间内允许同一 ip 地址的 ping 的请求数，通常不用修改，默认值为 1/s DENY_IP_LIMIT: 设置屏蔽 ip 地址的最大数量，保留太多数量会降低服务器性能 DENY_TEMP_IP_LIMIT: 与 DENY_IP_LIMIT 类似，但仅作用于临时屏蔽的 ip 地址 PACKET_FILTER: 过滤无效的，不需要的和非法的数据包 SYNFLOOD, SUNFLOOD_RATE 和 SYNFLOOD_BURST: 这三项提供了针对 SYN flood 攻击的保护，但会降低每个连接的初始化速度，仅当明确服务器正遭受攻击时启用该项 CONNLIMIT: 限制指定端口的并发连接数 如: 22;5;443;20 - 该值允许在 22 端口上最大 5 个并发连接数，在 443 端口上最大 20 个并发连接数 PORTFLOOD: 限制指定端口上单位时间内的最大连接数 如: 22;tcp;5;250 - 该值限制如果 22 端口上已有 5 个 tcp 连接，那么第 6 个来临的 tcp 连接将等待 250 秒，之后屏蔽解除，5 个新的 tcp 连接放行 其他设置在大多数情况都无需改动，如果确实需要自定义这些配置，阅读 /etc/csf/csf.conf 各项配置上的注释来了解其用途。 应用更改在应用更改前将第一项配置 TESTING = “1” 改为 TESTING = “0” 以使 csf 切换为工作模式。再以管理员权限执行 sudo csf -r 使更改生效。 屏蔽与允许 ip 地址防火墙最基础的功能是屏蔽，允许及忽略特定的 ip 地址，csf 可通过编辑 csf.deny, csf.allow 和 csf.ignore 文件来实现。 屏蔽 ip 地址使用编辑器打开 csf.deny1$sudo nano /etc/csf/csf.deny 每一行代表一条屏蔽项，可以是单一的 ip 地址，也可以是一个网段，例如：121.2.3.42.3.0.0/16 允许 ip 地址如果希望指定的 ip 地址或网段避开屏蔽和过滤扫描，可将它们加入到白名单列表，一旦将它们加入白名单，即便它们在 csf.deny 中已经存在，也会让它们绕过防火墙。 编辑 csf.allow 文件来加入白名单：1$sudo nano /etc/csf/csf.allow 忽略 ip 地址忽略名单与白名单的区别在于，忽略名单仅仅不进行过滤检查，但依然可能被加入黑名单中。1$sudo nano /etc/csf/csf.ignore 最后，执行 sudo csf -r 重载 csf 以使配置生效。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 指令与档案的搜索(which/whereis/locate/updatedb/find)]]></title>
    <url>%2Flinux%2Flinux-document-search%2F</url>
    <content type="text"><![CDATA[which(寻找「可执行文件」)1$ which [-a] command -a: 将所有定义在 $PATH 中与该指令相关的路径都列出来。 例如:1234567$ which ifconfig/sbin/ifconfig$ which whichalias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde' /bin/alias /usr/bin/which 这里涉及到了命令别名 which 预设是搜索 PATH 内定义的目录，有些 bash 内建的指令并没有在 PATH 中定义，所以有可能找不到，例如 history 指令。 whereiswhereis 仅针对特定目录进行查找，所以速度会比 find 指令快，想知道哪些目录，执行 whereis -l 即可。1$ whereis [-bmsu] 档案名或目录名 -l: 列出 whereis 查询的主要目录 -b: 只找 binary 类型的文件 -m: 只找在说明档 manual 路径下的档案 -s: 只招 source 来源档案 -u: 搜寻不在上述三个项目中的其他特殊档案 举例:12$ whereis ifconfig/sbin/ifconfig /usr/share/man/man8/ifconfig.8.gz locate / updatedblocate 在「已经建立的资料库(/var/lib/mlocate/)」中搜索，因此速度很快，但不同的 Linux 发行版建立资料库的预设周期都不同(CentOS 7.x 是每天更新一次)，如果在资料库新建之前使用该命令，有可能找不到目标资料，这时可执行 updatedb 手动更新资料库，updatedb 指令首先读取 /etc/updatedb.conf 配置文件，再去硬盘里搜索档案名，最后更新整个资料库档案，由于要进行硬盘操作，整个过程可能会比较慢。1$ locate [-ir] 关键字 -i: 忽略大小写 -c: 不输出档案名称，仅计算找到的档案数量 -l: 输出行数，如输出 5 行则是 -l 5 -S: 输出 locate 所使用的资料库档案的相关咨询，包括该资料库记录的档案/目录数量等 -r: 后接正则表达式 find12345678910111213141516171819202122232425262728293031323334353637$ find [PATH] [option] [action]选项与参数：1. 与档案权限及名称有关的参数： -name filename：搜寻档案名称为 filename 的档案； -size [+-]SIZE：搜寻比 SIZE 还要大 (+) 或小 (-) 的档案。这个 SIZE 的规格有： c: 代表 byte， k: 代表 1024 bytes。所以，要找比 50KB 还要大的档案，就是『 -size +50k 』 -type TYPE ：搜寻档案的类型为 TYPE 的，类型主要有：一般正规档案(f), 装置档案(b, c), 目录(d), 连结档(l), socket(s), 及 FIFO(p) 等属性。 -perm mode ：搜寻档案权限『刚好等于』 mode 的档案，这个 mode 为类似 chmod 的属性值，举例来说，`-rwsr-xr-x` 的属性为 4755 ！ -perm -mode ：搜寻档案权限『包含 mode 的权限』的档案，举例来说， 我们要搜寻 `-rwxr--r--` ，亦即 0744 的档案，使用 -perm -0744， 当一个档案的权限为 `-rwsr-xr-x` ，亦即 4755 时，也会被列出来， 因为 `-rwsr-xr-x` 的属性已经包括 -rwxr--r-- 的属性了。 -perm /mode ：搜寻档案权限『包含任一 mode 的权限』的档案，举例来说，我们搜寻 `-rwxr-xr-x` ，亦即 -perm /755 时，但一个档案属性为 `-rw-------` 也会被列出来，因为他有 -rw.... 的属性存在！找出档名为passwd这个档案 [root@study ~]# find / -name passwd找出档名包含了passwd这个关键字的档案 [root@study ~]# find / -name "*passwd*" #利用这个-name可以搜寻档名啊！预设是完整档名，如果想要找关键字，# 可以使用类似* 的任意字元来处理找出/run目录下，档案类型为Socket的档名有哪些？[root@study ~]# find /run -type s #这个-type的属性也很有帮助喔！尤其是要找出那些怪异的档案，# 例如socket 与FIFO 档案，可以用find /run -type p 或-type s 来找！搜寻档案当中含有SGID或SUID或SBIT的属性 [root@study ~]# find / -perm /7000 #所谓的7000就是---s--s--t ，那么只要含有s或t的就列出，所以当然要使用/7000，# 使用-7000 表示要同时含有---s--s--t 的所有三个权限。而只需要任意一个，就是/7000 ～了乎？ 更多参数参考 http://linux.vbird.org/linux_basic/0220filemanager.php。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 档案内容查阅(cat/tac/nl/more/less/head/tail/od/touch)]]></title>
    <url>%2Flinux%2Flinux-document-inspect%2F</url>
    <content type="text"><![CDATA[cat(concatenate)1cat [-AbEnTv] 档案名 参数与选项: -A: 忽略空白字符，列出不可见的特殊字符 -b: 列出行号，仅针对非空白行 -E: 将结尾的换行字符 $ 显示出来 -n: 列出行号，包括空白行 -T: 将 Tab 以 ^| 显示 -v: 列出不可见的特殊字符 在使用 cat -A 指令后，Tab 以 ^| 显示，而换行符以 $ 显示，这样有助于查看空白部分到底是什么字符。 tac(反向串联)tac 命令恰好是 cat 命令反写，cat 从第一行输出至最后一行，而 tac 从最后一行输出到第一行。 nl(添加行号)1nl [-bnw] 档案名 参数与选项: -b: 指定行号的方式，主要有两种: -ba: 类似 cat -n，空行也列出行号 -bt: 忽略空白行的行号，预设值 -n: 列出行号表示方法，主要三种: -n ln: 行号位于左侧 -n rn: 行号位于右侧 -n rz: 行号位于右侧，且添零补齐 -w: 行号占位字符数举例:1234nl -ba -n rz -w 3 /etc/issue001 \S002 Kernel \r on an \m003 翻页(more 和 less)ln, cat 和 tac 都是将档案的全部内容一次性输出到屏幕上，more 与 less 指令提供了翻页功能。 more12345678more /etc/man_db.conf### This file is used by the man-db package to configure the man and cat paths.# It is also used to provide a manpath for those without one by examining# their PATH environment variable. For details see the manpath(5) man page.#--More--(28%) 使用 more 指令后，可以注意到最下端一行多出了一个百分比，此时如果按下: Space: 下一页 b: 上一页 Enter: 下一行 q: 退出 more 指令 /{字符串}: 向下查找 「字符串」 匹配的文本，按下 Enter 开始查找，按下 n 查找下一个， :f: 显示档案名及当前显示的行数lessless 在 more 的基础上做了改进，less 使用 PGUp 和 PGDown 来翻页。并且多出了一些选项: Space: 下一页 pagedown: 下一页 pageup: 上一页 /{字符串}: 向下查找 「字符串」 匹配的文本 ?{字符串}: 向上查找 「字符串」 匹配的文本 n: 重复前一个查找 N: 反向重复前一个查找 g: 跳转至第一行 G: 跳转至最后一行 q: 退出 less 指令 man 指令是执行 less 指令产生的结果，所以两者在用法上是相通的 文本截取(head 和 tail)head1head [-n number] 档案名 选项与参数: -n: 后接数字，代表显示前 n 行，如果不指定 -n 参数，默认情况下该指令显示前 10 行，如果指定为 -100，则表示显示最后 100 行之前的所有行举例: 123head /etc/man_db.conf # 输出前 10 行head /etc/man_db.conf -n 20 # 输出前 20 行head /etc/man_db.conf -n -100 # 输出最后 100 行前的所有行 tail1tail [-n number] 档案名 选项与参数: -n: 后接数字，表示输出最后 n 行，默认值为 10， -f: 表示实时侦测文档，Ctrl + C 来取消举例:1234tail /etc/man_db.conf # 输出最后 10 行tail -n 20 /etc/man_db.conf # 输出最后 20 行tail -n +100 /etc/man_db.conf # 输出 100 行后的所有行tail -f /var/log/messages # 实时监测该文档的内容，可与 -n 并用 如果希望输出某个文档的第 10-20 行，那么可以执行 head -n 20 /etc/man_db.conf | tail -n 10，意为先去前 20 行，再将其结果交给 tail 指令输出最后 10 行。 1cat -n /etc/man_db.conf | head -n 20 | tail -n 10 # 取 /etc/man_db.conf 的 10-20 行，并显示行号 非纯文本档案: od上述的所有指令都是针对纯文本的档案读取，对非纯文本档案的读取使用 od 指令:1od [-t TYPE] 档案名 选项与参数: -t: 接档案类型，类型有: a: 使用预设的字符输出 c: 使用 ASCII 输出 d[size]: 使用十进制(decimal)输出资料，每个整数占用 [size] 字节 f[size]: 使用浮点数(floating)输出资料，每个数占用 [size] 字节 o[size]: 利用八进(octal)制输出资料，每个整数占用 [size] 字节 x[size]: 利用十六进制(hexadecimal)来输出资料，每个整数占用 [size] 字节 该命令可用于快速定位字符的 ASCII 编码，例如: echo password | od -t oCc 修改档案时间或新建档案: touch对于某个档案，其主要有 3 个时间变动的入口: modification time(mtime): 档案的「内容」更改时，会更新该时间 status time(ctime): 档案的「状态」改变时，会更新该时间，例如权限和属性被更改 access time(atime): 「档案的内容被读取」时，会更新该时间，例如用 cat 指令读取某个档案 默认情况下，当使用 ls 指令时，得到的时间是 mtime，即该档案内容上次被修改的时间，如果发现时间不对，可 touch 指令修改时间:1touch [-acdmt] 档案名 选项和参数: -a: 仅修改 aceess time -c: 仅修改档案时间，若档案不存在则不建立新档案 -d: 修改 atime 和 mtime，后接目标时间，可用 –date=”日期或时间”代替 -m: 仅修改 mtime -t: 修改 atime 和 mtime，后接目标时间，格式为 [YYYYMMDDhhmm]举例:123touch testtouch # 建立新的空档案，三个时间都会更新会当前时间date; ll bashrc; ll - -time=atime bashrc; ll --time=ctime bashrctouch -d "2 days ago" bashrc # 修改 ; 用于分割连续下达的指令，这些指令会依次执行，ctime 是无法通过指令修改的，即便是完全复制一条档案，也无法复制 ctime，该属性记录了档案的状态变化时间。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 档案与目录管理(ls/cp/rm/mv/basename/dirname)]]></title>
    <url>%2Flinux%2Flinux-document-management%2F</url>
    <content type="text"><![CDATA[档案管理主要涉及: 显示档案详情 拷贝 删除档案 移动档案 ls(检视档案)123ls [-aAdfFhilnrRSt] 档名或目录名ls [--color=&#123;never,auto,always&#125;] 档名或目录名ls [--full-time] 档名或目录名 ls 常用的选项有: -a: 全部档案，包括隐藏档案 -A: 全部的档案，包括隐藏档案，但不包括 . 与 .. 两个目录 -d: 仅列出目录 -f: 直接列出结果，而不进行排序(ls 预设会以名称排序) -F: 为档案名添加特殊符号以标识其类别，例如 * 代表可执行档案，/ 代表目录，=: 代表 socket 档案，|: 代表 FIFO 档案。 -h: 将档案容量以人类友好的方式列出(GB, KB) -i: 列出 inode 号码 -l: 列出详情 -n: 使用 UID 与 GID 而非用户名称和群组名称 -r: 将排序结果反向输出 -R: 递归显示所有目录 -S: 以档案容量大小排序 -t: 以时间排序 ls 指令包含了很多功能，Linux 档案系统记录了与档案有关的权限和属性，这些数据都放在 i-node 里面，有关 i-node 的详情，见后文。 由于 ls -l 非常常用，很多 Linux 的发布版本使用 ll 指令使其成为 ls -l 的缩写，而这是由 Bash shell 的 alias 功能实现的，有关这部分内容，见后文。 cp(复制档案或目录)12cp [-adfilprsu] source destinationcp [options] source1 source2 source3 .... directory cp 常用选项: -i: 若目标已经存在，则要求询问是否覆盖 -d: 若源为 link 档，则复制 link 档的属性而非档案本身的属性 -f: force 的简写，若目标档已经存在且无法开启，则移除后再试一次 -p: 复制档案及其属性，备份常用 -r: 启用递归复制 -s: 复制为符号链接(symbolic link) -l: 复制为硬式链接(hard link) --preserve=all: 除了 -p 的相关属性外，还加入 SELinux 的属性，links，xattr 也复制。 -a: 相当于 -dr --preserve=all 如果不加任何选项，档案被复制后其属性会发生改变，如果想要完全复制档案，则需要加上 -a 选项。 在复制其他用户的资料时(必须要有 Read 权限)，总是希望得到的档案权限归自身用户所有，所以 cp 指令预设复制后的档案归复制者所有，这意味着在不加任何选项的情况下，得到的档案权限与复制者用户一致。在使用 cp 指令进行复制时，考虑以下几点: 是否需要完整保留原始档案的咨询？ 原始档是否为符号链接档 原始档是否为特殊的档案，例如 FIFO，socket 等？ 原始档是否为目录？ rm(移除档案或目录)1rm [-fir]档案或目录 rm 指令关键选项: -f: force 的简写，忽略不存在的档案，不会出现警告 -i: 互动模式，在删除询问使用者 -r: 递归删除 使用 * 通配符可以删除任意匹配的档案或目录 Linux 系统下，为了防止档案被 root 误删，很多发行版预设加入了 -i 这个选项。但是使用 rm -r 这个指令系统不会再次询问，使用前要特别注意。如果确定目录不要了，那么使用 rm -r 来递归删除是不错的方式。 mv(移动档案与目录，或更改名称)12mv [-fiu] source destination mv [options] source1 source2 source3 .... directory mv 指令关键选项: -f: force 的缩写，如果目标档已经存在，则不询问而直接覆盖 -i: 互动模式，若目标档案已经存在，则会询问是否覆盖 -u: 若目标档案已经存在，且原始档较新才会执行移动 -u 选项可以用来测试新旧档案，看看是否需要搬移；rm 指令可以用来重命名文件，但 Linux 有另外一个 rename 指令可以进行批量改名。 basename 和 dirname12basename /etc/sysconfig/networknetwork basename 用于获取档案本身的名称12dirname /etc/sysconfig/network/etc/sysconfig dirname 用于获取包含档案的目录的完整路径]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 目录操作与 $PATH]]></title>
    <url>%2Flinux%2Flinux-directory-operations%2F</url>
    <content type="text"><![CDATA[本文大纲: 绝对路径与相对路径 常用目录操作 cd(change directory, 变换目录) pwd(print working directory, 显示当前所在目录) mkdir(make directory, 新建目录) rmdir(remove directory, 删除目录) 执行程序的环境变量: $PATH 绝对路径与相对路径Linux 中路径分为绝对路径与相对路径 绝对路径：由根目录作为起始路径 相对路径：由当前所在路径作为起始路径，当前路径可用 ./ 表示 在编写 shell 脚本时最好采用绝对路径，因为这样不会随着脚本文件所在的路径而对代码执行结果产生影响 常用目录操作Linux 系统常见的目录操作指令有: cd: 变换目录 pwd: 显示当前的目录 mkdir: 创建新目录 rmdir: 删除目录 cd(change directory, 变换目录)每一个登入 Linux 系统的用户第一个进入的目录都是该用户的 Home 目录，即 ~，同样也可以执行 cd ~ 回到 Home 目录，如果仅仅输入 cd 代表的就是 cd ~，cd - 代表回到前一个目录，在预设指令模式(bash shell)中，可以利用 tab 键来自动补齐路径。 pwd(print working directory, 显示当前所在目录)如果想要知道当前所在的工作目录，执行 pwd 即可。 -P 选项是显示当前目录链接的真实目录，例如，ubuntu 系统在 /var/spool/mail 下执行 pwd 会显示 /var/spool/mail，而执行 pwd -P 指令会显示 /var/mail，这表明 /var/spool/mail 链接到了 /var/mail。 返回上一级目录执行 ls -al 我们会看到 mail 目录指向了 ../mail/。12345drwxr-xr-x 0 root root 512 Sep 23 2017 ./drwxr-xr-x 0 root root 512 Mar 1 01:54 ../drwxr-xr-x 0 root root 512 Sep 23 2017 cron/lrwxrwxrwx 1 root root 7 Sep 23 2017 mail -&gt; ../mail/drwx------ 0 syslog adm 512 Apr 5 2016 rsyslog/ mkdir(make directory, 新建目录)默认情况下，执行 mkdir /home/pango/testing 目标目录需要一层一层的新建才行，为了递归创建目录，添加 -p 选项可以自动创建不存在的目录。 另外，通过 -m 选项可以在新建目录时为该目录指定权限，例如当执行 mkdir -p -m 711 /home/pango/testing 时，在该目录树上的所有新建的目录都会具有 drwx--x--x 权限。如果不指定 -m 选项，其默认权限会与 umask 有关，见后文。 rmdir(remove directory, 删除目录)与 mkdir 指令类似，默认情况下，目标目录需要一层一层的删除才行，且被删除的目录必须为空，即该目录下不能存在任何目录或文件。 而 -p 提供了递归删除目录选项，且会删除指定目录下的任何目录和文件，该操作比较危险，使用时需谨慎。 执行程序的环境变量: $PATH指令 ls 的二进制可执行程序所在的目录为 /bin/ls，可是为何可以在任何目录下执行 ls 这个指令呢？这就是环境变量 $PATH 的作用。 当执行 ls 指令时，系统会根据 PATH 的值去每个定义的目录下搜索名称为 ‘ls’ 的可执行文件，如果在 PATH 定义的目录中包含多个名称为 ‘ls’ 的可执行文件，那么先被找到的指令会被执行。 执行 echo $PATH 会在屏幕上打印出所有定义的路径值，$ 表示环境变量，PATH 表示环境变量的键，注意 PATH 一定都是大写字母，其定义的多个路径每个之间由 : 分隔。 现在，如果将 ls 指令从 /bin/ls 通过 mv /bin/ls /root/ 移动到 /root/ 目录下， 即使执行 cd /root 切换到与其相同的目录下，执行 ls 指令仍被告知找不到指定的指令，因为 PATH 中并未定义 /root 路径，系统搜索不到该指令。 可以通过使用绝对路径或相对路径来执行该指令: /root/ls 或 ./ls。 可通过 PATH=&quot;${PATH}:/root&quot; 将 /root 加入到 PATH 变量中。 此外，关于 PATH 需要注意以下几项: 不同的用户的环境变量 PATH 的值是不同的。 PATH 是可以修改的 相比修改 PATH 的值，优先使用绝对路径或相对路径来执行某个指令。 指令放置到正确的目录下 最好不要将当前目录 . 加入到 PATH 当中。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 目录配置]]></title>
    <url>%2Flinux%2Flinux-directory-configuration%2F</url>
    <content type="text"><![CDATA[参考资料: http://linux.vbird.org/linux_basic/0210filepermission.php#dir 本文大纲: Linux 目录配置 根目录 /usr 的意义与内容 /var 的意义与内容 Linux 目录配置Linux 采用 FHS(Filesystem Hierarchy Standard) 标准作为配置目录结构的参考依据，目前，大多数 Linux 的发行版本都遵循了这套标准。FHS 将目录定义为四种交互形态，表格如下： 可分享的(shareable) 不可分享的(unshareable) 不变的(static) /usr(软件所在位置) /etc(配置目录) /opt(第三方协议软件) /boot(开机与核心档) 可变的(variable) /var/mail(用户邮件信箱) /var/run(程序相关) /var/spool/news(新闻群组) /var/lock(程序相关) 可分享的: 可分享给其他系统挂载使用的目录，包括可执行程序与邮件等 不可分享的: 仅与本地系统或配置有关的，分享给其他主机将无法直接使用 不变的: 例如函数库，文件说明系统管理员管理的主机服务配置文件等 可变的: 经常需要改变的资料，如登录档案，用户收取的新闻等。 事实上, FHS 仅定义以下三层目录的用途： /(root, 根目录): 与开机系统有关 /usr(unix software resource): 与软件安装/执行有关 /var(variable): 与系统运作过程有关 从用户角度看，root 指系统的超级管理员身份，从目录角度看，root 指系统的根目录 根目录所有的目录都是从根目录衍生出来。根目录与开机/还原/系统修复等动作有关，FHS 建议根目录不要放在非常大的分区内，且应用程序安装的软件最好也不要与根目录放在同一个分区内，内容越多的分区出错的几率也越高。以下是 FHS 定义的次目录： 必须要存在的目录 档案 描述 /bin 系统有多个放置可执行程序的目录，但 /bin 比较特殊，该目录放置的是「在单人维护模式下仍然能够执行的程序」，该目录的指令可以被 root 与一般用户执行，主要有 cat, chmod, chown, date, mv, mkdir, cp, bash 等常用指令 /boot 主要放置开机会使用的档案，包括 Linux 核心档案及开机选单与开机所需配置等 /dev 在 Linux 系统中，任何装置与外设都是以文件形态存在于这个目录中，存取该目录下的文件等同于存取某个设备，比较重要的有 /dev/null, /dev/zero, /dev/tty, /dev/loop, /dev/sd 等等 /etc 系统主要的⎡配置文件⎦几乎都放在该目录下，FHS 不建议放置任何可执行文件(binary)在这个目录下，例如 /etc/modprobe.d/, /etc/passwd, /etc/fstab, /etc/issue。同时，FHS 还建议：/etc/opt(必须): 这里放置 /opt 下第三方协议软件对应的配置信息 /etc/X11(建议): 与 X Window 有关的各种配置信息放置在这里，尤其是 xorg.conf 这个 X Server 的配置 /etc/sgml(建议): 与 SGML 格式有关的各项配置 /etc/xml(建议): 与 XML 格式有关的各项配置 /lib 系统的函数库非常多，「/lib 放置的是开机会用到以及在 /bin 或 /sbin 下的指令会调用的函数库」。另外，FHS 要求在该目录必须要存在: /lib/modules: 放置可插拔的核心相关模块(驱动程序等) /media 放置可移除的装置，包括软碟，光碟，DVD 等都暂时挂载于此。常见的有 /media/floppy, /media/cdrom 等 /mnt 暂时挂载额外装置的目录，早些时候该目录与 /media 用途相同，在有了 /media 之后，该目录就主要是暂时挂载用途了 /opt 第三方协议软件的目录(即非 Linux 发行版本自带的软件程序)，早些时候，这些软件多数放在 /usr/local 下 /run 早期的 FHS 规定系统开机后所产生的各项资讯应该要放置到 /var/run 目录下，新版的 FHS 则规范到 /run 底下。由于 /run 可以使用内存来模拟，因此具有更高的性能 /sbin Linux 系统有非常多的指令用来设定系统环境，这些指令只有 root 才能使用，其他用户只能使用查询功能，/sbin 下包含了开机，修复，还原系统所需要的指令。某些服务器软件程序，一般放置在 /usr/sbin 中，而本机自行安装的软件产生的系统执行文件(system binary)，则放置在 /usr/local/sbin 中了，常见的指令包括: fdisk, fsck, ifconfig, mkfs 等等 /srv srv 可视为「service」的缩写，某些网络服务启动后，需要取用的资料目录，例如 www，ftp 服务，如果这些服务不需要公开提供给互联网，那么建议放置到 /var/lib 下 /tmp 一般用户或正在执行的程序暂时放置档案的地方，任何用户都可以存取该目录，所以需要定期清理，FHS 建议在开机时应该要将 /tmp 下的资料都删除 /usr 见后文 /var 见后文 FHS 建议可以存在的目录： 档案 描述 /home 系统预设的用户 Home 目录，新用户的 Home 目录会默认创建在该目录下，其代号字符为：~：当表当前用户的 Home 目录~&lt;username&gt;: 代表 username 的 Home 目录 /lib/qual 存放与 /lib 不同格式的二进制函数库，例如 64 位 /lib64 函数库等 /root 系统管理员 root 的 Home 目录，如果进入单人维护模式而仅挂载根目录时，该目录就能够拥有 root 的 Home 目录，所以通常 root 用户的 Home 目录与根目录放置在同一个分区中 /usr 的意义与内容首先 usr 是 Unix Software Resource 的缩写，表示「操作系统软件资源」所放置的目录，而不是 User 的缩写。FHS 建议所有软件开发者应该将他们的资料分别合理的放置到该目录下的次目录中，而不要自行创建独立的目录。Linux 发行版本的所有内置软件都会放到 /usr 下，系统安装完毕后该目录会占用最多的硬盘容量，/usr 次目录的建议有： FHS 要求必须要存在的目录 档案 描述 /usr/bin/ 一般用户能够使用的指令都放在这里，目前 CentOS 7 已经将全部的用户指令放置于此，而使用链接将 /bin 链接至此，意即，/usr/bin 与 /bin 是一摸一样了。另外，FHS 要求在此目录下不应该有子目录。 /usr/lib/ 基本与 /lib 相同，/lib 就是链接到此目录中的 /usr/local/ 系统管理员在本机自行下载安装的软件建议安装到此目录 /usr/sbin/ 非系统正常运行所需要的系统指令，目前 /sbin 链接到此目录中 /usr/share/ 主要放置只读架构的资料档案，包括共享文件 FHS 建议可以存在的目录 档案 描述 /usr/games/ 与游戏相关的资料目录 /usr/include /usr/libexec 某些不会被一般用户常用的执行文件或脚本会放置在该目录 /usr/lib 与 /lib/ 功能相同，/lib 就是链接到此目录 /usr/src 源代码放置目录 /var 的意义与内容/var 会在系统运行过程中渐渐产生内容，包括缓存，日志及某些软件运行所产生的数据，例如 MySQL 的数据文件等，常见的次目录有：FHS 要求必须要存在的目录 档案 描述 /var/cache/ 应用程序运行过程中产生的暂存数据 /var/lib/ 应用程序运行过程中，需要使用的资料档案放置的目录，此目录下每个软件应该有自己的目录，举例来说，MySQL 的资料库应该放置到 /var/lib/mysql。 /var/lock/ 用于对某些装置或档案资源进行排他加锁，目前此目录已移动到 /run/lock /var/log/ 至关重要，放置了程序运行所产生的日志文件 /var/mail/ 放置个人电子邮件的目录，已移动到 /var/spool/mail/ 目录中 /var/run/ 某些程序或服务启动后，会将他们的 PID 放置在这个目录下。与 /run 相同，链接到 /run /var/spool/ 该目录通常放置队列数据，crontab 文件就放置在 /var/spool/cron/ 目录中 以 CentOS 7 为例，比较其目录结构与 FHS 规定的内容的差异： /bin –&gt; /usr/bin /sbin –&gt; /usr/sbin /lib –&gt; /usr/lib /lib64 –&gt; /usr/lib64 /var/lock –&gt; /run/lock /var/run –&gt; /run 主要是将许多原本在根目录下 / 的资料移到了 /usr 中，然后设置了链接]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 预设权限(umask)，隐藏权限(chattr/lsattr)和特殊权限(SUID/SGID/SBIT)]]></title>
    <url>%2Flinux%2Flinux-document-preset%26hidden-privilage%2F</url>
    <content type="text"><![CDATA[预设权限umaskumask 表示当前用户建立档案和目录时的预设权限。12$ umask0022 umask 针对档案和目录的权限是不同的，对于新的档案，是不包括执行(x)权限的，而目录则包括所有权限，返回的数字是指应该减去的分数，进行反向计算，0022 表示，新的档案的权限为 -rwxr-xr-x。例如:123456789$ mkdir test$ cd test$ umask 0022$ touch test_file$ mkdir test_dir$ ls -Aldrwxr-xr-x 0 Pango Pango 512 May 14 23:23 test_dir-rw-r--r-- 1 Pango Pango 0 May 14 23:23 test_file umask -S 加上 -S 参数则显示正向计算的结果12$ umask -Su=rwx, g=rx,o=rx 0022 表示，user 拥有全部权限，而 group 和 others 则被减去了 write 权限。 1$ umask 002 # 直接通过后接数字来设置 umask 的值。 在架设 SAMBA Server 或 FTP Server 时，umask 的值涉及到你的用户能否对新建的档案进行进一步操作的问题。 root 用户的 umask 是 022，一般用户则是 002，这是出于安全的考虑，关于 umask 预设设定可以参考 /etc/bashrc 这个文件，不过不建议修改该档案。 隐藏权限chattr修改档案或目录的隐藏属性，change attribute。1$ chattr [+-=][ASacdistu] 档案或目录名称 +: 追加一个特殊参数 -: 移除一个特殊参数 =: 设置特殊参数 A: 当设定了该参数后，当读取此档案或目录时，其 atime 不会被修改 S: 一般的档案是异步写入磁盘的，如果设置了 S 参数，则对档案进行的任何修改，都会以同步方式写入磁盘 a: 该档案只能新增资料，而不能删除也不能修改资料，只有 root 用户能设定该属性 c: 自动压缩档案，读取时自动解压缩，存储时先压缩再存储 d: 当 dump 程序执行的时候，被设定为 d 属性的档案不参与备份。 i: 使一个档案「无法被删除，修改，改名，设置连接」，只有 root 用户可以设置此属性。 s: 硬删除，无法进行磁盘复原 u: 与 s 相反，如果该档案被删除，可以使用相关工具复原。 以上属性中最重要的当属 a 和 i 了。+a 常用于对某些非常重要的档案的安全控制，而 +i 则常用于日志档案。 lsattr显示档案或目录的隐藏属性，list attributes1$ lsattr -[adR] 档案或目录 -a: 将隐藏档案的属性也显示出来 -d: 如果连接的是目录，仅列出目录本身的属性而非目录内的档案 -R: 递归显示 特殊权限SUIDSet UID 的缩写，该权限有以下限制和功能: SUID 仅对二进制程序有效 执行者对于该程序需要有 x 的执行权限 SUID 仅在执行该程序的过程中有效 执行者将获得该程序拥有者的权限。 例如，/etc/shadow 档案保存了所有用户的密码，该文档的权限为 --------- l root root，意即该档案仅 root 用户可以强制写入，那么一般用户在执行 passwd 命令时其实是可以修改自己的密码的，这就是 SUID 的功能，即: 一般用户对 /usr/bin/passwd 程序具有 x 权限 shadow 档案拥有者是 root 用户 一般用户执行 passwd 的过程中，会「暂时」获得 root 的权限 /etc/shadow 就能被一般用户执行的 passwd 修改。 SUID 对目录是无效的，其以 s 取代「拥有者用户」一栏的 x 权限。 SGID当 s 出现在群组权限的执行权限上时，则称为 SGID。SGID 有如下功能: SGID 对二进制程序有用 使用该程序的用户必须对该程序有 x 权限 用户在执行过程中会获得该程序群组的权限 当 SGIP 作用于目录时，将: 用户对此目录具有 r 和 x 的权限，该用户能够进入此目录 用户在此目录下的有效群组将会变成该目录的群组若用户在目录下有 w 权限，则用户新建的档案，该档案所属的群组与目录所属的群组相同。 当设置 SGID 后，将以 s 取代在「群组」权限一栏的 x 权限。 Sticky Bit目前只针对目录有效，其作用是，用户对某个目录具有 w 和 x 权限时，在该目录下创建档案或目录后，仅用户自己与 root 才能删除这些档案或目录。该功能让多个用户在同一目录下管理与自己相关的文件。 对目录设置 SBIT 后，将以 t 取代「其他用户」权限的 x 权限。 设定 SUID/SGID/SBIT之前的权限设定是只有 3 个数字，加上 SUID/SGID/SBIT 之后变成了 4 个数字，执行 chmod 4755 filename 来为该文件新增特殊权限。4755 的字符串表示为 -rwsr-xr-x。 s 与 t 都是取代 x 这个权限，但当权限为 7666 时，由于在「用户」，「群组」和「其他用户」三栏都不包含「执行(x)」权限，字符串将以大写 S 和 大写 T 来表示，以示该权限实际上无效。例如:12$ chmod 7666 test; ls -l test -rw S rw S rw T 1 root root 0 Jun 16 02:53 test 除了数字法赋值权限之外，还可以使用 u+s 追加 SUID 权限、g+s 追加 SGID 权限和 o+t 追加 SBIT 权限。 file(观察档案类型)file 指令用于判断档案的类型，例如:12$ file ~/.bashrc/root/.bashrc: ASCII text]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 档案与权限(chgrp/chown/chmod)]]></title>
    <url>%2Flinux%2Flinux-profile-and-privilege%2F</url>
    <content type="text"><![CDATA[参考资料: http://linux.vbird.org/linux_basic/0210filepermission.php#filepermission_perm 本文大纲: 修改档案属性及权限 更改群组 更改档案拥有者 更改权限 权限的意义 当执行 ls -al 后，当前目录下所有档案的详情被列出 12dr-xr-xr-x. 17 root root 4096 May 4 17:56 ..-rw-------. 1 root root 1816 May 4 17:57 anaconda-ks.cfg 第 1 列用一组字符组合来表示其类别和属性，该字符串的第一位标识该档案的类别，详情见后文，而后连续 9 个字符，每 3 个一组，表示该权限的拥有者，同群组的其他用户和其他用户对该档案的权限，例如 -rwxr-xr-- 表示一个档案的权限为： 该档案为文件 拥有者对其拥有读取，修改和执行权限 同一群组中的其他用户对其拥有读取和执行权限 其他用户对其仅有读取权限 第 2 列表示有多少不同的档案名链接到该档案的 i-node，每个档案将会将它的权限与属性记录到档案系统的 i-node 中 第 3 列表示档案的拥有者用户 第 4 列表示该档案所属第群组 第 5 列表示该档案所占用第磁盘空间，预设单位为 bytes 第 6 列表示该档案的创建日期或修改日期 第 7 列为档案名 修改档案属性及权限更改群组chgrp: 该指令为 change group 的缩写，目标群组必须在 /etc/group 中存在，使用 -R 指定递归改变父级目录下的群组:1$sudo chgrp &lt;target-group&gt; -R filename/directory 更改档案拥有者chown 为 change owner 的缩写，目标用户必须在 /etc/passwd 中存在，可使用 -R 选项指定递归更改选项。 该命令还可以顺便修改群组，例如:1$sudo chown user1:group1 filename 可使用 . 或者 : 来隔开用户名和群组名，但如果在用户名中包含 . 字符，则容易造成混淆，所以一般建议使用 : 来隔开用户名和群组。 chown 也可通过 . 直接更改群组,如:1$sudo chown .group1 filename 更改群组一个常见的应用为，当通过 cp 复制一个档案时，也会同时复制命令执行者的属性与权限，此时更改属性和权限就显得尤其必要了 更改权限chmod 为更改档案权限的指令，默认情况下，可使用字符来为三种角色指定权限，首先指定角色: u: 代表拥有者，即 users g: 代表拥有者群组，即 group o: 代表其他，即 others a: 代表所有，即 all 然后使用以下连字符修改权限: =: 代表赋值，指定 +: 代表增加权限 -: 代表移除权限 例如 chmod u=rwx,go=r-x filename 表示设定档案的权限为 rwxr-xr-x，或 chmod g+w filename 在不知道原来权限的基础上增加群组的写入权限。 chmod 可以使用数字或字符来更改档案的权限，数字代表的含义为: r: 4 w: 2 x: 1 -: - 每种身份的权限由三者数字累加起来得到一个数字，例如 chmod 740 filename 表示拥有者具有 rwx 权限，同群组其他用户具有 r-- 权限，其他用户则具有 --- 权限。 权限的意义权限分为三种： r: 读取权限 针对文件：读取文件的实际内容 针对目录：读取目录结构清单，例如使用 ls 指令 w: 写入权限 针对文件: 可编辑，新增或修改文件的内容，但不含删除操作 针对目录: 具有对改动该目录结构的权限，包含： 新建文件与目录 删除已经存在的文件或目录(不受被删除文件和目录的权限限制) 重命名文件或目录 移动文件或目录 x: 执行权限 针对文件: 执行文件(如果该文件可执行) 针对目录: 指示用户能否进入该目录，如执行 cd 指令表格形式如下： 元件 内容 迭代物件 r w x 文件 详细资料 文件夹文件 读取文件内容 修改文件内容 执行文件内容 目录 目录名称 可分类抽屉 读取目录内容 修改目录结构 进入该目录 除了文件和目录两种档案之外，还有其他一些档案类型： 常规文件(regular file)，常规文件档案，权限的第一栏为 -，例如 -rwxrwxrwx，常规文件又可分为三类： 文本文件: 可以直接读取的内如，几乎所有的配置型文件都属于这一类，可执行 cat &lt;filename&gt; 来查看文件内容 二进制文件: Linux 系统仅认知且可执行二进制文件(binary file)，script 脚本及批处理不算，例如，当执行 cat 指令时，cat 就是一个 binary file。 数据文件(data): 供不同的程序自己使用的特殊格式的文件，例如，当使用 Linux 登入时，系统会将登录日志记录在 /var/log/wtmp 文件内，但该文件是一个数据文件，只能通过 last 指令读出来，若使用 cat 读取该文件会显示乱码 目录(directory): 目录档案，权限第一栏为 d，例如 drwxrwxrwx 链接(link): 类似于 windows 系统的快捷方式，第一栏为 l，例如 lrwxrwxrwx 软链接: 见 inode 硬链接: 见 inode 设备与装置档案: 系统周边及存储等相关的档案，通常集中在 /dev 目录下，他们又分为： 区块(block)设备档案: 存储资料，提供系统随机存储的设备，硬盘与软盘都属于这类，第一栏为 b，可查阅 /dev/sda 下的内容。 字符(character)设备档: 亦即外设，如键盘，鼠标等，这些设备的特点是「一次性读取」，不能截断输出，例如，不能让鼠标「跳跃到」另外一个地方，而是「连续滑动」到另一个地方。权限第一栏为 c 数据接口档案(sockets): 主要用于网络数据通信，第一栏为 s，该类型通常位于 /run 或 /tmp 这些目录中 数据传输接口档案(FIFO, pipe): FIFO(first in first out)也是一种特殊的档案类型，它是为了解决多个解决同时存取一个文件所造成的错误问题。第一栏为 p 设备是系统文件，最好不好随意修改。 windows 系统中可执行文件通常带有 .exe .bat 等扩展名，而 Linux 系统中基本没有所谓的「扩展名」概念，一个 Linux 文件能否被执行与它的权限栏 10 个属性有关，与其文件名或扩展名没有任何关系，只要其权限中带有 x 属性，该文件就可以被执行。 为了保持可读性，通常还是会在文件末尾加上扩展名以让用户了解该文件是什么，常用的扩展名有： *.sh: 脚本或批处理文件，因为这些文件基于 shell 写出，所以为其添加了 .sh 扩展名 Z, .tar, .tar.gz, .zip, *.tgz: 压缩包文件，由不同的压缩程序如 gunzip, tar 等输出，所以根据不同的程序命名不同的扩展名 .html, .php: php 语法写成的网页文件等 总之，Linux 的文件名仅仅是为了让用户了解其用途而已，能否执行取决于文件权限，如果 /bin/ls 文件的可执行权限被移除，那么 ls 便无法使用了。 关于档案的几点注意： 单一文件或目录文件名长度不超过 256 字节 名称中不得包含 * ? &gt; &lt; ; &amp; ! [ ] | \ ‘ “ ` ( ) { } 等特殊字符 第一个字符为 . 时，表示该档案为隐藏档案]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础 - 用户和群组]]></title>
    <url>%2Flinux%2Flinux-account-and-group%2F</url>
    <content type="text"><![CDATA[参考资料： http://linux.vbird.org/linux_basic/0410accountmanager.php 本文大纲: 用户帐号管理 新建用户帐号 - useradd 设置帐号密码 - passwd 管理帐号密码策略 - chage 修改用户帐号信息 - usermod 删除用户帐号 - userdel 用户端指令 - id, finger, chfn, chsh 群组管理 新增群组 - groupadd 修改群组 - groupmod 删除群组 - groupdel 群组管理员 - gpasswd 初始群组，次要群组和有效群组 Linux 使用文件及配置相应的权限来存储用户帐号和群组信息，在成功创建好用户帐号之后，以下几处文件会发生相应的改变 /etc/passwd: Linux 管理用户帐号的数据库 /etc/shadow: Linux 管理用户帐号密码配置的数据库 /etc/group: Linux 管理群组的数据库 /etc/gshadow: Linux 管理群组密码配置的数据库 用户帐号管理Linux 系统用户管理相关命令主要有： useradd：新建用户 passwd：设置用户密码 chage: 变更密码策略 usermod：编辑用户 userdel：删除用户 新建用户帐号 - useraddlinux 系统以 useradd 命令来新建 Unix 用户，该命令带有一系列参数： 12345678910111213useradd [-u UID] [-g 初始群组] [-G 次要群组] [-mM] [-c 说明栏] [-d 家目录绝对路径] [- s shell] &lt;new-user-name&gt;-u ：接 UID ，一组数字，直接指定一个特定的 UID 给这个用户-g ：为该用户指定初始群组 initial group，该群组的 GID 会被放置到/etc/passwd 的第四个栏位-G ：为该用户指定次要群组，该选项与参数会修改 /etc/group 内第四栏的值-M ：不要为用户建立 Home 目录(系统帐号预设值)-m ：为用户建立 Home 目录(一般帐号预设值)-c ：用户备注，该值修改 /etc/passwd 文件的第五栏的内容-d ：指定特定目录成为该用户的 Home 目录，而不采用预设值，需指定绝对路径-r ：建立一个系统帐号，该帐号的 UID 会有限制(参考/etc/login.defs)-s ：指定 `shell` ，若不指定则预设为 /bin/bash-e ：指定该用户的过期日期，格式为『YYYY-MM-DD』此项写入 `/etc/shadow` 第八栏-f ：指定密码是否立即失效，写入 /etc/shadow 的第七栏位。0 为立刻失效，-1 为永远不失效(密码只会过期而强制于登入时重新设定而已) 例如，执行以下命令新建一个用户：1$sudo useradd pango 该命令创建一个名为 pango 的用户，由于没有使用 -r 指令，将随机分配一个 1000 以上的数值作为 uid，创建一个名为 pango 的群组并作初始群组，为该用户创建 Home 目录，使用 /bin/bash 作为工作环境。该命令对系统的以下部分做出修改： 在 /etc/passwd 里面新建一行用户值，包含 UID/GID/Home 目录等； 在 /etc/shadow 里面将此用户的密码相关参数填入，但是尚设置密码； 在 /etc/group 里面加入一个与用户名称一样的群组名称，如果已经存在与该用户名同名的群组名，则不会新建群组 在 /home 目录下新建一个与帐号同名的目录作为用户的 Home 目录，且权限为 700 注意，在 linux 系统中新增用户帐号和群组时，名称是区分大小写的 useradd 指令参考 /etc/defaults/useradd 文件中的配置作为缺省参数的预设值，使用 useradd -D 指令查看预设值：12345678$ useradd -DGROUP=100 # 预设的群组，系统内 GID = 100 的群组为 users 群组，但目前很多发行版都采用私有群组制，即为每个新建的用户帐号创建与之同名的群组来隔离权限HOME=/home # 预设的 Home 目录INACTIVE=-1 # 密码失效日期，对应 /etc/shadow 第 7 栏EXPIRE= # 用户帐号失效日，对应 /etc/shadow 第 8 栏SHELL=/bin/sh # 默认使用的 shell 程序名，如果 server 不希望任何新建的用户帐号登入系统取得 shell，那么将此项改为 /sbin/nologinSKEL=/etc/skel # Home 目录初始内容参考目录，即为新建用户创建 Home 目录时需要复制的初始内容，例如，在该目录下新增 www 目录，那么后续每个创建的新用户 Home 目录都会有 www 目录CREATE_MAIL_SPOOL=no # 是否主动帮用户建立邮件信箱，如果该项为 yes，则会新建 /var/spool/mail/&#123;username&#125; 目录作为该用户的邮箱 使用 nano 或 vim 查看 /etc/default/useradd 文件可以得到更多的详细信息：1$ nano /etc/default/useradd 登录信息参考在 /etc/login.defs 文件中定义，使用 nano 查看该文件123456789101112131415161718192021$ nano /etc/login.defsMAIL_DIR /var/spool/mail # 用户预设邮件信箱放置目录PASS_MAX_DAYS 99999 # /etc/shadow 内的第 5 栏，多久需变更密码日数 PASS_MIN_DAYS 0 # /etc/shadow 内的第 4 栏，多久不可重新设定密码日数 PASS_MIN_LEN 5 # 密码最短字符长度，已被 pam 模组取代PASS_WARN_AGE 7 # /etc/shadow 内的第 6 栏，过期前会警告的日数UID_MIN 1000 # 用户最小的 UID，意即小于 1000 的 UID 为系统保留 UID_MAX 60000 # 用户能够用的最大 UID SYS_UID_MIN 201 # 保留给用户自行设定的系统帐号最小值 UID SYS_UID_MAX 999 # 保留给用户自行设定的系统帐号最大值 UID GID_MIN 1000 # 用户自订群组的最小 GID，小于 1000 为系统保留 GID_MAX 60000 # 用户自订群组的最大 GID SYS_GID_MIN 201 # 保留给用户自行设定的系统帐号最小值 GID SYS_GID_MAX 999 # 保留给用户自行设定的系统帐号最大值 GIDCREATE_HOME yes # 在不加 -M 及 -m 时，是否主动建立用户家目录UMASK 077 # 用户 Home 目录建立的 umask ，因此权限会是 700 USERGROUPS_ENAB yes # 使用 userdel 删除时，是否删除初始群组 ENCRYPT_METHOD SHA512 # 密码加密的机制使用的是 sha512 加密算法 所以，系统在执行 useradd 命令时至少会参考： /etc/default/useradd /etc/login.defs /etc/skel/* 现在查看 /etc/passwd 档案的最后一行：1pango:x:1000:1000::/home/pango:/bin/bash 该文件是存放所有用户信息的数据库，每一行代表一个用户，每行由 6 个 : 分隔成了 7 个栏位： 用户名 密码 用户 UID：0 保留给系统管理员 root 账号；1~999 保留给系统账号，这些账号通常为执行某些系统服务所用，不能通过 shell 登录；1000~4294967295，保留给一般账号 用户初始群组 GIP 备注 Home 目录，预设值为 /home/{username} 执行 Shell 的程序目录，预设值为 /bin/bash 设置帐号密码 - passwd现在，新建的用户 pango 还没有密码，在设置密码之前是无法登录的，执行 passwd 来为新用户指定密码：12345678910111213141516$ passwd [--stdin] [帐号名称] # 所有人均可使用来改自己的密码 $ passwd [-l] [-u] [--stdin] [-S] [-n 日数] [-x 日数] [-w 日数] [-i 日数] 帐号 # root 功能选项与参数：--stdin ：可以透过来自前一个管线的资料，作为密码输入，对 shell script 有用-l ：是 Lock 的意思，会将 /etc/shadow 第 2 栏最前面加上 '!' 使密码失效；-u ：与 -l 相对，是 Unlock 的意思-S ：列出密码相关参数，亦即 shadow 档案内的大部分信息-n ：自上一次修改密码后的密码不可修改天数，shadow 的第 4 栏位-x ：自上一次修改密码后的密码有效天数，shadow 的第 5 栏位-w ：密码过期前的警告天数，shadow 的第 6 栏位-i ：密码失效缓冲天数，shadow 的第 7 栏位$sudo passwd pangoEnter new Unix Password:Retype new Unix Password: passwd 指令在不接参数时表示修改当前登录用户的密码 密码设置成功后，在 /etc/shadow 文件中发生了一些改变，定位到该文件的最后一行：1pango:$6$csIys5qj$OslSKU.3SljbHXTXJEPgWvNi1w9CGlBKO3uqJyWueQN1ypA7SuNzJWjesdSvg6KPv0X6tRmkkDBFI2cbSJ.xR/:17600:0:99999:7::: 该文件存储了所有用户的密码相关配置，每一行代表一个用户的密码信息，每行由 8 个 : 分隔为 9 个栏位： 用户名 经过加密的密码 最近修改密码日期，该值为一个代表 day 的数值，从 1970-01-01 日算起 密码不可修改天数：该值指示密码在最近一次修改后多久之后才能再次被修改 密码有效期（天）：该值指示密码再最近一次修改后的有效天数 密码需要变更前的警告天数 密码失效延迟（天）：密码过期后的缓冲时间 账号失效日期（天）：该值指示该用户的总生命周期多长，与密码有效无效无关 系统保留扩展项： 通过标准输入 --stdin 来修改密码，例如，帮助 pango 用户将其密码修改为 ‘abc543CC’ 123$ echo "abc543CC" | passwd --stdin pangoChanging password for user pango.passwd: all authentication tokens updated successfully. --stdin 选项并不存在于所有 Linux 发行版本的系统中，使用前先 man passwd 查看是否支持 管理帐号密码策略 - chage除了使用 passwd 指令，还可以使用 chage 指令来管理密码策略，其用法大致如下：123456789$ chage [-ldEImMW] 用户帐号名选项与参数：-l ：列出该帐号的详细密码参数；-d ：修改 shadow 第 3 栏位(最近一次更改密码的日期)，格式 YYYY-MM-DD-E ：修改 shadow 第 8 栏位(帐号失效日)，格式 YYYY-MM-DD-I ：修改 shadow 第 7 栏位(密码失效日期)-m ：修改 shadow 第 4 栏位(密码最短保留天数)-M ：修改 shadow 第 5 栏位(密码多久需要进行变更)-W ：修改 shadow 第 6 栏位(密码过期前警告日期) passwd -S 只是简单显示了密码详情，而 chage -l 更多为管理员提供了更强的参考信息，列出 pango 的详细密码参数 ：12345678$ chage -l pangoLast password change : Jul 20, 2015Password expires : Sep 18, 2015Password inactive : Sep 28, 2015Account expires : neverMinimum number of days between password change : 0Maximum number of days between password change : 60Number of days of warning before password expires : 7 修改用户帐号信息 - usermod当需要修改用户帐号的信息时，我们可以通过前往相应的文件例如，/etc/passwd 和 /etc/shadow 中去修改对应行的值以达到修改用户帐号信息的目的。也可以使用 usermod 指令对用户帐号进行修改：1234567891011121314$ usermod [-cdegGlsuLU] username 选项与参数：-c ：修改用户备注，对应 /etc/passwd 第 5 栏-d ：修改帐号 Home 目录，对应 /etc/passwd 的第 6 栏；-e ：修改帐号失效日期，格式是 YYYY-MM-DD，对应 /etc/shadow 内的第 8 栏-f ：修改密码失效延迟时间，对应 shadow 的第 7 栏。-g ：修改初始群组，修改 /etc/passwd 的第 4 栏-G ：修改次要群组组，修改该用户支援的群组，修改应用到 /etc/group-a ：与 -G 合用，表示 append，追求次要群组而非改变-l ：修改帐号名称， /etc/passwd 的第 1 栏-s ：修改 shell 接入程序，后面接 Shell 的实际程序，例如 /bin/bash 或 /bin/csh 等等-u ：修改 uid 对应 /etc/passwd 第 3 栏-L ：暂时锁定用户，暂时无法登入。仅修改 /etc/shadow 的密码栏。-U ：解锁用户，移除 /etc/shadow 密码栏的 '!' 删除用户帐号 - userdel当需要删除用户帐号时，执行 userdel 命令，该命令会对以下文件造成影响： 用户帐号/密码相关值：/etc/passwd，/etc/shadow 用户群组相关参数：/etc/group，/etc/gshadow 用户个人资料目录：/home/{username}，/var/spool/mail/{username} 该指令用法如下：123$ userdel [-r] username 选项与参数：-r ：连同用户的 Home 目录也一起删除 例如，删除 pango 用户帐号及其 Home 目录：1$sudo userdel -r pango 通常，在移除一个帐号时，我们可以手动修改 /etc/passwd 和 /etc/shadow 文件中该用户关联的行数据。如果该帐号只是「暂时冻结」，那么将 /etc/shadow 中第 8 栏（帐号失效日）设为 0 就可让该账户无法使用，但所有与该帐号相关的资料都会保留，使用 userdel 意味着「真的确定该用户不会在主机上的使用任何资料了」。 通常，一个用户在使用主机一段时间之后，会在更多其他的目录中产生属于他的文档，因此，在下达 userdel -r username 之前，先以 find / -user username 指令查出整个系统内属于 username 的档案，删除之后，再删除该用户帐号。 用户端指令 - id, finger, chfn, chshuseradd，usermod，userdel 都是系统管理员才能使用的命令，一般用户无法进行操作，有以下几个指令供一般用户使用： id: 该指令可以查询当前用户或其他用户的 UID/GID 值 finger: 查询用户的口令信息，将有 Login: 用户帐号信息 Name: 备注信息 Directory: Home 目录 Shell: shell 对应的程序 Never logged in: 登入信息 No mail: 查看 /var/spool/mail 中的信箱资料 No Plan: 查看 ~{username}/.plan 资料 chfn: 修改指纹信息，不常用 chsh: 修改 shell，参数如下： -l，列出所有可用的 shell，即 /etc/shells 中的内容 -s，修改为指定的 shell 群组管理群组管理涉及新增，修改和删除，群组的内容与以下两个档案有关： /etc/group /etc/gshadow 新增群组 - groupadd使用 groupadd 来新增群组，该命令使用方法如下：1234$ groupadd [-g gid] [-r] 群组名称选项与参数：-g ：指定群组 GID-r ：指定该群组为系统群组，与 /etc/login.defs 内的 GID_MIN 有关 执行命令查看新建的群组：123$ nano /etc/group...&lt;your-new-group-name&gt;:x:&lt;gid&gt;: 该命令查询 /etc/group 文件，该文件是 linux 系统保存所有群组的数据库，每一行代表一个群组，每行的值由 3 个 : 分隔为 4 栏不同的值，其具体指： 群组名称 群组密码，以 x 表示，引用 /etc/gshadow 的第 2 栏 群组 id 该群组包含的用户名称，每个用户名称由 , 分隔 x 表示该群组的密码，其在 /etc/gshadow 文件中对应第 2 栏的值，gshadow 文件保存了所有群组的详细配置，但只有拥有 root 权限的用户才能查看该文件。由于该群组刚刚创建，没有指定任何用户，故该值为空。 查看 /etc/gshadow 文件可以看到对应的行，其格式如下：1&lt;your-new-group-name&gt;:!::usernames 这个文件内的格式几乎与 /etc/group 一摸一样，第 2 栏是密码栏，如果密码栏为 ‘!’ 或空，表示该群组不具有群组管理员： 群组名称 密码栏，若为 ‘!’ 表示无合法密码，无群组管理员 群组管理员的帐号 所有加入该群组的帐号，与 /etc/group 内容相同 修改群组 - groupmod与 usermod 类似，groupmod 指令用于对已有群组的信息进行修改1234$ groupmod [-g gid] [-n group_name] 群组名选项与参数：-g ：修改既有的 GID 值；-n ：修改既有的群组名称 一个群组创建之后，为了避免引起不必要的错乱，通常不建议随意修改其 GID 删除群组 - groupdel1groupdel [groupname] 使用 groupdel 来删除已有的群组，如果有用户帐号已经关联一个群组作为其初始群组，将无法直接删除该群组，否则当用户登入系统后，系统将找不到其对应的群组。想要删除该群组，必须： 修改与之关联的用户的初始群组 删除该用户，再删除群组 群组管理员 - gpasswd1234567891011$ gpasswd groupname $ gpasswd [-A user1,...] [-M user3,...] groupname $ gpasswd [-rR] groupname 选项与参数：若没有任何参数时，表示给予 groupname 一个密码(/etc/gshadow)-A ：将 groupname 的主控权交由参数代表的多个用户管理(该群组的管理员)-M ：将某些帐号加入这个群组当中！-r ：将 groupname 的密码移除-R ：让 groupname 的密码栏失效-a ：将用户加入到 groupname 这个群组中-d ：将用户移除 groupname 这个群组中 初始群组，次要群组和有效群组在执行 useradd 命令新建用户时，使用 -g 命令指定初始群组，该群组的 GID 被写入到 /etc/passwd 对应的 GID 栏位，而 -G 指定的次要群组则将用户名写入 /etc/group 第 4 栏。 当用户登入系统进入 shell 环境后，系统总是以该用户所在的初始群组作为有效群组，即，当用户执行类似 touch 的指令创建新的文件或文件夹时，其权限会给予用户当前的有效群组。执行 groups 可查看当前登录用户所属的所有群组，排在第一位的即表示当前有效群组。 可通过执行 newgrp 指令将用户所属的另一个群组切换为当前有效群组，该动作导致用户进入另一个 shell 环境，当用户完成操作不再需要该群组支持时，应使用 exit 指令退回到之前的 shell 环境。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Module 与嵌套 Repository]]></title>
    <url>%2FVersion-Control%2FGit%2Fgit-embeded-repository%2F</url>
    <content type="text"><![CDATA[本文大纲: 添加一个 Submodule 获取子项目更新 Git Module Git 可将一个 Git repository 作为另外一个 Git repository 的子目录，这允许在你的项目中引用另一个项目，并将两个项目分开维护。假设我们希望将一个现有的 Git repository 作为一个 submodule 添加到当前工作项目上，执行 git submodule add 并跟上绝对或相对 url 作为参数来添加 submodule。 Submodules 至少有两种应用场景： 项目依赖一个外部项目，并希望将两者分开维护 将一个大项目拆分为多个小项目并将它们黏合在一起 添加一个 Submodule1$ git submodule add https://github.com/chaconinc/DbConnector 默认情况下，submodules 将使用与 Git repository 一致的名称作为 directory 添加到当前项目的根目录下，在这个例子中为 “DbConnector”。也可以在命令最后指定一个自定义的路径作为该 submodule 的目录。 执行 git status，看到以下变化：123456789$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: .gitmodules new file: DbConnector 一个新的文件 .gitmodules 被创建了，该配置文件存储了当前项目与 submodule 之间的关系映射，以下是其内容举例：123[submodule "DbConnector"] path = DbConnector url = https://github.com/chaconinc/DbConnector 如果包含多个 submodule，那么该文件会有更多的 entry，值得注意的是，该文件一起受 git 版本控制。 作为 submodule 被管理的 Git repository 将不会受到父级 Git repository 的状态追踪。因此，在当前项目执行 git push origin master 之后，从另一台机器 git clone 父级项目时，将只包含 .gitmodules 文件及对应于各个 submodules 的空的 directory。123456789101112131415161718192021222324$ git clone https://github.com/chaconinc/MainProjectCloning into 'MainProject'...remote: Counting objects: 14, done.remote: Compressing objects: 100% (13/13), done.remote: Total 14 (delta 1), reused 13 (delta 0)Unpacking objects: 100% (14/14), done.Checking connectivity... done.$ cd MainProject$ ls -latotal 16drwxr-xr-x 9 schacon staff 306 Sep 17 15:21 .drwxr-xr-x 7 schacon staff 238 Sep 17 15:21 ..drwxr-xr-x 13 schacon staff 442 Sep 17 15:21 .git-rw-r--r-- 1 schacon staff 92 Sep 17 15:21 .gitmodulesdrwxr-xr-x 2 schacon staff 68 Sep 17 15:21 DbConnector-rw-r--r-- 1 schacon staff 756 Sep 17 15:21 Makefiledrwxr-xr-x 3 schacon staff 102 Sep 17 15:21 includesdrwxr-xr-x 4 schacon staff 136 Sep 17 15:21 scriptsdrwxr-xr-x 4 schacon staff 136 Sep 17 15:21 src$ cd DbConnector/$ ls$ 要获得与提交之前的 submodules，一种方式是执行 git submodule init 指令来初始化本地配置和 git submodule update 从 submodules 对应的地址 clone 所有数据并签出对应的 commit。12$ git submodule init$ git submodule update 或者：1git submodule update --init --recursive 另外一种方式是在执行 git clone 父级项目时添加 --recurse-submodules 参数，将一次完成父级项目及所有 submodules 的克隆1$ git clone --recurse-submodules https://github.com/chaconinc/MainProject 之后，如果希望同父级项目一同获取所有 submodules 的更新，执行1git pull --recurse-submodules 获取子项目更新使用 Git Submodules 一个最典型的应用场景是，引用一个由外部维护的 Git repository，仅仅使用它而不做任何修改。 首先导航到指定 submodule 所在目录，执行 git fetch 和 git merge 获取本地更新。12345678910$ git fetchFrom https://github.com/chaconinc/DbConnector c3f01dc..d0354fc master -&gt; origin/master$ git merge origin/masterUpdating c3f01dc..d0354fcFast-forward scripts/connect.sh | 1 + src/db.c | 1 + 2 files changed, 2 insertions(+) 回到父级项目目录，执行 git diff --submodule，可以看到 submodules 已经获得更新并列出一个添加到该项目的 commit 列表。如果不想每次在执行 git diff 时都输入 --submodule 参数，可在 git config 文件中添加该命令的默认参数或执行 git config diff.submodule log，之后再执行 diff 将会将列出所有子项目更新日志。12345$ git config --global diff.submodule log$ git diffSubmodule DbConnector c3f01dc..d0354fc: &gt; more efficient db routine &gt; better connection routine 此时如果主项目提交至远程 Git repository，之后其他开发人员再获取代码时将会得到与主项目同步后的 submodule。 另外一种方式是直接在主项目目录下执行 git submodule update --remote 命令，git 将自动更新所有 submodules。12345678$ git submodule update --remote DbConnectorremote: Counting objects: 4, done.remote: Compressing objects: 100% (2/2), done.remote: Total 4 (delta 2), reused 4 (delta 2)Unpacking objects: 100% (4/4), done.From https://github.com/chaconinc/DbConnector 3f19983..d0354fc master -&gt; origin/masterSubmodule path 'DbConnector': checked out 'd0354fc054692d3906c85c3af05ddce39a1c0644' 该命令将默认以所有 submodules 的 master branch 作为更新依据，如何以另外一个 branch name 作为默认更新的依据，参考下文的Git Module 可在 git config 文件中为 status 命令添加默认参数或执行 git config status.submodulesummary 1，之后执行 git status 显示简短的摘要1234567891011121314151617$ git config status.submodulesummary 1$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: .gitmodules modified: DbConnector (new commits)Submodules changed but not updated:* DbConnector c3f01dc...c87d55d (4): &gt; catch non-null terminated lines Git Modulesubmodule 是嵌套在另一个 repository 中的 repository，submodule 有自己的版本历史，包含子模块的 repository 称为 superproject .gitmodules 放置在一个 Git 工作树的顶级目录下，该文件是一个匹配 git-config 的文本文件。该文件的每个 subsection 代表一个 submodule 的配置项，subsection 的值为 submodule 的名称，如果不显式指定 name 选项，该名称值将取该 submodule 的路径作为名称。同时，每一个 subsection 包含以下必填值： submodule.&lt;name&gt;.path: 相对于 Git working tree 顶层目录的路径，默认迁出位置，该值不能以 / 结尾，所有 submodule 的路径必须唯一。 submodule.&lt;name&gt;.url: 克隆 submodule 的 url，该值可以是一个绝对 url，也可以 ./ 或 ../ 起头作为 superproject origin repository 的相对 url。 同时，还有以下可选参数： submodule.&lt;name&gt;.update: 定义 submodue 的默认更新行为，在 superproject 执行 git submodule update 指令时如何更新 submodule.&lt;name&gt;.branch: 提供用于检测更新的 branch 名称，如果不指定该值，默认取 master。. 作为特殊值告知 git 取与当前 repository 当前 branch 一致的名称 submodule.&lt;name&gt;.fetchRecurseSubmodules: 该用于控制对 submodule 的递归 fetch，如果在 superproject 的 .git/config 中已经设置了该值，那么该值将覆盖在 .gitmodules 中的值。两者均可被 git fetch 和 git pull 使用 --[no-]recurse-submodules 选项覆盖 submodule.&lt;name&gt;.ignore: git status 和比较器如何对已经做出修改的 submodules 进行反应，可取以下值 all: submodule 永远不会被认为已经 modified，但在 staged 后会显示出来 dirty: 所有对 submodule 工作目录下做出的修改将被忽略，只有其 Head 与 superproject 的记录状态会纳入考虑 untracked: 只有 untracked 文件会被忽略 none: 所有修改都不会被忽略 submodule.&lt;name&gt;.shallow: 当设置为 true 时，该 submodule 会执行浅 clone(只包含一层深度的历史信息) 例子：123[submodule "libfoo"] path = include/foo url = git://foo.com/git/lib.git 文件系统中，一个 submodule 通常 在 superproject 的 $GID_DIR/modules/ 有一个 Git 目录 在 superproject 工作目录下有一个对应的子目录作为其工作目录 在其工作目录中根目录下包含一个 .git 文件指向 (1) 所在的位置 假设一个 submodule 的 Git 目录位于 $GIT_DIR/modules/foo/，工作目录位于 path/to/bar/，superproject 通过一个 path/to/bar/ 目录树下的 gitlink 和 .gitmodules 文件中的一个条目 submodule.foo.path = path/to/bar 来追踪这个 submodule。 参考资料： https://git-scm.com/book/en/v2/Git-Tools-Submodules https://git-scm.com/docs/gitmodules https://git-scm.com/docs/gitsubmodules]]></content>
      <categories>
        <category>Version Control</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 MacOS 上安装 nginx]]></title>
    <url>%2FMacOS%2Fmacos-install-nginx%2F</url>
    <content type="text"><![CDATA[不同于其他 Linux 系统，MacOS 使用了 brew 作为包管理器，并同时肩负了服务进程管理器的角色，在本机完成网站内容的准备工作之后，开始考虑把 MacOS 变成一个 web server。 安装 nginx使用 HomeBrew 在 MacOS 上安装 nginx:1$ brew install nginx 安装完成后，会显示如下提示信息：12345678910111213==&gt; CaveatsDocroot is: /usr/local/var/wwwThe default port has been set in /usr/local/etc/nginx/nginx.conf to 8080 so thatnginx can run without sudo.nginx will load all files in /usr/local/etc/nginx/servers/.To have launchd start nginx now and restart at login: brew services start nginxOr, if you don&apos;t want/need a background service you can just run: nginx==&gt; Summary 简要归纳为： 网站根目录在 /usr/local/var/www 默认端口在 为 /usr/local/etc/nginx/nginx.conf 文件中配置为 8080，这样 nginx 无需 $sudo 权限就可执行 nginx 将从 /usr/local/etc/nginx/servers/ 目录读取所有配置文件信息 要让 nginx 随用户登录一同启动请执行命令 brew services start nginx 若不需要作为后台服务启动，请直接执行命令 nginx 配置新站点创建一个目录作为新站点的根目录，例如1mkdir -p /usr/local/var/www/your-site-name 查看 /usr/local/etc/nginx 目录，有一个名为 nginx.conf.default 的文件，该文件通常为网站配置的起始点。首先导航到 /usr/local/etc/nginx/servers/ 目录，然后将该默认配置文件复制一份。12cd /usr/local/etc/nginx/servers/cp ../nginx.conf.default your-site-name.conf 修改 listen, server_name, location 块下的 root 和 index 项的值到对应的值，保存并退出配置文件。利用 nginx -t 检查配置的有效性，一切准备就绪后，执行 brew services reload nginx 命令重新加载配置。 之后，通过在浏览器中的输入对应主机的 ip 地址或域名及端口号，即可访问到部署到 nginx 的站点]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>diy</tag>
        <tag>make-the-best-of-my-macmini</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git push 到 MacOS]]></title>
    <url>%2FMacOS%2Fmacos-git-push-to-repository%2F</url>
    <content type="text"><![CDATA[MacOS 没有自带 git，需要通过 XCode Command Line Tools 进行安装，当通过 ssh 客户端远程登录到 MacOS 并执行 git --version，MacOS 会自动弹出确认框以进行安装，这里需要手动在 MacOS 系统界面下点击确认按钮才能继续。完成安装后再次执行 git --version，已能够看到版本号。新建或导航到一个目录，将其作为新 repository 的根目录。 12mkdir -p development/hobby-project/my-first-git-repositorycd development/hobby-project/my-first-git-repository 因为这里有 3 层目录结构，不带任何参数的 mkdir 命令将不会递归创建目录，-p 选项表示，如果任何层级的目录不存在，将递归创建目录，如果所有层次的目录都已存在，则不会进行任何操作。同时，要确保登录到远程机器的用户拥有针对创建目录的所有层级的读写权限，否则 git pull 和 git push 将会报错。如果权限不正确，使用 $sudo chown -R 命令更改文件夹或文件权限。 现在，development/hobby-project/my-first-git-repository 将作为远程 repository 根目录，执行：12$sudo git init --bareInitialized empty Git repository in /Users/XXX/development/hobby-project/my-first-git-repository/ git init --bare 参数指明该 repository 用于分布式版本控制的中心仓库，git 将仅保存历史记录，一般来说服务器上的仓库多使用 --bare 创建，其目的在于分发而非修改，参考 what is a git bare repository。 远程仓库创建完成后，回到客户端主机，这里采用 ssh 协议进行通信，如果没有现成的 key，需要先生成 key pair。ssh 默认配置存放在用户 home 目录下的 .ssh 文件夹下，如果没有指定 config 文件，则 id_rsa 作为默认的 key 文件名，具体参考 How To Configure Custom Connection Options for your SSH Client。]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>diy</tag>
        <tag>make-the-best-of-my-macmini</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 Windows 环境下远程控制 MacOS]]></title>
    <url>%2FMacOS%2Fmacos-remote-control%2F</url>
    <content type="text"><![CDATA[为了把闲置已久的 Mac mini 变成一个可在 Windows 环境下远程操作的小型私有服务器，需要现实至少以下两点目标： 使用 ssh 客户端远程登录，类似通过 PuTTy 管理其他类 Linux 系统 使用 sftp客户端实现文件传输的需求 同时，要确保 MacOS 主机与客户端主机在网络上是互通的，要么在同一局域网下通过 ip 地址访问，要么使用域名映射固定 ip 地址。 准备工作MacOS 自带了 ssh，但默认是禁用的，通常我们进行以下步骤的确认： 12$sudo systemsetup -getremoteloginRemote Login: Off 该命令查询本地计算机远程登录功能是否启用，如果显示 Off，则执行以下命令开启：1$sudo systemsetup -setremotelogin on 再次执行查询命令以确认上一条命令生效并且该功能启用：12$sudo systemsetup -getremoteloginRemote Login: On $sudo systemsetup -getremotelogin 将同时启用 ssh 以及 sftp server 在这些命令前加上 sudo 是必须的，因为这些操作均要求管理员权限。 在 Windows 下远程操作 MacOS现在，在 Windows 系统下使用 bash 或 PuTTy 客户端，连接到该 Mac 所在的 ip 地址：1ssh username@ipaddress 登录完成后便与在 MacOS 本机上使用该账号操作 terminal 一样了。 远程登录同时开启了 sftp 服务，这意味着如果使用类似 winscp 之类的文件传输客户端也可以连接 MacOS 并完成文件传输。 为远程登录添加安全访问机制ssh 针对不同 group 的用户提供了不同的远程访问策略，使用密码登录是一件非常不安全的，如果该 MacOS 主机在通过内网穿透或拥有公网 ip 暴露在互联网下，那么随时有遭受攻击的可能，为了加强安全性，我们可以： 禁止使用密码验证登录 开启 ssh key pair 验证登录 确保我们已经进行远程登录后，首先导航到 ssh 配置所在的目录：1cd /etc/ssh/ 列出文件列表：1ls -ll 使用 vim 编辑器打开 sshd_config 文件：1vi sshd_config 按下 ‘i’ 使 vim 进入 insert 模式找到 #PasswordAuthentication yes 一行，删除行头的 ‘#’ 字符，并将 ‘yes’ 改为 ‘no’ ‘#’ 表示对该行进行注释 找到 #PubKeyAuthentication yes 一行，删除行头的 ‘#’ 字符，按下 ‘esc’ 键退出 ‘insert’ 模式，按下 ‘:wq’ 以保存并退出 vim 编辑器。 若要修改生效， sshd 进程需要重新读取该配置，但这会让已经通过密码登录的会话中断，并且在 public key 部署前没有任何机会重新进行远程连接，所以这一步放到最后来做。 现在，复制先准备好的 ssh public key 的值，回到登录用户的主目录并导航到 .ssh 目录下：12cd #cd .ssh 新建 authorized_keys 文件并使用 vim 编辑器编辑：12touch authorized_keys$sudo vi authorized_keys 粘贴已经复制到剪贴板中的 ssh public key 的值，’:wq’ 保存并退出。 在前文提及的 /etc/ssh/sshd_config 中有一行 AuthorizedKeyFiles，该行的默认值为 .ssh/authorized_keys，该项配置是 sshd 进程提取 public key 的依据，如果对该值进行了修改，那么这里新建的文件也必须要与之对应。 现在，重新读取 sshd 的配置文件信息1$sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>diy</tag>
        <tag>make-the-best-of-my-macmini</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建 Hexo 轻博客]]></title>
    <url>%2FWeb%2Fhexo%2Fhexo_setup%2F</url>
    <content type="text"><![CDATA[搭建博客的第一步，就是要找到符合自己需求的博客框架。之前用 LAMP 搭过 Wordpress，但就写博客这个需求来说 Wordpress 的功能太重了，希望环境尽量越简单越好。与同事们交流得知 Hexo 是一个轻量级的基于 Markdown(.md) 的轻博客框架，自己便开始了 DIY。 环境准备 Node.js 安装 hexo npm 包 git bash 客户端 hexo 是 npm 的一个包，首先从 Node.js 官网下载对应系统的安装包。Node.js 安装完成后会自动在环境变量中加入 npm cli 的路径，接着在 cli 中使用以下命令安装 hexo 包：1npm install -g hexo-cli 确认安装已经成功：1hexo version 初始化以下两种途径初始化 hexo： 首先创建指定目录作为 hexo 的根目录，在该目录下运行命令行工具，执行 hexo init，该命令会以当前目录作为 hexo 的根 启动命令行工具，执行 hexo init &lt;folder&gt; 初始化指定目录为 hexo 的根，随后执行 cd &lt;folder&gt; 导航到该目录 hexo init &lt;folder&gt; 命令中，如果 folder 所包含的目录层级中包含不存在的层级，该命令会自动创建该层级下的目录 初始化完成后，得到以下的目录结构：12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 在初始化完成后的目录中包含了 package.json 文件，意味着该项目有其自己的 npm 包依赖，执行 npm install 解析依赖并在本地安装。 配置 _config.yml_config.yml 文件包含了对 hexo 博客站点的配置信息。 网站(site) 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 author 作者名字 language 网站使用的语言 timezone 时区 其中，description 主要用于 SEO，为搜索引擎提供一个关于站点的简单描述，通常建议包含网站的关键词。author 参数用于主题显示文章的作者。 网址(url) 参数 描述 默认值(示例值) url 根地址 http://www.your-site.conm/blog/ root 站点根目录 /blog/ permalink 文章的永久链接格式 :year/:month/:day/:title/ permalink_defaults 永久链接中各部分的默认值 永久链接是指，当执行 hexo s 或发布到 web server 之后，其生成文件的 url 组成策略，默认值为 :year/:month/:day/:title/，这意味着 url 将以分隔的日期值来定位文章。 url 和 root 总是成对修改，例如，想要把站点的根目录放在子目录 /blog 下，那么 root 的值应该修改为 /blog/，url 的值对应修改为 http://www.your-site.com/blog/。官方文档中的网站存放于子目录的意思是，hexo 项目包含的是生成站点的源代码文件，包括 source/_posts 目录，均用于生成网站，而“网站”是指生成好之后的静态资源文件集。 更多配置详情参考官方文档 本地测试本地生成并 serve：123cd &lt;target-directory&gt;hexo ghexo server 此时在浏览器中访问 localhost 对应的端口号将看到一个新的模板页面已经生成成功，执行如下命令来创建新的文章，在 source/_posts 目录下一个对应的文件被创建1hexo new "my-first-post" 导航到该文件可以看到其模板内容 12345---title: my-first-postdate: 2018-03-01 02:51:53tags:--- 重新生成博客文章并启动 hexo server12hexo generatehexo server -o --debug -o 参数代表在成功 serve 立即打开一个浏览器窗口以访问站点的根页面，--debug 参数启用在控制台打印 debug 级别的日志信息，方便排错。hexo generate 可简写为 hexo g，hexo server 可简写为 hexo s 文章列表中已经列出刚刚创建的文章名称，进一步修改该文件，hexo server 将实时更新页面内容 部署到 Github Pages当站点准备就绪后，下一步就是要给它找个家，GitHub 为每个账号提供了站点寄宿服务——GitHub Pages，在 GitHub 中创建一个 username.github.io 的公开仓库，将 build 好的站点资源文件集 push 到该仓库，再访问唯一的 https://username.github.io/ 即可。 hexo 提供了自动部署机制，在 _config.yml 中找到 deploy 这一项：123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:username/username.github.io.git branch: master 部署的更多详情参考官方文档 GitHub Pages 同时支持自定域名解析，假设已经有了一个域名 example.com，那么在 dns 提供商后台将自定的二级域名或 www 域名指向 username.github.io 即可，具体参考 https://help.github.com/articles/using-a-custom-domain-with-github-pages/GitHub Pages 使用自定域名后无法启用 https 通信协议。]]></content>
      <categories>
        <category>Web</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于我的博客]]></title>
    <url>%2Fdiary%2Fabout%2F</url>
    <content type="text"><![CDATA[自成为程序员以来，就养成了记笔记的习惯，很多细节问题当时解决了，大多数时候是以某种 workaround 的方式解决的，如果问题的根源没有找准，那么以后遇到又会 repeat yourself。虽然记笔记一度花掉很多时间，但它能够帮助自己对遇到的问题以及寻求解决途径的过程进行梳理，在这个过程中，也能训练自己对表达和记录某件事的思路，进而加深理解并巩固印象。同时，这也为所有不同种类的信息提供了一个统一的入口进行查阅。 最近，用了两年的 WizNote 又到期了，借着这个契机，便有了自建博客的想法。 我想在这里写下所有我认为有价值的内容，其中可能包括某个技术的备忘要点，某些技术的深入探索，读书笔记，和日常工作中遇到的问题。这些内容将主要涵盖： .NET 技术生态 Web 技术生态 数据库技术栈 架构设计与模式 Linux 学习与心得 自己 diy 的一些过程 这里的内容主要是给我自己看，但又不像私人日记那么随意。大部分内容是从以往的云笔记经过重新梳理搬移过来，我保持了与当时一致的创建日期，在搬移和记录过程中我也会随时调整文件的分类和标签，以使其符合某种逻辑进行组织。 总之，我暂时想象不出这个站点日后会变成什么样子。希望今后某一天我回看这些内容的时候，会欣慰的发现原来自己已经学到了这么多东西，并且能够追寻这些文字找到来时的路。]]></content>
      <categories>
        <category>diary</category>
      </categories>
  </entry>
</search>
